-                "export_format": self.signal_export_format,
-            },
-            "weights": {
-                "collision": self.collision_weight,
-                "ttc": self.ttc_weight,
-                "boundary": self.boundary_weight,
-            },
-        }
-
-    def save_json(self, path: str | Path) -> None:
-        """保存为 JSON 文件。"""
-        with open(path, "w", encoding="utf-8") as f:
-            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
-
-    def save_yaml(self, path: str | Path) -> None:
-        """保存为 YAML 文件。"""
-        try:
-            import yaml
-        except ImportError as e:
-            raise ImportError("需要安装 pyyaml: pip install pyyaml") from e
-
-        with open(path, "w", encoding="utf-8") as f:
-            yaml.dump(self.to_dict(), f, allow_unicode=True, default_flow_style=False)
-
-    def get_principle(self, name: str) -> PrincipleConfig | None:
-        """获取指定名称的原则配置。"""
-        for p in self.principles:
-            if p.name == name:
-                return p
-        return None
-
-    def is_principle_enabled(self, name: str) -> bool:
-        """检查原则是否启用。"""
-        p = self.get_principle(name)
-        return p.enabled if p else False
diff --git a/src/aylm/constitution/principles/__init__.py b/src/aylm/constitution/principles/__init__.py
deleted file mode 100644
index 3640bf4..0000000
--- a/src/aylm/constitution/principles/__init__.py
+++ /dev/null
@@ -1,12 +0,0 @@
-"""内置宪法原则实现。
-
-提供常用的宪法原则参考实现，用户可以直接使用或作为自定义实现的参考。
-"""
-
-from .collision import NoCollisionPrinciple
-from .ttc import TTCSafetyPrinciple
-
-__all__ = [
-    "NoCollisionPrinciple",
-    "TTCSafetyPrinciple",
-]
diff --git a/src/aylm/constitution/principles/collision.py b/src/aylm/constitution/principles/collision.py
deleted file mode 100644
index 736cef4..0000000
--- a/src/aylm/constitution/principles/collision.py
+++ /dev/null
@@ -1,139 +0,0 @@
-"""碰撞检测宪法原则。
-
-提供碰撞检测的参考实现，用户可以继承或修改。
-"""
-
-from typing import TYPE_CHECKING
-
-import numpy as np
-
-from ..base import ConstitutionPrinciple, Severity, ViolationResult
-from ..registry import ConstitutionRegistry
-
-if TYPE_CHECKING:
-    from ..types import AIDecision, SceneState
-
-
-@ConstitutionRegistry.register_principle("no_collision")
-class NoCollisionPrinciple(ConstitutionPrinciple):
-    """无碰撞原则。
-
-    检测 AI 决策轨迹是否与障碍物发生碰撞。
-
-    数学表述：
-        ∀t: V_ego(t) ∩ V_obstacle(t) = ∅
-
-    参数：
-        safety_margin: 安全边距（米）
-        prediction_horizon: 预测时间范围（秒）
-    """
-
-    def __init__(
-        self,
-        safety_margin: float = 0.5,
-        prediction_horizon: float = 2.0,
-    ):
-        self.safety_margin = safety_margin
-        self.prediction_horizon = prediction_horizon
-
-    @property
-    def name(self) -> str:
-        return "no_collision"
-
-    @property
-    def severity(self) -> Severity:
-        return Severity.CRITICAL
-
-    @property
-    def description(self) -> str:
-        return "检测 AI 决策是否会导致碰撞"
-
-    def evaluate(
-        self,
-        state: "SceneState",
-        decision: "AIDecision",
-    ) -> ViolationResult:
-        """评估碰撞风险。
-
-        Args:
-            state: 场景状态（包含障碍物列表）
-            decision: AI 决策（包含规划轨迹）
-
-        Returns:
-            ViolationResult: 违规检测结果
-        """
-        if not state.obstacles:
-            return ViolationResult(
-                violated=False,
-                severity=self.severity,
-                confidence=1.0,
-                description="无障碍物",
-            )
-
-        if not decision.trajectory:
-            return ViolationResult(
-                violated=False,
-                severity=self.severity,
-                confidence=0.5,
-                description="无轨迹信息，无法评估",
-            )
-
-        # 检测轨迹与障碍物的最小距离
-        min_distance = float("inf")
-        collision_obstacle = None
-
-        for traj_point in decision.trajectory:
-            if traj_point.timestamp > self.prediction_horizon:
-                break
-
-            for obstacle in state.obstacles:
-                # 获取障碍物中心（支持不同格式）
-                if hasattr(obstacle, "center_robot"):
-                    obs_center = np.array(obstacle.center_robot)
-                elif hasattr(obstacle, "center"):
-                    obs_center = np.array(obstacle.center)
-                else:
-                    continue
-
-                # 计算距离
-                distance = np.linalg.norm(traj_point.position - obs_center)
-
-                # 考虑障碍物尺寸
-                if hasattr(obstacle, "dimensions"):
-                    # 简化：使用最大维度作为半径
-                    obs_radius = max(obstacle.dimensions) / 2
-                    distance -= obs_radius
-
-                if distance < min_distance:
-                    min_distance = distance
-                    collision_obstacle = obstacle
-
-        # 判断是否碰撞
-        collision = min_distance < self.safety_margin
-
-        return ViolationResult(
-            violated=collision,
-            severity=self.severity,
-            confidence=0.9 if collision else 0.95,
-            description=(
-                f"检测到碰撞风险，最小距离 {min_distance:.2f}m"
-                if collision
-                else f"安全，最小距离 {min_distance:.2f}m"
-            ),
-            metrics={
-                "min_distance": float(min_distance),
-                "safety_margin": self.safety_margin,
-            },
-            correction_hint=(
-                {
-                    "action": "avoid",
-                    "obstacle_position": (
-                        collision_obstacle.center_robot
-                        if hasattr(collision_obstacle, "center_robot")
-                        else None
-                    ),
-                }
-                if collision and collision_obstacle
-                else None
-            ),
-        )
diff --git a/src/aylm/constitution/principles/ttc.py b/src/aylm/constitution/principles/ttc.py
deleted file mode 100644
index 8f871e7..0000000
--- a/src/aylm/constitution/principles/ttc.py
+++ /dev/null
@@ -1,151 +0,0 @@
-"""TTC（碰撞时间）安全原则。
-
-提供 TTC 计算的参考实现。
-"""
-
-from typing import TYPE_CHECKING
-
-import numpy as np
-
-from ..base import ConstitutionPrinciple, Severity, ViolationResult
-from ..registry import ConstitutionRegistry
-
-if TYPE_CHECKING:
-    from ..types import AIDecision, SceneState
-
-
-@ConstitutionRegistry.register_principle("ttc_safety")
-class TTCSafetyPrinciple(ConstitutionPrinciple):
-    """TTC 安全原则。
-
-    计算与动态障碍物的碰撞时间（Time To Collision）。
-
-    数学表述：
-        ∀o ∈ O_dynamic: TTC(ego, o) > τ_min
-
-    其中：
-        TTC = (d - d_safe) / v_rel
-
-    参数：
-        warning_threshold: 警告阈值（秒）
-        critical_threshold: 关键阈值（秒）
-        min_safe_distance: 最小安全距离（米）
-    """
-
-    def __init__(
-        self,
-        warning_threshold: float = 3.0,
-        critical_threshold: float = 1.5,
-        min_safe_distance: float = 2.0,
-    ):
-        self.warning_threshold = warning_threshold
-        self.critical_threshold = critical_threshold
-        self.min_safe_distance = min_safe_distance
-
-    @property
-    def name(self) -> str:
-        return "ttc_safety"
-
-    @property
-    def severity(self) -> Severity:
-        return Severity.HIGH
-
-    @property
-    def description(self) -> str:
-        return "检测与动态障碍物的碰撞时间是否安全"
-
-    def evaluate(
-        self,
-        state: "SceneState",
-        decision: "AIDecision",
-    ) -> ViolationResult:
-        """评估 TTC 安全性。
-
-        Args:
-            state: 场景状态
-            decision: AI 决策
-
-        Returns:
-            ViolationResult: 违规检测结果
-        """
-        if not state.obstacles:
-            return ViolationResult(
-                violated=False,
-                severity=self.severity,
-                confidence=1.0,
-                description="无障碍物",
-            )
-
-        min_ttc = float("inf")
-
-        ego_pos = state.ego_state.position
-        ego_vel = state.ego_state.velocity
-
-        for obstacle in state.obstacles:
-            # 只检测动态障碍物
-            if hasattr(obstacle, "motion") and obstacle.motion:
-                motion = obstacle.motion
-                if motion.is_stationary:
-                    continue
-
-                # 获取障碍物位置和速度
-                if hasattr(obstacle, "center_robot"):
-                    obs_pos = np.array(obstacle.center_robot)
-                else:
-                    continue
-
-                obs_vel = np.array(motion.velocity_robot)
-
-                # 计算相对位置和速度
-                rel_pos = obs_pos - ego_pos
-                rel_vel = obs_vel - ego_vel
-
-                # 计算距离
-                distance = np.linalg.norm(rel_pos)
-
-                # 计算相对速度在连线方向的分量
-                if distance > 0:
-                    direction = rel_pos / distance
-                    closing_speed = -np.dot(rel_vel, direction)
-
-                    # 只有在接近时才计算 TTC
-                    if closing_speed > 0.1:  # 接近速度阈值
-                        ttc = (distance - self.min_safe_distance) / closing_speed
-
-                        if ttc < min_ttc:
-                            min_ttc = ttc
-                            _ = obstacle  # 保留用于未来扩展
-
-        # 判断违规
-        if min_ttc < self.critical_threshold:
-            violated = True
-            severity = Severity.CRITICAL
-            desc = f"TTC 关键警告: {min_ttc:.1f}s < {self.critical_threshold}s"
-        elif min_ttc < self.warning_threshold:
-            violated = True
-            severity = Severity.HIGH
-            desc = f"TTC 警告: {min_ttc:.1f}s < {self.warning_threshold}s"
-        else:
-            violated = False
-            severity = self.severity
-            desc = f"TTC 安全: {min_ttc:.1f}s"
-
-        return ViolationResult(
-            violated=violated,
-            severity=severity,
-            confidence=0.85,
-            description=desc,
-            metrics={
-                "min_ttc": float(min_ttc) if min_ttc != float("inf") else -1,
-                "warning_threshold": self.warning_threshold,
-                "critical_threshold": self.critical_threshold,
-            },
-            correction_hint=(
-                {
-                    "action": "slow_down",
-                    "target_ttc": self.warning_threshold,
-                }
-                if violated
-                else None
-            ),
-        )
diff --git a/src/aylm/constitution/registry.py b/src/aylm/constitution/registry.py
deleted file mode 100644
index 2c45f2f..0000000
--- a/src/aylm/constitution/registry.py
+++ /dev/null
@@ -1,165 +0,0 @@
-"""宪法原则注册表。
-
-提供插件机制，允许第三方注册自定义的宪法原则和打分器。
-"""
-
-from typing import TYPE_CHECKING, ClassVar
-
-if TYPE_CHECKING:
-    from .base import ConstitutionPrinciple
-    from .scorer import SafetyScorer
-    from .training import TrainingSignalGenerator
-
-
-class ConstitutionRegistry:
-    """宪法原则注册表。
-
-    支持第三方插件注册自定义的宪法原则、打分器和训练信号生成器。
-
-    Example:
-        >>> from aylm.constitution import ConstitutionRegistry, ConstitutionPrinciple
-        >>>
-        >>> @ConstitutionRegistry.register_principle("my_custom_rule")
-        ... class MyCustomPrinciple(ConstitutionPrinciple):
-        ...     @property
-        ...     def name(self) -> str:
-        ...         return "my_custom_rule"
-        ...     # ... 实现其他方法
-        >>>
-        >>> # 获取已注册的原则
-        >>> principle_cls = ConstitutionRegistry.get_principle("my_custom_rule")
-        >>> principle = principle_cls()
-    """
-
-    _principles: ClassVar[dict[str, type["ConstitutionPrinciple"]]] = {}
-    _scorers: ClassVar[dict[str, type["SafetyScorer"]]] = {}
-    _generators: ClassVar[dict[str, type["TrainingSignalGenerator"]]] = {}
-
-    @classmethod
-    def register_principle(cls, name: str):
-        """装饰器：注册自定义宪法原则。
-
-        Args:
-            name: 原则名称（唯一标识符）
-
-        Example:
-            >>> @ConstitutionRegistry.register_principle("no_collision")
-            ... class NoCollisionPrinciple(ConstitutionPrinciple):
-            ...     pass
-        """
-
-        def decorator(principle_cls: type["ConstitutionPrinciple"]):
-            cls._principles[name] = principle_cls
-            return principle_cls
-
-        return decorator
-
-    @classmethod
-    def register_scorer(cls, name: str):
-        """装饰器：注册自定义打分器。
-
-        Args:
-            name: 打分器名称
-
-        Example:
-            >>> @ConstitutionRegistry.register_scorer("weighted")
-            ... class WeightedScorer(SafetyScorer):
-            ...     pass
-        """
-
-        def decorator(scorer_cls: type["SafetyScorer"]):
-            cls._scorers[name] = scorer_cls
-            return scorer_cls
-
-        return decorator
-
-    @classmethod
-    def register_generator(cls, name: str):
-        """装饰器：注册自定义训练信号生成器。
-
-        Args:
-            name: 生成器名称
-
-        Example:
-            >>> @ConstitutionRegistry.register_generator("tfrecord")
-            ... class TFRecordGenerator(TrainingSignalGenerator):
-            ...     pass
-        """
-
-        def decorator(generator_cls: type["TrainingSignalGenerator"]):
-            cls._generators[name] = generator_cls
-            return generator_cls
-
-        return decorator
-
-    @classmethod
-    def get_principle(cls, name: str) -> type["ConstitutionPrinciple"] | None:
-        """获取已注册的原则类。"""
-        return cls._principles.get(name)
-
-    @classmethod
-    def get_scorer(cls, name: str) -> type["SafetyScorer"] | None:
-        """获取已注册的打分器类。"""
-        return cls._scorers.get(name)
-
-    @classmethod
-    def get_generator(cls, name: str) -> type["TrainingSignalGenerator"] | None:
-        """获取已注册的生成器类。"""
-        return cls._generators.get(name)
-
-    @classmethod
-    def list_principles(cls) -> list[str]:
-        """列出所有已注册的原则名称。"""
-        return list(cls._principles.keys())
-
-    @classmethod
-    def list_scorers(cls) -> list[str]:
-        """列出所有已注册的打分器名称。"""
-        return list(cls._scorers.keys())
-
-    @classmethod
-    def list_generators(cls) -> list[str]:
-        """列出所有已注册的生成器名称。"""
-        return list(cls._generators.keys())
-
-    @classmethod
-    def create_principle(cls, name: str, **kwargs) -> "ConstitutionPrinciple":
-        """创建原则实例。
-
-        Args:
-            name: 原则名称
-            **kwargs: 传递给原则构造函数的参数
-
-        Returns:
-            原则实例
-
-        Raises:
-            KeyError: 如果原则未注册
-        """
-        principle_cls = cls._principles.get(name)
-        if principle_cls is None:
-            raise KeyError(f"原则 '{name}' 未注册。已注册: {cls.list_principles()}")
-        return principle_cls(**kwargs)
-
-    @classmethod
-    def create_scorer(cls, name: str, **kwargs) -> "SafetyScorer":
-        """创建打分器实例。"""
-        scorer_cls = cls._scorers.get(name)
-        if scorer_cls is None:
-            raise KeyError(f"打分器 '{name}' 未注册。已注册: {cls.list_scorers()}")
-        return scorer_cls(**kwargs)
-
-    @classmethod
-    def create_generator(cls, name: str, **kwargs) -> "TrainingSignalGenerator":
-        """创建生成器实例。"""
-        generator_cls = cls._generators.get(name)
-        if generator_cls is None:
-            raise KeyError(f"生成器 '{name}' 未注册。已注册: {cls.list_generators()}")
-        return generator_cls(**kwargs)
-
-    @classmethod
-    def clear(cls) -> None:
-        """清除所有注册（主要用于测试）。"""
-        cls._principles.clear()
-        cls._scorers.clear()
-        cls._generators.clear()
diff --git a/src/aylm/constitution/scorer.py b/src/aylm/constitution/scorer.py
deleted file mode 100644
index 453116e..0000000
--- a/src/aylm/constitution/scorer.py
+++ /dev/null
@@ -1,137 +0,0 @@
-"""安全打分器模块。
-
-本模块定义了安全打分的抽象接口：
-- SafetyScore: 安全评分结果
-- SafetyScorer: 安全打分器基类
-
-用户可以继承 SafetyScorer 实现自定义的打分逻辑。
-"""
-
-from abc import ABC, abstractmethod
-from dataclasses import dataclass, field
-from enum import Enum
-from typing import TYPE_CHECKING, Any
-
-if TYPE_CHECKING:
-    from ..tools.obstacle_marker import ObstacleBox3D
-    from .types import AIDecision, EgoState
-
-
-class RecommendedAction(Enum):
-    """推荐动作。"""
-
-    SAFE = "safe"  # 安全，无需干预
-    CAUTION = "caution"  # 注意，轻微风险
-    WARNING = "warning"  # 警告，需要关注
-    INTERVENTION = "intervention"  # 干预，建议接管
-    EMERGENCY_STOP = "emergency_stop"  # 紧急停车
-
-
-@dataclass
-class SafetyScore:
-    """安全评分结果。
-
-    Attributes:
-        overall: 综合安全分数 (0.0-1.0，1.0 最安全)
-        collision_score: 碰撞风险分数
-        ttc_score: TTC（碰撞时间）分数
-        boundary_score: 边界合规分数
-        violations: 违规的原则名称列表
-        violation_details: 违规详情
-        recommended_action: 推荐动作
-        confidence: 评分置信度
-    """
-
-    overall: float
-    collision_score: float = 1.0
-    ttc_score: float = 1.0
-    boundary_score: float = 1.0
-    violations: list[str] = field(default_factory=list)
-    violation_details: list[dict[str, Any]] = field(default_factory=list)
-    recommended_action: RecommendedAction = RecommendedAction.SAFE
-    confidence: float = 1.0
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典格式。"""
-        return {
-            "overall": self.overall,
-            "scores": {
-                "collision": self.collision_score,
-                "ttc": self.ttc_score,
-                "boundary": self.boundary_score,
-            },
-            "violations": self.violations,
-            "violation_details": self.violation_details,
-            "recommended_action": self.recommended_action.value,
-            "confidence": self.confidence,
-        }
-
-    @property
-    def is_safe(self) -> bool:
-        """是否安全（无违规）。"""
-        return len(self.violations) == 0
-
-    @property
-    def needs_intervention(self) -> bool:
-        """是否需要干预。"""
-        return self.recommended_action in (
-            RecommendedAction.INTERVENTION,
-            RecommendedAction.EMERGENCY_STOP,
-        )
-
-
-class SafetyScorer(ABC):
-    """安全打分器基类。
-
-    用户可以继承此类实现自定义的打分逻辑。
-    不同的 AI 系统可能有不同的安全标准和打分方式。
-
-    Example:
-        >>> class MySafetyScorer(SafetyScorer):
-        ...     def score(self, obstacles, ego_state, ai_decision) -> SafetyScore:
-        ...         # 实现自定义打分逻辑
-        ...         collision_risk = self._check_collision(obstacles, ai_decision)
-        ...         ttc = self._compute_ttc(obstacles, ego_state)
-        ...         return SafetyScore(
-        ...             overall=min(collision_risk, ttc),
-        ...             collision_score=collision_risk,
-        ...             ttc_score=ttc,
-        ...         )
-    """
-
-    @abstractmethod
-    def score(
-        self,
-        obstacles: list["ObstacleBox3D"],
-        ego_state: "EgoState",
-        ai_decision: "AIDecision",
-    ) -> SafetyScore:
-        """计算安全分数。
-
-        Args:
-            obstacles: 3D 障碍物列表（来自 A-YLM 感知模块）
-            ego_state: 自车状态（位置、速度、航向等）
-            ai_decision: AI 的决策（规划轨迹、控制指令等）
-
-        Returns:
-            SafetyScore: 安全评分结果
-        """
-        pass
-
-    def score_from_scene(
-        self,
-        scene_data: dict[str, Any],
-    ) -> SafetyScore:
-        """从场景数据计算安全分数（便捷方法）。
-
-        Args:
-            scene_data: 场景数据字典，包含 obstacles、ego_state、ai_decision
-
-        Returns:
-            SafetyScore: 安全评分结果
-        """
-        return self.score(
-            obstacles=scene_data.get("obstacles", []),
-            ego_state=scene_data.get("ego_state"),
-            ai_decision=scene_data.get("ai_decision"),
-        )
diff --git a/src/aylm/constitution/training.py b/src/aylm/constitution/training.py
deleted file mode 100644
index 57b3508..0000000
--- a/src/aylm/constitution/training.py
+++ /dev/null
@@ -1,175 +0,0 @@
-"""训练信号生成模块。
-
-本模块定义了自监督学习的训练信号接口：
-- TrainingSignal: 训练信号数据结构
-- TrainingSignalGenerator: 训练信号生成器基类
-
-这是实现"几何宪法式 AI 自循环自训练"的关键模块。
-用户可以根据自己的 AI 训练框架实现具体的信号生成逻辑。
-"""
-
-from abc import ABC, abstractmethod
-from dataclasses import dataclass, field
-from enum import Enum
-from typing import TYPE_CHECKING, Any
-
-if TYPE_CHECKING:
-    from .scorer import SafetyScore
-    from .types import AIDecision, SceneState
-
-
-class SignalType(Enum):
-    """训练信号类型。"""
-
-    POSITIVE = "positive"  # 正样本：安全决策
-    NEGATIVE = "negative"  # 负样本：违规决策
-    CORRECTION = "correction"  # 纠正样本：包含正确行为指导
-
-
-@dataclass
-class TrainingSignal:
-    """训练信号。
-
-    用于自监督学习的训练数据，可上传到云端用于 AI 模型改进。
-
-    Attributes:
-        timestamp: 时间戳
-        frame_id: 帧 ID
-        signal_type: 信号类型（正/负/纠正）
-        safety_score: 安全分数
-        violations: 违规的原则列表
-        scene_context: 场景上下文（障碍物、自车状态等）
-        ai_decision: AI 的原始决策
-        correction_target: 纠正目标（如果是纠正样本）
-        metadata: 额外元数据
-    """
-
-    timestamp: float
-    frame_id: int
-    signal_type: SignalType
-    safety_score: float
-    violations: list[str] = field(default_factory=list)
-    scene_context: dict[str, Any] = field(default_factory=dict)
-    ai_decision: dict[str, Any] = field(default_factory=dict)
-    correction_target: dict[str, Any] | None = None
-    metadata: dict[str, Any] = field(default_factory=dict)
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典格式（用于序列化）。"""
-        return {
-            "timestamp": self.timestamp,
-            "frame_id": self.frame_id,
-            "signal_type": self.signal_type.value,
-            "safety_score": self.safety_score,
-            "violations": self.violations,
-            "scene_context": self.scene_context,
-            "ai_decision": self.ai_decision,
-            "correction_target": self.correction_target,
-            "metadata": self.metadata,
-        }
-
-    @classmethod
-    def from_dict(cls, data: dict[str, Any]) -> "TrainingSignal":
-        """从字典创建实例。"""
-        return cls(
-            timestamp=data["timestamp"],
-            frame_id=data["frame_id"],
-            signal_type=SignalType(data["signal_type"]),
-            safety_score=data["safety_score"],
-            violations=data.get("violations", []),
-            scene_context=data.get("scene_context", {}),
-            ai_decision=data.get("ai_decision", {}),
-            correction_target=data.get("correction_target"),
-            metadata=data.get("metadata", {}),
-        )
-
-
-class TrainingSignalGenerator(ABC):
-    """训练信号生成器基类。
-
-    用户可以继承此类实现自定义的训练信号生成逻辑。
-    不同的 AI 训练框架可能需要不同格式的训练数据。
-
-    Example:
-        >>> class MySignalGenerator(TrainingSignalGenerator):
-        ...     def generate(self, safety_result, scene_state, ai_decision):
-        ...         if not safety_result.is_safe:
-        ...             return TrainingSignal(
-        ...                 timestamp=scene_state.timestamp,
-        ...                 frame_id=scene_state.frame_id,
-        ...                 signal_type=SignalType.NEGATIVE,
-        ...                 safety_score=safety_result.overall,
-        ...                 violations=safety_result.violations,
-        ...             )
-        ...         return None
-    """
-
-    @abstractmethod
-    def generate(
-        self,
-        safety_result: "SafetyScore",
-        scene_state: "SceneState",
-        ai_decision: "AIDecision",
-    ) -> TrainingSignal | None:
-        """生成训练信号。
-
-        Args:
-            safety_result: 安全评分结果
-            scene_state: 当前场景状态
-            ai_decision: AI 的决策
-
-        Returns:
-            TrainingSignal: 训练信号，如果不需要生成则返回 None
-        """
-        pass
-
-    @abstractmethod
-    def export(
-        self,
-        signals: list[TrainingSignal],
-        output_path: str,
-        format: str = "json",
-    ) -> None:
-        """导出训练信号。
-
-        Args:
-            signals: 训练信号列表
-            output_path: 输出路径
-            format: 导出格式（json, tfrecord, parquet 等）
-        """
-        pass
-
-    def should_generate_positive(self) -> bool:
-        """是否生成正样本（可覆盖）。
-
-        默认不生成正样本，因为正样本数量通常远大于负样本。
-        用户可以根据需要覆盖此方法。
-        """
-        return False
-
-    def filter_signals(
-        self,
-        signals: list[TrainingSignal],
-        min_score: float = 0.0,
-        max_score: float = 1.0,
-        signal_types: list[SignalType] | None = None,
-    ) -> list[TrainingSignal]:
-        """过滤训练信号。
-
-        Args:
-            signals: 原始信号列表
-            min_score: 最小安全分数
-            max_score: 最大安全分数
-            signal_types: 允许的信号类型
-
-        Returns:
-            过滤后的信号列表
-        """
-        filtered = []
-        for signal in signals:
-            if not (min_score <= signal.safety_score <= max_score):
-                continue
-            if signal_types and signal.signal_type not in signal_types:
-                continue
-            filtered.append(signal)
-        return filtered
diff --git a/src/aylm/constitution/types.py b/src/aylm/constitution/types.py
deleted file mode 100644
index 0b2b1bc..0000000
--- a/src/aylm/constitution/types.py
+++ /dev/null
@@ -1,136 +0,0 @@
-"""宪法式 AI 类型定义。
-
-定义场景状态、AI 决策等核心数据结构。
-"""
-
-from dataclasses import dataclass, field
-from typing import Any
-
-import numpy as np
-from numpy.typing import NDArray
-
-
-@dataclass
-class EgoState:
-    """自车/机器人状态。
-
-    Attributes:
-        position: 位置 [x, y, z]（机器人坐标系）
-        velocity: 速度 [vx, vy, vz]（m/s）
-        heading: 航向角（弧度）
-        speed: 标量速度（m/s）
-        acceleration: 加速度 [ax, ay, az]（m/s²）
-        dimensions: 尺寸 [length, width, height]（米）
-    """
-
-    position: NDArray[np.float32]
-    velocity: NDArray[np.float32]
-    heading: float
-    speed: float
-    acceleration: NDArray[np.float32] | None = None
-    dimensions: NDArray[np.float32] | None = None
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典格式。"""
-        return {
-            "position": self.position.tolist(),
-            "velocity": self.velocity.tolist(),
-            "heading": self.heading,
-            "speed": self.speed,
-            "acceleration": (
-                self.acceleration.tolist() if self.acceleration is not None else None
-            ),
-            "dimensions": (
-                self.dimensions.tolist() if self.dimensions is not None else None
-            ),
-        }
-
-
-@dataclass
-class TrajectoryPoint:
-    """轨迹点。"""
-
-    position: NDArray[np.float32]  # [x, y, z]
-    velocity: NDArray[np.float32] | None = None  # [vx, vy, vz]
-    timestamp: float = 0.0  # 相对时间（秒）
-
-
-@dataclass
-class AIDecision:
-    """AI 决策。
-
-    表示端到端 AI 系统输出的决策，可以是轨迹规划或控制指令。
-
-    Attributes:
-        decision_type: 决策类型（trajectory, control, waypoint）
-        trajectory: 规划轨迹（轨迹点列表）
-        control: 控制指令（steering, throttle, brake）
-        target_speed: 目标速度（m/s）
-        confidence: 决策置信度
-        metadata: 额外元数据
-    """
-
-    decision_type: str  # "trajectory", "control", "waypoint"
-    trajectory: list[TrajectoryPoint] = field(default_factory=list)
-    control: dict[str, float] = field(default_factory=dict)  # steering, throttle, brake
-    target_speed: float | None = None
-    confidence: float = 1.0
-    metadata: dict[str, Any] = field(default_factory=dict)
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典格式。"""
-        return {
-            "decision_type": self.decision_type,
-            "trajectory": [
-                {
-                    "position": p.position.tolist(),
-                    "velocity": p.velocity.tolist() if p.velocity is not None else None,
-                    "timestamp": p.timestamp,
-                }
-                for p in self.trajectory
-            ],
-            "control": self.control,
-            "target_speed": self.target_speed,
-            "confidence": self.confidence,
-            "metadata": self.metadata,
-        }
-
-
-@dataclass
-class SceneState:
-    """场景状态。
-
-    包含当前帧的所有感知信息，供宪法原则评估使用。
-
-    Attributes:
-        frame_id: 帧 ID
-        timestamp: 时间戳（秒）
-        ego_state: 自车状态
-        obstacles: 障碍物列表（来自 A-YLM 感知模块）
-        lane_boundaries: 车道边界（可选）
-        traffic_signs: 交通标志（可选）
-        metadata: 额外元数据
-    """
-
-    frame_id: int
-    timestamp: float
-    ego_state: EgoState
-    obstacles: list[Any] = field(default_factory=list)  # ObstacleBox3D 列表
-    lane_boundaries: list[Any] | None = None
-    traffic_signs: list[Any] | None = None
-    metadata: dict[str, Any] = field(default_factory=dict)
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典格式。"""
-        return {
-            "frame_id": self.frame_id,
-            "timestamp": self.timestamp,
-            "ego_state": self.ego_state.to_dict(),
-            "obstacles": [
-                obs.to_dict() if hasattr(obs, "to_dict") else obs
-                for obs in self.obstacles
-            ],
-            "lane_boundaries": self.lane_boundaries,
-            "traffic_signs": self.traffic_signs,
-            "metadata": self.metadata,
-        }
diff --git a/src/aylm/tools/__init__.py b/src/aylm/tools/__init__.py
index f46f532..b73e5de 100644
--- a/src/aylm/tools/__init__.py
+++ b/src/aylm/tools/__init__.py
@@ -6,22 +6,7 @@ from .coordinate_utils import (
     transform_for_navigation,
     transform_obstacle_center,
 )
-from .motion_estimator import (
-    KalmanConfig,
-    MotionEstimator,
-    MotionVector,
-    TrackedObject3D,
-    create_tracked_object,
-)
-from .multiframe_fusion import (
-    FramePose,
-    FusionResult,
-    MultiframeFusion,
-    RegistrationConfig,
-    RegistrationResult,
-)
 from .object_detector import DetectorConfig, ObjectDetector
-from .object_tracker import MultiObjectTracker, TrackedObject, TrackerConfig
 from .obstacle_marker import (
     ObstacleBox3D,
     ObstacleMarker,
@@ -94,20 +79,4 @@ __all__ = [
     "opencv_to_robot",
     "robot_to_opencv",
     "transform_obstacle_center",
-    # 多帧融合
-    "MultiframeFusion",
-    "RegistrationConfig",
-    "RegistrationResult",
-    "FramePose",
-    "FusionResult",
-    # 运动估计
-    "MotionEstimator",
-    "MotionVector",
-    "TrackedObject3D",
-    "KalmanConfig",
-    "create_tracked_object",
-    # 目标跟踪
-    "MultiObjectTracker",
-    "TrackedObject",
-    "TrackerConfig",
 ]
diff --git a/src/aylm/tools/coordinate_utils.py b/src/aylm/tools/coordinate_utils.py
index 295edb8..7c69813 100644
--- a/src/aylm/tools/coordinate_utils.py
+++ b/src/aylm/tools/coordinate_utils.py
@@ -223,7 +223,7 @@ def _read_ply(
                 raise ValueError("PLY文件数据不完整")
             values = line.split()
             points.append([float(values[0]), float(values[1]), float(values[2])])
-            if has_colors and colors is not None and len(values) >= 6:
+            if has_colors and len(values) >= 6:
                 colors.append([int(values[3]), int(values[4]), int(values[5])])

     return header_lines, np.array(points), has_colors, colors
diff --git a/src/aylm/tools/motion_estimator.py b/src/aylm/tools/motion_estimator.py
deleted file mode 100644
index e67046d..0000000
--- a/src/aylm/tools/motion_estimator.py
+++ /dev/null
@@ -1,408 +0,0 @@
-"""运动矢量估计器模块。
-
-基于连续帧的 3D 位置变化计算速度，使用 Kalman 滤波器平滑轨迹。
-"""
-
-from dataclasses import dataclass
-from typing import Optional
-
-import numpy as np
-from numpy.typing import NDArray
-
-from .coordinate_utils import opencv_to_robot
-from .semantic_types import SemanticLabel
-
-
-@dataclass
-class MotionVector:
-    """运动矢量。"""
-
-    velocity_cv: NDArray[np.float32]  # OpenCV 坐标系速度 [vx, vy, vz] m/s
-    velocity_robot: NDArray[np.float32]  # 机器人坐标系速度 [vx, vy, vz] m/s
-    speed: float  # 标量速度 m/s
-    heading: float  # 航向角（弧度，相对于机器人前进方向）
-    is_stationary: bool  # 是否静止（速度 < 阈值）
-
-
-@dataclass
-class TrackedObject3D:
-    """3D 跟踪对象。"""
-
-    track_id: int
-    center_cv: NDArray[np.float32]  # OpenCV 坐标系中心
-    center_robot: NDArray[np.float32]  # 机器人坐标系中心
-    dimensions: NDArray[np.float32]  # [width, height, depth]
-    motion: Optional[MotionVector]  # 运动信息
-    semantic_label: SemanticLabel
-    confidence: float
-    frame_id: int
-    timestamp: float  # 秒
-
-
-@dataclass
-class KalmanConfig:
-    """Kalman 滤波器配置。"""
-
-    process_noise: float = 0.1  # 过程噪声
-    measurement_noise: float = 0.5  # 观测噪声
-    initial_covariance: float = 1.0  # 初始协方差
-
-
-@dataclass
-class _TrackState:
-    """单个轨迹的 Kalman 滤波器状态。"""
-
-    # 状态向量: [x, y, z, vx, vy, vz]
-    state: NDArray[np.float64]
-    # 协方差矩阵 6x6
-    covariance: NDArray[np.float64]
-    # 上一帧 ID
-    last_frame_id: int
-    # 上一帧时间戳
-    last_timestamp: float
-    # 初始化标志
-    initialized: bool = False
-
-
-class MotionEstimator:
-    """运动矢量估计器。
-
-    使用 Kalman 滤波器平滑轨迹并估计速度。
-
-    状态向量: [x, y, z, vx, vy, vz]
-    观测向量: [x, y, z]
-    """
-
-    def __init__(
-        self,
-        fps: float = 30.0,
-        stationary_threshold: float = 0.1,
-        kalman_config: Optional[KalmanConfig] = None,
-    ):
-        """初始化运动估计器。
-
-        Args:
-            fps: 帧率，用于计算默认时间间隔
-            stationary_threshold: 静止判定阈值（m/s）
-            kalman_config: Kalman 滤波器配置
-        """
-        self.fps = fps
-        self.dt_default = 1.0 / fps
-        self.stationary_threshold = stationary_threshold
-        self.config = kalman_config or KalmanConfig()
-
-        # 轨迹状态字典 {track_id: _TrackState}
-        self._tracks: dict[int, _TrackState] = {}
-
-        # 预计算 Kalman 矩阵
-        self._init_kalman_matrices()
-
-    def _init_kalman_matrices(self) -> None:
-        """初始化 Kalman 滤波器矩阵。"""
-        # 观测矩阵 H: 3x6，只观测位置
-        self._H = np.array(
-            [
-                [1, 0, 0, 0, 0, 0],
-                [0, 1, 0, 0, 0, 0],
-                [0, 0, 1, 0, 0, 0],
-            ],
-            dtype=np.float64,
-        )
-
-        # 观测噪声协方差 R: 3x3
-        self._R = np.eye(3, dtype=np.float64) * self.config.measurement_noise
-
-    def _get_transition_matrix(self, dt: float) -> NDArray[np.float64]:
-        """获取状态转移矩阵 F。
-
-        Args:
-            dt: 时间间隔（秒）
-
-        Returns:
-            6x6 状态转移矩阵
-        """
-        return np.array(
-            [
-                [1, 0, 0, dt, 0, 0],
-                [0, 1, 0, 0, dt, 0],
-                [0, 0, 1, 0, 0, dt],
-                [0, 0, 0, 1, 0, 0],
-                [0, 0, 0, 0, 1, 0],
-                [0, 0, 0, 0, 0, 1],
-            ],
-            dtype=np.float64,
-        )
-
-    def _get_process_noise(self, dt: float) -> NDArray[np.float64]:
-        """获取过程噪声协方差矩阵 Q。
-
-        使用离散白噪声加速度模型。
-
-        Args:
-            dt: 时间间隔（秒）
-
-        Returns:
-            6x6 过程噪声协方差矩阵
-        """
-        q = self.config.process_noise
-        dt2 = dt * dt
-        dt3 = dt2 * dt
-        dt4 = dt3 * dt
-
-        # 离散白噪声加速度模型
-        return (
-            np.array(
-                [
-                    [dt4 / 4, 0, 0, dt3 / 2, 0, 0],
-                    [0, dt4 / 4, 0, 0, dt3 / 2, 0],
-                    [0, 0, dt4 / 4, 0, 0, dt3 / 2],
-                    [dt3 / 2, 0, 0, dt2, 0, 0],
-                    [0, dt3 / 2, 0, 0, dt2, 0],
-                    [0, 0, dt3 / 2, 0, 0, dt2],
-                ],
-                dtype=np.float64,
-            )
-            * q
-        )
-
-    def _init_track(
-        self, position: NDArray, frame_id: int, timestamp: float
-    ) -> _TrackState:
-        """初始化新轨迹。
-
-        Args:
-            position: 初始位置 [x, y, z]
-            frame_id: 帧 ID
-            timestamp: 时间戳
-
-        Returns:
-            新的轨迹状态
-        """
-        state = np.zeros(6, dtype=np.float64)
-        state[:3] = position
-
-        covariance = np.eye(6, dtype=np.float64) * self.config.initial_covariance
-        # 速度初始不确定性更大
-        covariance[3:, 3:] *= 10.0
-
-        return _TrackState(
-            state=state,
-            covariance=covariance,
-            last_frame_id=frame_id,
-            last_timestamp=timestamp,
-            initialized=True,
-        )
-
-    def _predict(self, track: _TrackState, dt: float) -> None:
-        """Kalman 预测步骤。
-
-        Args:
-            track: 轨迹状态
-            dt: 时间间隔
-        """
-        F = self._get_transition_matrix(dt)
-        Q = self._get_process_noise(dt)
-
-        # 状态预测: x' = F * x
-        track.state = F @ track.state
-
-        # 协方差预测: P' = F * P * F^T + Q
-        track.covariance = F @ track.covariance @ F.T + Q
-
-    def _update_kalman(
-        self, track: _TrackState, measurement: NDArray[np.float64]
-    ) -> None:
-        """Kalman 更新步骤。
-
-        Args:
-            track: 轨迹状态
-            measurement: 观测值 [x, y, z]
-        """
-        H = self._H
-        R = self._R
-
-        # 创新（残差）: y = z - H * x
-        y = measurement - H @ track.state
-
-        # 创新协方差: S = H * P * H^T + R
-        S = H @ track.covariance @ H.T + R
-
-        # Kalman 增益: K = P * H^T * S^(-1)
-        K = track.covariance @ H.T @ np.linalg.inv(S)
-
-        # 状态更新: x = x + K * y
-        track.state = track.state + K @ y
-
-        # 协方差更新: P = (I - K * H) * P
-        identity = np.eye(6, dtype=np.float64)
-        track.covariance = (identity - K @ H) @ track.covariance
-
-    def update(
-        self,
-        track_id: int,
-        position_cv: NDArray,
-        frame_id: int,
-        timestamp: Optional[float] = None,
-    ) -> Optional[MotionVector]:
-        """更新轨迹，返回运动矢量。
-
-        Args:
-            track_id: 轨迹 ID
-            position_cv: OpenCV 坐标系位置 [x, y, z]
-            frame_id: 帧 ID
-            timestamp: 时间戳（秒），如果为 None 则根据帧率计算
-
-        Returns:
-            运动矢量，如果是第一帧则返回 None
-        """
-        position = np.asarray(position_cv, dtype=np.float64)
-
-        # 计算时间戳
-        if timestamp is None:
-            timestamp = frame_id / self.fps
-
-        # 检查是否是新轨迹
-        if track_id not in self._tracks:
-            self._tracks[track_id] = self._init_track(position, frame_id, timestamp)
-            return None
-
-        track = self._tracks[track_id]
-
-        # 计算时间间隔
-        dt = timestamp - track.last_timestamp
-        if dt <= 0:
-            dt = self.dt_default
-
-        # Kalman 预测
-        self._predict(track, dt)
-
-        # Kalman 更新
-        self._update_kalman(track, position)
-
-        # 更新时间信息
-        track.last_frame_id = frame_id
-        track.last_timestamp = timestamp
-
-        # 提取速度
-        velocity_cv = track.state[3:6].astype(np.float32)
-
-        # 转换到机器人坐标系
-        velocity_robot = opencv_to_robot(velocity_cv).astype(np.float32)
-
-        # 计算标量速度
-        speed = float(np.linalg.norm(velocity_cv))
-
-        # 计算航向角（在机器人坐标系中，相对于 X 轴/前进方向）
-        # heading = atan2(vy, vx)，范围 [-pi, pi]
-        heading = float(np.arctan2(velocity_robot[1], velocity_robot[0]))
-
-        # 判断是否静止
-        is_stationary = speed < self.stationary_threshold
-
-        return MotionVector(
-            velocity_cv=velocity_cv,
-            velocity_robot=velocity_robot,
-            speed=speed,
-            heading=heading,
-            is_stationary=is_stationary,
-        )
-
-    def predict(self, track_id: int, dt: float) -> Optional[NDArray[np.float64]]:
-        """预测未来位置。
-
-        Args:
-            track_id: 轨迹 ID
-            dt: 预测时间间隔（秒）
-
-        Returns:
-            预测的 OpenCV 坐标系位置 [x, y, z]，如果轨迹不存在则返回 None
-        """
-        if track_id not in self._tracks:
-            return None
-
-        track = self._tracks[track_id]
-        F = self._get_transition_matrix(dt)
-
-        # 预测状态
-        predicted_state = F @ track.state
-
-        return predicted_state[:3]
-
-    def get_velocity(self, track_id: int) -> Optional[NDArray[np.float64]]:
-        """获取当前速度估计。
-
-        Args:
-            track_id: 轨迹 ID
-
-        Returns:
-            OpenCV 坐标系速度 [vx, vy, vz]，如果轨迹不存在则返回 None
-        """
-        if track_id not in self._tracks:
-            return None
-
-        return self._tracks[track_id].state[3:6].copy()
-
-    def remove_track(self, track_id: int) -> bool:
-        """移除轨迹。
-
-        Args:
-            track_id: 轨迹 ID
-
-        Returns:
-            是否成功移除
-        """
-        if track_id in self._tracks:
-            del self._tracks[track_id]
-            return True
-        return False
-
-    def clear(self) -> None:
-        """清除所有轨迹。"""
-        self._tracks.clear()
-
-    @property
-    def active_tracks(self) -> list[int]:
-        """获取活跃轨迹 ID 列表。"""
-        return list(self._tracks.keys())
-
-
-def create_tracked_object(
-    track_id: int,
-    center_cv: NDArray,
-    dimensions: NDArray,
-    semantic_label: SemanticLabel,
-    confidence: float,
-    frame_id: int,
-    timestamp: float,
-    motion: Optional[MotionVector] = None,
-) -> TrackedObject3D:
-    """创建 3D 跟踪对象。
-
-    Args:
-        track_id: 轨迹 ID
-        center_cv: OpenCV 坐标系中心
-        dimensions: 尺寸 [width, height, depth]
-        semantic_label: 语义标签
-        confidence: 置信度
-        frame_id: 帧 ID
-        timestamp: 时间戳
-        motion: 运动信息
-
-    Returns:
-        TrackedObject3D 实例
-    """
-    center_cv = np.asarray(center_cv, dtype=np.float32)
-    center_robot = opencv_to_robot(center_cv).astype(np.float32)
-    dimensions = np.asarray(dimensions, dtype=np.float32)
-
-    return TrackedObject3D(
-        track_id=track_id,
-        center_cv=center_cv,
-        center_robot=center_robot,
-        dimensions=dimensions,
-        motion=motion,
-        semantic_label=semantic_label,
-        confidence=confidence,
-        frame_id=frame_id,
-        timestamp=timestamp,
-    )
diff --git a/src/aylm/tools/multiframe_fusion.py b/src/aylm/tools/multiframe_fusion.py
deleted file mode 100644
index 002a741..0000000
--- a/src/aylm/tools/multiframe_fusion.py
+++ /dev/null
@@ -1,597 +0,0 @@
-"""多帧点云配准融合模块。
-
-使用 Open3D 实现 ICP 配准和位姿图优化，将多帧点云融合成全局地图。
-"""
-
-from __future__ import annotations
-
-import json
-import logging
-import time
-from dataclasses import dataclass, field
-from pathlib import Path
-from typing import Any
-
-import numpy as np
-from numpy.typing import NDArray
-
-from .pointcloud_voxelizer import PointCloud
-
-logger = logging.getLogger(__name__)
-
-# 检查 Open3D 是否可用
-try:
-    import open3d as o3d
-
-    HAS_OPEN3D = True
-except ImportError:
-    HAS_OPEN3D = False
-    o3d = None  # type: ignore[assignment]
-
-
-@dataclass
-class RegistrationConfig:
-    """配准配置参数。"""
-
-    # ICP 配准参数
-    icp_max_correspondence_distance: float = 0.05  # ICP 最大对应点距离（米）
-    icp_max_iteration: int = 50  # ICP 最大迭代次数
-    icp_relative_fitness: float = 1e-6  # 相对适应度收敛阈值
-    icp_relative_rmse: float = 1e-6  # 相对 RMSE 收敛阈值
-
-    # 特征提取参数
-    voxel_size_for_features: float = 0.05  # 特征提取用的体素大小
-    fpfh_radius: float = 0.25  # FPFH 特征半径
-    fpfh_max_nn: int = 100  # FPFH 最大近邻数
-    normal_radius: float = 0.1  # 法向量估计半径
-
-    # 位姿图优化参数
-    pose_graph_edge_prune_threshold: float = 0.25  # 边修剪阈值
-    pose_graph_preference_loop_closure: float = 0.1  # 闭环偏好
-
-    # 全局融合参数
-    fusion_voxel_size: float = 0.02  # 融合后体素下采样大小
-
-    # 配准质量阈值
-    min_fitness: float = 0.3  # 最小配准适应度
-    min_points: int = 1000  # 最小点数要求
-
-
-@dataclass
-class FramePose:
-    """单帧位姿信息。"""
-
-    frame_index: int  # 帧索引
-    transformation: NDArray[np.float64]  # 4x4 变换矩阵
-    fitness: float = 0.0  # 配准适应度
-    rmse: float = 0.0  # 配准 RMSE
-    is_keyframe: bool = False  # 是否为关键帧
-
-    def to_dict(self) -> dict[str, Any]:
-        """转换为字典。"""
-        return {
-            "index": self.frame_index,
-            "transformation": self.transformation.tolist(),
-            "fitness": self.fitness,
-            "rmse": self.rmse,
-            "is_keyframe": self.is_keyframe,
-        }
-
-    @classmethod
-    def from_dict(cls, data: dict[str, Any]) -> FramePose:
-        """从字典创建。"""
-        return cls(
-            frame_index=data["index"],
-            transformation=np.array(data["transformation"]),
-            fitness=data.get("fitness", 0.0),
-            rmse=data.get("rmse", 0.0),
-            is_keyframe=data.get("is_keyframe", False),
-        )
-
-
-@dataclass
-class RegistrationResult:
-    """配准结果。"""
-
-    transformation: NDArray[np.float64]  # 4x4 变换矩阵
-    fitness: float  # 适应度
-    inlier_rmse: float  # 内点 RMSE
-    correspondence_count: int  # 对应点数量
-
-
-@dataclass
-class FusionResult:
-    """融合结果。"""
-
-    fused_pointcloud: PointCloud  # 融合后的点云
-    frame_poses: list[FramePose] = field(default_factory=list)  # 所有帧的位姿
-    total_frames: int = 0  # 总帧数
-    successful_registrations: int = 0  # 成功配准数
-    fusion_time: float = 0.0  # 融合耗时
-
-
-class MultiframeFusion:
-    """多帧点云配准融合器。
-
-    使用 ICP 算法进行相邻帧配准，位姿图优化进行全局一致性调整，
-    最终将所有帧变换到世界坐标系并合并。
-
-    Example:
-        >>> config = RegistrationConfig(icp_max_correspondence_distance=0.05)
-        >>> fusion = MultiframeFusion(config)
-        >>> result = fusion.fuse_sequence(pointcloud_list)
-        >>> fusion.save_fused_map(result, "fused_map.ply")
-    """
-
-    def __init__(self, config: RegistrationConfig | None = None):
-        """初始化融合器。
-
-        Args:
-            config: 配准配置，为 None 时使用默认配置
-        """
-        if not HAS_OPEN3D:
-            raise ImportError("Open3D 未安装，请运行: pip install open3d")
-
-        self.config = config or RegistrationConfig()
-        logger.info("MultiframeFusion 初始化完成")
-
-    def fuse_sequence(
-        self,
-        pointclouds: list[PointCloud],
-        initial_poses: list[NDArray[np.float64]] | None = None,
-    ) -> FusionResult:
-        """融合点云序列。
-
-        Args:
-            pointclouds: 点云列表（按时间顺序）
-            initial_poses: 可选的初始位姿估计
-
-        Returns:
-            FusionResult: 融合结果
-        """
-        start_time = time.time()
-        n_frames = len(pointclouds)
-
-        if n_frames == 0:
-            logger.warning("输入点云列表为空")
-            return FusionResult(fused_pointcloud=PointCloud(np.zeros((0, 3))))
-
-        if n_frames == 1:
-            logger.info("只有一帧，直接返回")
-            pose = FramePose(0, np.eye(4), 1.0, 0.0, True)
-            return FusionResult(
-                fused_pointcloud=pointclouds[0],
-                frame_poses=[pose],
-                total_frames=1,
-                successful_registrations=1,
-                fusion_time=time.time() - start_time,
-            )
-
-        logger.info(f"开始融合 {n_frames} 帧点云")
-
-        # 1. 预处理所有点云
-        logger.info("预处理点云...")
-        processed = []
-        valid_indices = []
-        for i, pc in enumerate(pointclouds):
-            if len(pc.points) < self.config.min_points:
-                logger.warning(f"帧 {i} 点数不足 ({len(pc.points)}), 跳过")
-                continue
-            pcd, fpfh = self._preprocess_pointcloud(pc)
-            processed.append((pcd, fpfh, pc))
-            valid_indices.append(i)
-
-        if len(processed) < 2:
-            logger.error("有效帧数不足，无法配准")
-            return FusionResult(fused_pointcloud=PointCloud(np.zeros((0, 3))))
-
-        # 2. 相邻帧配准（前一帧 -> 后一帧）
-        logger.info("执行相邻帧配准...")
-        pairwise_results: list[RegistrationResult | None] = []
-        for i in range(len(processed) - 1):
-            source_pcd, source_fpfh, _ = processed[i]
-            target_pcd, target_fpfh, _ = processed[i + 1]
-
-            result = self._register_pair_o3d(
-                source_pcd, target_pcd, source_fpfh, target_fpfh
-            )
-
-            if result.fitness < self.config.min_fitness:
-                logger.warning(
-                    f"帧 {valid_indices[i]} -> {valid_indices[i+1]} "
-                    f"配准质量低 (fitness={result.fitness:.3f})"
-                )
-            pairwise_results.append(result)
-            logger.debug(
-                f"帧 {valid_indices[i]} -> {valid_indices[i+1]}: "
-                f"fitness={result.fitness:.3f}, rmse={result.inlier_rmse:.4f}"
-            )
-
-        # 3. 构建位姿图
-        logger.info("构建位姿图...")
-        pose_graph = self._build_pose_graph(processed, pairwise_results)
-
-        # 4. 优化位姿图
-        logger.info("优化位姿图...")
-        pose_graph = self._optimize_pose_graph(pose_graph)
-
-        # 5. 提取优化后的位姿
-        frame_poses = []
-        successful = 0
-        for i, node in enumerate(pose_graph.nodes):
-            original_idx = valid_indices[i]
-            prev_result = pairwise_results[i - 1] if i > 0 else None
-            pose = FramePose(
-                frame_index=original_idx,
-                transformation=np.asarray(node.pose),
-                fitness=prev_result.fitness if prev_result else 1.0,
-                rmse=prev_result.inlier_rmse if prev_result else 0.0,
-                is_keyframe=(i == 0),
-            )
-            frame_poses.append(pose)
-            if i == 0 or (
-                prev_result and prev_result.fitness >= self.config.min_fitness
-            ):
-                successful += 1
-
-        # 6. 变换并合并点云
-        logger.info("合并点云...")
-        original_pcs = [p[2] for p in processed]
-        fused_pc = self._transform_and_merge(original_pcs, frame_poses)
-
-        # 7. 体素下采样
-        logger.info("体素下采样...")
-        fused_pc = self._voxel_downsample(fused_pc, self.config.fusion_voxel_size)
-
-        fusion_time = time.time() - start_time
-        logger.info(
-            f"融合完成: {len(fused_pc.points)} 点, "
-            f"{successful}/{len(processed)} 帧成功, 耗时 {fusion_time:.2f}s"
-        )
-
-        return FusionResult(
-            fused_pointcloud=fused_pc,
-            frame_poses=frame_poses,
-            total_frames=n_frames,
-            successful_registrations=successful,
-            fusion_time=fusion_time,
-        )
-
-    def fuse_from_directory(
-        self,
-        input_dir: Path,
-        pattern: str = "vox_*.ply",
-    ) -> FusionResult:
-        """从目录加载并融合点云序列。
-
-        Args:
-            input_dir: 输入目录
-            pattern: 文件匹配模式
-
-        Returns:
-            FusionResult: 融合结果
-        """
-        input_dir = Path(input_dir)
-        ply_files = sorted(input_dir.glob(pattern))
-
-        if not ply_files:
-            logger.error(f"未找到匹配 {pattern} 的文件: {input_dir}")
-            return FusionResult(fused_pointcloud=PointCloud(np.zeros((0, 3))))
-
-        logger.info(f"加载 {len(ply_files)} 个点云文件")
-        pointclouds = []
-        for ply_path in ply_files:
-            pc = PointCloud.from_ply(ply_path)
-            pointclouds.append(pc)
-            logger.debug(f"加载 {ply_path.name}: {len(pc.points)} 点")
-
-        return self.fuse_sequence(pointclouds)
-
-    def register_pair(
-        self,
-        source: PointCloud,
-        target: PointCloud,
-        initial_transform: NDArray[np.float64] | None = None,
-    ) -> RegistrationResult:
-        """配准两帧点云。
-
-        Args:
-            source: 源点云
-            target: 目标点云
-            initial_transform: 初始变换估计
-
-        Returns:
-            RegistrationResult: 配准结果
-        """
-        source_pcd, source_fpfh = self._preprocess_pointcloud(source)
-        target_pcd, target_fpfh = self._preprocess_pointcloud(target)
-
-        if initial_transform is not None:
-            # 直接使用 ICP 精配准
-            return self._fine_registration(source_pcd, target_pcd, initial_transform)
-        else:
-            # 粗配准 + 精配准
-            return self._register_pair_o3d(
-                source_pcd, target_pcd, source_fpfh, target_fpfh
-            )
-
-    def save_fused_map(
-        self,
-        result: FusionResult,
-        output_path: Path,
-        include_poses: bool = True,
-    ) -> None:
-        """保存融合后的地图。
-
-        Args:
-            result: 融合结果
-            output_path: 输出 PLY 文件路径
-            include_poses: 是否同时保存位姿文件
-        """
-        output_path = Path(output_path)
-        output_path.parent.mkdir(parents=True, exist_ok=True)
-
-        # 保存点云
-        result.fused_pointcloud.to_ply(output_path)
-        logger.info(f"融合地图已保存: {output_path}")
-
-        # 保存位姿
-        if include_poses and result.frame_poses:
-            poses_path = output_path.with_suffix(".poses.json")
-            self.save_poses(result.frame_poses, poses_path)
-
-    def save_poses(
-        self,
-        poses: list[FramePose],
-        output_path: Path,
-    ) -> None:
-        """保存位姿轨迹到 JSON 文件。
-
-        Args:
-            poses: 位姿列表
-            output_path: 输出路径
-        """
-        output_path = Path(output_path)
-
-        data = {
-            "version": "1.0",
-            "coordinate_system": "opencv",
-            "frames": [p.to_dict() for p in poses],
-            "statistics": {
-                "total_frames": len(poses),
-                "average_fitness": np.mean([p.fitness for p in poses]),
-                "average_rmse": np.mean([p.rmse for p in poses]),
-            },
-        }
-
-        with open(output_path, "w", encoding="utf-8") as f:
-            json.dump(data, f, indent=2, ensure_ascii=False)
-
-        logger.info(f"位姿轨迹已保存: {output_path}")
-
-    # ========== 内部方法 ==========
-
-    def _to_o3d(self, pc: PointCloud) -> o3d.geometry.PointCloud:
-        """转换为 Open3D 点云格式。"""
-        pcd = o3d.geometry.PointCloud()
-        pcd.points = o3d.utility.Vector3dVector(pc.points.astype(np.float64))
-        if pc.colors is not None:
-            colors = pc.colors.astype(np.float64)
-            if colors.max() > 1.0:
-                colors = colors / 255.0
-            pcd.colors = o3d.utility.Vector3dVector(colors)
-        return pcd
-
-    def _from_o3d(self, pcd: o3d.geometry.PointCloud) -> PointCloud:
-        """从 Open3D 点云格式转换。"""
-        points = np.asarray(pcd.points)
-        colors = None
-        if pcd.has_colors():
-            colors = (np.asarray(pcd.colors) * 255).astype(np.uint8)
-        return PointCloud(points=points, colors=colors)
-
-    def _preprocess_pointcloud(
-        self,
-        pc: PointCloud,
-    ) -> tuple[o3d.geometry.PointCloud, o3d.pipelines.registration.Feature]:
-        """预处理点云：下采样 + 计算法向量 + 提取 FPFH 特征。"""
-        pcd = self._to_o3d(pc)
-
-        # 体素下采样
-        pcd_down = pcd.voxel_down_sample(self.config.voxel_size_for_features)
-
-        # 估计法向量
-        pcd_down.estimate_normals(
-            o3d.geometry.KDTreeSearchParamHybrid(
-                radius=self.config.normal_radius, max_nn=30
-            )
-        )
-
-        # 计算 FPFH 特征
-        fpfh = o3d.pipelines.registration.compute_fpfh_feature(
-            pcd_down,
-            o3d.geometry.KDTreeSearchParamHybrid(
-                radius=self.config.fpfh_radius, max_nn=self.config.fpfh_max_nn
-            ),
-        )
-
-        return pcd_down, fpfh
-
-    def _register_pair_o3d(
-        self,
-        source_pcd: o3d.geometry.PointCloud,
-        target_pcd: o3d.geometry.PointCloud,
-        source_fpfh: o3d.pipelines.registration.Feature,
-        target_fpfh: o3d.pipelines.registration.Feature,
-    ) -> RegistrationResult:
-        """配准两帧点云（粗配准 + 精配准）。"""
-        # 粗配准: RANSAC
-        coarse_result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(
-            source_pcd,
-            target_pcd,
-            source_fpfh,
-            target_fpfh,
-            mutual_filter=True,
-            max_correspondence_distance=self.config.icp_max_correspondence_distance * 2,
-            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(
-                False
-            ),
-            ransac_n=4,
-            checkers=[
-                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),
-                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(
-                    self.config.icp_max_correspondence_distance * 2
-                ),
-            ],
-            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(
-                max_iteration=4000000, confidence=0.999
-            ),
-        )
-
-        # 精配准: Point-to-Plane ICP
-        return self._fine_registration(
-            source_pcd, target_pcd, coarse_result.transformation
-        )
-
-    def _fine_registration(
-        self,
-        source_pcd: o3d.geometry.PointCloud,
-        target_pcd: o3d.geometry.PointCloud,
-        initial_transform: NDArray[np.float64],
-    ) -> RegistrationResult:
-        """精配准：Point-to-Plane ICP。"""
-        # 确保有法向量
-        if not target_pcd.has_normals():
-            target_pcd.estimate_normals(
-                o3d.geometry.KDTreeSearchParamHybrid(
-                    radius=self.config.normal_radius, max_nn=30
-                )
-            )
-
-        result = o3d.pipelines.registration.registration_icp(
-            source_pcd,
-            target_pcd,
-            self.config.icp_max_correspondence_distance,
-            initial_transform,
-            o3d.pipelines.registration.TransformationEstimationPointToPlane(),
-            o3d.pipelines.registration.ICPConvergenceCriteria(
-                relative_fitness=self.config.icp_relative_fitness,
-                relative_rmse=self.config.icp_relative_rmse,
-                max_iteration=self.config.icp_max_iteration,
-            ),
-        )
-
-        return RegistrationResult(
-            transformation=np.asarray(result.transformation),
-            fitness=result.fitness,
-            inlier_rmse=result.inlier_rmse,
-            correspondence_count=len(result.correspondence_set),
-        )
-
-    def _build_pose_graph(
-        self,
-        processed: list[tuple],
-        pairwise_results: list[RegistrationResult | None],
-    ) -> o3d.pipelines.registration.PoseGraph:
-        """构建位姿图。
-
-        以第一帧为世界坐标系原点，累积后续帧的位姿。
-        配准结果 T_{i->i+1} 表示把帧i变换到帧i+1。
-        我们需要的是把每帧变换到帧0（世界坐标系）。
-        """
-        pose_graph = o3d.pipelines.registration.PoseGraph()
-        n_frames = len(processed)
-
-        # 第一帧位姿为单位矩阵（世界坐标系原点）
-        pose_graph.nodes.append(o3d.pipelines.registration.PoseGraphNode(np.eye(4)))
-
-        # 累积位姿：T_0^i = T_0^{i-1} @ inv(T_{i-1->i})
-        # 因为 T_{i-1->i} 把帧i-1变换到帧i，所以逆变换把帧i变换到帧i-1
-        cumulative_pose = np.eye(4)
-        for i in range(1, n_frames):
-            result = pairwise_results[i - 1]
-            if result is not None:
-                # T_{i-1->i} 的逆，把帧i变换到帧i-1的坐标系
-                inv_transform = np.linalg.inv(result.transformation)
-                cumulative_pose = cumulative_pose @ inv_transform
-            pose_graph.nodes.append(
-                o3d.pipelines.registration.PoseGraphNode(cumulative_pose.copy())
-            )
-
-        # 添加边（相邻帧）
-        for i in range(n_frames - 1):
-            result = pairwise_results[i]
-            if result is None:
-                continue
-
-            # 信息矩阵（基于配准质量）
-            information = (
-                np.eye(6) * result.fitness if result.fitness > 0 else np.eye(6) * 0.1
-            )
-
-            pose_graph.edges.append(
-                o3d.pipelines.registration.PoseGraphEdge(
-                    i,
-                    i + 1,
-                    result.transformation,
-                    information,
-                    uncertain=False,
-                )
-            )
-
-        return pose_graph
-
-    def _optimize_pose_graph(
-        self,
-        pose_graph: o3d.pipelines.registration.PoseGraph,
-    ) -> o3d.pipelines.registration.PoseGraph:
-        """优化位姿图。"""
-        option = o3d.pipelines.registration.GlobalOptimizationOption(
-            max_correspondence_distance=self.config.icp_max_correspondence_distance,
-            edge_prune_threshold=self.config.pose_graph_edge_prune_threshold,
-            preference_loop_closure=self.config.pose_graph_preference_loop_closure,
-            reference_node=0,
-        )
-
-        o3d.pipelines.registration.global_optimization(
-            pose_graph,
-            o3d.pipelines.registration.GlobalOptimizationLevenbergMarquardt(),
-            o3d.pipelines.registration.GlobalOptimizationConvergenceCriteria(),
-            option,
-        )
-
-        return pose_graph
-
-    def _transform_and_merge(
-        self,
-        pointclouds: list[PointCloud],
-        poses: list[FramePose],
-    ) -> PointCloud:
-        """将所有点云变换到世界坐标系并合并。"""
-        all_points = []
-        all_colors = []
-        has_colors = pointclouds[0].colors is not None
-
-        for pc, pose in zip(pointclouds, poses):
-            # 应用变换
-            points_h = np.hstack([pc.points, np.ones((len(pc.points), 1))])
-            transformed = (pose.transformation @ points_h.T).T[:, :3]
-            all_points.append(transformed)
-
-            if has_colors and pc.colors is not None:
-                all_colors.append(pc.colors)
-
-        merged_points = np.vstack(all_points)
-        merged_colors = np.vstack(all_colors) if has_colors else None
-
-        return PointCloud(points=merged_points, colors=merged_colors)
-
-    def _voxel_downsample(
-        self,
-        pc: PointCloud,
-        voxel_size: float,
-    ) -> PointCloud:
-        """体素下采样去重。"""
-        pcd = self._to_o3d(pc)
-        pcd_down = pcd.voxel_down_sample(voxel_size)
-        return self._from_o3d(pcd_down)
diff --git a/src/aylm/tools/object_detector.py b/src/aylm/tools/object_detector.py
index 72a94fa..086be78 100644
--- a/src/aylm/tools/object_detector.py
+++ b/src/aylm/tools/object_detector.py
@@ -7,14 +7,11 @@
 import logging
 from dataclasses import dataclass, field
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, ClassVar, Optional, Union
+from typing import Optional, Union

 import numpy as np
 from numpy.typing import NDArray

-if TYPE_CHECKING:
-    from ultralytics import YOLO
-
 from .semantic_types import COCO_TO_SEMANTIC, Detection2D, SemanticLabel

 logger = logging.getLogger(__name__)
@@ -25,7 +22,7 @@ class DetectorConfig:
     """检测器配置参数。"""

     model_name: str = "yolo11n-seg.pt"  # YOLO11 模型名称
-    confidence_threshold: float = 0.25  # 置信度阈值（降低以检测更多目标）
+    confidence_threshold: float = 0.5  # 置信度阈值
     iou_threshold: float = 0.45  # NMS IoU 阈值
     device: str = "auto"  # 设备：auto/cuda/mps/cpu
     half_precision: bool = True  # 是否使用半精度（FP16）
@@ -44,7 +41,7 @@ class ObjectDetector:
     """

     # 默认检测类别：人、自行车、汽车、摩托车、公交车、卡车
-    DEFAULT_CLASSES: ClassVar[list[int]] = [0, 1, 2, 3, 5, 7]
+    DEFAULT_CLASSES: list[int] = [0, 1, 2, 3, 5, 7]

     def __init__(self, config: Optional[DetectorConfig] = None):
         """初始化检测器。
@@ -53,7 +50,7 @@ class ObjectDetector:
             config: 检测器配置，为 None 时使用默认配置
         """
         self.config = config or DetectorConfig()
-        self._model: Optional[YOLO] = None
+        self._model = None
         self._device = self._detect_device()

         # 如果未指定类别，使用默认类别
@@ -145,7 +142,7 @@ class ObjectDetector:

     def detect(
         self,
-        image: NDArray[Any],
+        image: NDArray[np.uint8],
         return_masks: bool = True,
     ) -> list[Detection2D]:
         """执行目标检测。
@@ -266,13 +263,13 @@ class ObjectDetector:
         self.load()
         return self

-    def __exit__(self, _exc_type, _exc_val, _exc_tb) -> None:
+    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
         """上下文管理器出口，自动卸载模型。"""
         self.unload()

     def save_detection_image(
         self,
-        image: NDArray[Any],
+        image: NDArray[np.uint8],
         detections: list[Detection2D],
         output_path: Union[str, Path],
         draw_masks: bool = True,
@@ -321,7 +318,7 @@ class ObjectDetector:

             # 绘制标签背景
             label_text = f"{label_name} {det.confidence:.2f}"
-            (text_w, text_h), _baseline = cv2.getTextSize(
+            (text_w, text_h), baseline = cv2.getTextSize(
                 label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
             )
             cv2.rectangle(
@@ -350,9 +347,7 @@ class ObjectDetector:
                 )
                 mask_colored = np.zeros_like(result_image)
                 mask_colored[mask_resized > 0] = color
-                result_image = cv2.addWeighted(
-                    result_image, 1.0, mask_colored, 0.3, 0
-                )  # type: ignore[assignment]
+                result_image = cv2.addWeighted(result_image, 1.0, mask_colored, 0.3, 0)

         # 保存图片
         output_path.parent.mkdir(parents=True, exist_ok=True)
diff --git a/src/aylm/tools/object_tracker.py b/src/aylm/tools/object_tracker.py
deleted file mode 100644
index 63b111c..0000000
--- a/src/aylm/tools/object_tracker.py
+++ /dev/null
@@ -1,357 +0,0 @@
-"""多目标跟踪模块。
-
-基于 IoU 匹配的多目标跟踪器，使用匈牙利算法进行检测-轨迹匹配。
-支持轨迹的创建、更新和删除。
-"""
-
-import logging
-from dataclasses import dataclass, field
-from typing import Optional
-
-import numpy as np
-from numpy.typing import NDArray
-from scipy.optimize import linear_sum_assignment
-
-from .semantic_types import Detection2D
-
-logger = logging.getLogger(__name__)
-
-
-@dataclass
-class TrackedObject:
-    """跟踪目标。"""
-
-    track_id: int  # 唯一跟踪 ID
-    bbox: NDArray[np.float32]  # [x1, y1, x2, y2]
-    class_id: int  # COCO 类别 ID
-    confidence: float  # 置信度
-    age: int = 1  # 跟踪帧数
-    hits: int = 1  # 成功匹配次数
-    time_since_update: int = 0  # 距上次更新的帧数
-
-    def __post_init__(self) -> None:
-        """确保 bbox 是正确的类型。"""
-        if not isinstance(self.bbox, np.ndarray):
-            self.bbox = np.array(self.bbox, dtype=np.float32)
-        elif self.bbox.dtype != np.float32:
-            self.bbox = self.bbox.astype(np.float32)
-
-
-@dataclass
-class TrackerConfig:
-    """跟踪器配置参数。"""
-
-    max_age: int = 30  # 轨迹最大存活帧数（无匹配时）
-    min_hits: int = 3  # 轨迹确认所需最小匹配次数
-    iou_threshold: float = 0.3  # IoU 匹配阈值
-
-
-@dataclass
-class _Track:
-    """内部轨迹状态。"""
-
-    track_id: int
-    bbox: NDArray[np.float32]
-    class_id: int
-    confidence: float
-    age: int = 1
-    hits: int = 1
-    time_since_update: int = 0
-    velocity: NDArray[np.float32] = field(
-        default_factory=lambda: np.zeros(4, dtype=np.float32)
-    )
-
-    def predict(self) -> NDArray[np.float32]:
-        """预测下一帧位置（简单线性预测）。"""
-        predicted_bbox = self.bbox + self.velocity
-        # 确保边界框有效
-        predicted_bbox[2] = max(predicted_bbox[2], predicted_bbox[0] + 1)
-        predicted_bbox[3] = max(predicted_bbox[3], predicted_bbox[1] + 1)
-        return predicted_bbox
-
-    def update(self, detection: Detection2D) -> None:
-        """使用新检测更新轨迹。"""
-        new_bbox = detection.bbox.astype(np.float32)
-        # 更新速度（指数移动平均）
-        self.velocity = 0.7 * self.velocity + 0.3 * (new_bbox - self.bbox)
-        self.bbox = new_bbox
-        self.class_id = detection.class_id
-        self.confidence = detection.confidence
-        self.hits += 1
-        self.time_since_update = 0
-
-    def mark_missed(self) -> None:
-        """标记为未匹配。"""
-        self.time_since_update += 1
-        # 使用预测位置
-        self.bbox = self.predict()
-
-    def to_tracked_object(self) -> TrackedObject:
-        """转换为 TrackedObject。"""
-        return TrackedObject(
-            track_id=self.track_id,
-            bbox=self.bbox.copy(),
-            class_id=self.class_id,
-            confidence=self.confidence,
-            age=self.age,
-            hits=self.hits,
-            time_since_update=self.time_since_update,
-        )
-
-
-def _compute_iou(bbox1: NDArray[np.float32], bbox2: NDArray[np.float32]) -> float:
-    """计算两个边界框的 IoU。
-
-    Args:
-        bbox1: 边界框 1，[x1, y1, x2, y2]
-        bbox2: 边界框 2，[x1, y1, x2, y2]
-
-    Returns:
-        IoU 值
-    """
-    # 计算交集
-    x1 = max(bbox1[0], bbox2[0])
-    y1 = max(bbox1[1], bbox2[1])
-    x2 = min(bbox1[2], bbox2[2])
-    y2 = min(bbox1[3], bbox2[3])
-
-    if x2 <= x1 or y2 <= y1:
-        return 0.0
-
-    intersection = (x2 - x1) * (y2 - y1)
-
-    # 计算并集
-    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
-    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
-    union = area1 + area2 - intersection
-
-    if union <= 0:
-        return 0.0
-
-    return float(intersection / union)
-
-
-def _compute_iou_matrix(
-    tracks: list[_Track], detections: list[Detection2D]
-) -> NDArray[np.float32]:
-    """计算轨迹和检测之间的 IoU 矩阵。
-
-    Args:
-        tracks: 轨迹列表
-        detections: 检测列表
-
-    Returns:
-        IoU 矩阵，shape (len(tracks), len(detections))
-    """
-    if not tracks or not detections:
-        return np.empty((len(tracks), len(detections)), dtype=np.float32)
-
-    iou_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float32)
-
-    for i, track in enumerate(tracks):
-        for j, det in enumerate(detections):
-            iou_matrix[i, j] = _compute_iou(track.bbox, det.bbox)
-
-    return iou_matrix
-
-
-class MultiObjectTracker:
-    """多目标跟踪器。
-
-    基于 IoU 匹配的简化版跟踪器，使用匈牙利算法进行检测-轨迹匹配。
-
-    使用示例:
-        >>> tracker = MultiObjectTracker()
-        >>> for frame_id, detections in enumerate(all_detections):
-        ...     tracked = tracker.update(detections, frame_id)
-        ...     for obj in tracked:
-        ...         print(f"Track {obj.track_id}: {obj.bbox}")
-    """
-
-    def __init__(
-        self,
-        max_age: int = 30,
-        min_hits: int = 3,
-        iou_threshold: float = 0.3,
-        config: Optional[TrackerConfig] = None,
-    ):
-        """初始化跟踪器。
-
-        Args:
-            max_age: 轨迹最大存活帧数（无匹配时）
-            min_hits: 轨迹确认所需最小匹配次数
-            iou_threshold: IoU 匹配阈值
-            config: 跟踪器配置（优先级高于单独参数）
-        """
-        if config is not None:
-            self.config = config
-        else:
-            self.config = TrackerConfig(
-                max_age=max_age,
-                min_hits=min_hits,
-                iou_threshold=iou_threshold,
-            )
-
-        self._tracks: list[_Track] = []
-        self._next_id: int = 1
-        self._frame_count: int = 0
-
-        logger.info(
-            f"MultiObjectTracker 初始化: max_age={self.config.max_age}, "
-            f"min_hits={self.config.min_hits}, iou_threshold={self.config.iou_threshold}"
-        )
-
-    def update(
-        self, detections: list[Detection2D], frame_id: Optional[int] = None
-    ) -> list[TrackedObject]:
-        """更新跟踪器，返回跟踪结果。
-
-        Args:
-            detections: 当前帧的检测结果
-            frame_id: 帧 ID（可选，用于日志）
-
-        Returns:
-            确认的跟踪目标列表
-        """
-        self._frame_count += 1
-        current_frame = frame_id if frame_id is not None else self._frame_count
-
-        logger.debug(f"帧 {current_frame}: 收到 {len(detections)} 个检测")
-
-        # 增加所有轨迹的年龄
-        for track in self._tracks:
-            track.age += 1
-
-        if not detections:
-            # 没有检测，标记所有轨迹为未匹配
-            for track in self._tracks:
-                track.mark_missed()
-            self._remove_dead_tracks()
-            return self._get_confirmed_tracks()
-
-        if not self._tracks:
-            # 没有轨迹，为所有检测创建新轨迹
-            for det in detections:
-                self._create_track(det)
-            return self._get_confirmed_tracks()
-
-        # 计算 IoU 矩阵
-        iou_matrix = _compute_iou_matrix(self._tracks, detections)
-
-        # 使用匈牙利算法进行匹配
-        matched_tracks, matched_dets, unmatched_tracks, unmatched_dets = (
-            self._hungarian_match(iou_matrix)
-        )
-
-        logger.debug(
-            f"匹配结果: {len(matched_tracks)} 匹配, "
-            f"{len(unmatched_tracks)} 未匹配轨迹, "
-            f"{len(unmatched_dets)} 未匹配检测"
-        )
-
-        # 更新匹配的轨迹
-        for track_idx, det_idx in zip(matched_tracks, matched_dets):
-            self._tracks[track_idx].update(detections[det_idx])
-
-        # 标记未匹配的轨迹
-        for track_idx in unmatched_tracks:
-            self._tracks[track_idx].mark_missed()
-
-        # 为未匹配的检测创建新轨迹
-        for det_idx in unmatched_dets:
-            self._create_track(detections[det_idx])
-
-        # 删除死亡轨迹
-        self._remove_dead_tracks()
-
-        return self._get_confirmed_tracks()
-
-    def _hungarian_match(
-        self, iou_matrix: NDArray[np.float32]
-    ) -> tuple[list[int], list[int], list[int], list[int]]:
-        """使用匈牙利算法进行匹配。
-
-        Args:
-            iou_matrix: IoU 矩阵
-
-        Returns:
-            (匹配的轨迹索引, 匹配的检测索引, 未匹配的轨迹索引, 未匹配的检测索引)
-        """
-        if iou_matrix.size == 0:
-            return [], [], list(range(len(self._tracks))), []
-
-        # 转换为代价矩阵（1 - IoU）
-        cost_matrix = 1 - iou_matrix
-
-        # 匈牙利算法
-        track_indices, det_indices = linear_sum_assignment(cost_matrix)
-
-        matched_tracks: list[int] = []
-        matched_dets: list[int] = []
-        unmatched_tracks: list[int] = list(range(len(self._tracks)))
-        unmatched_dets: list[int] = list(range(iou_matrix.shape[1]))
-
-        for track_idx, det_idx in zip(track_indices, det_indices):
-            # 检查 IoU 是否超过阈值
-            if iou_matrix[track_idx, det_idx] >= self.config.iou_threshold:
-                matched_tracks.append(int(track_idx))
-                matched_dets.append(int(det_idx))
-                unmatched_tracks.remove(int(track_idx))
-                unmatched_dets.remove(int(det_idx))
-
-        return matched_tracks, matched_dets, unmatched_tracks, unmatched_dets
-
-    def _create_track(self, detection: Detection2D) -> _Track:
-        """创建新轨迹。"""
-        track = _Track(
-            track_id=self._next_id,
-            bbox=detection.bbox.astype(np.float32),
-            class_id=detection.class_id,
-            confidence=detection.confidence,
-        )
-        self._tracks.append(track)
-        self._next_id += 1
-
-        logger.debug(f"创建新轨迹 ID={track.track_id}")
-        return track
-
-    def _remove_dead_tracks(self) -> None:
-        """删除死亡轨迹。"""
-        before_count = len(self._tracks)
-        self._tracks = [
-            t for t in self._tracks if t.time_since_update < self.config.max_age
-        ]
-        removed = before_count - len(self._tracks)
-        if removed > 0:
-            logger.debug(f"删除 {removed} 条死亡轨迹")
-
-    def _get_confirmed_tracks(self) -> list[TrackedObject]:
-        """获取确认的轨迹（hits >= min_hits）。"""
-        confirmed = [
-            t.to_tracked_object()
-            for t in self._tracks
-            if t.hits >= self.config.min_hits and t.time_since_update == 0
-        ]
-        logger.debug(f"返回 {len(confirmed)} 条确认轨迹")
-        return confirmed
-
-    def reset(self) -> None:
-        """重置跟踪器状态。"""
-        self._tracks.clear()
-        self._next_id = 1
-        self._frame_count = 0
-        logger.info("跟踪器已重置")
-
-    def get_all_tracks(self) -> list[TrackedObject]:
-        """获取所有轨迹（包括未确认的）。"""
-        return [t.to_tracked_object() for t in self._tracks]
-
-    @property
-    def track_count(self) -> int:
-        """当前轨迹数量。"""
-        return len(self._tracks)
-
-    @property
-    def confirmed_track_count(self) -> int:
-        """确认轨迹数量。"""
-        return len([t for t in self._tracks if t.hits >= self.config.min_hits])
diff --git a/src/aylm/tools/obstacle_marker.py b/src/aylm/tools/obstacle_marker.py
index 251b4de..4e72cc6 100644
--- a/src/aylm/tools/obstacle_marker.py
+++ b/src/aylm/tools/obstacle_marker.py
@@ -74,11 +74,6 @@ class ObstacleBox3D:
     label: SemanticLabel  # 语义标签
     confidence: float  # 平均置信度
     point_indices: NDArray[np.int64] = field(repr=False)  # 属于该障碍物的点索 引
-    track_id: Optional[int] = None  # 跟踪 ID（跨帧关联）
-    frame_id: Optional[int] = None  # 帧 ID（时序关联）
-    timestamp: Optional[float] = None  # 时间戳（秒）
-    velocity: Optional[tuple[float, float, float]] = None  # 速度向量 (vx, vy, vz) m/s
-    motion_vector: Optional[tuple[float, float, float]] = None  # 运动矢量（帧 间位移）

     @property
     def is_movable(self) -> bool:
@@ -145,7 +140,7 @@ class ObstacleBox3D:
         - center_cv: OpenCV 坐标系 (X右,Y下,Z前)
         - center_robot: 机器人坐标系 (X前,Y左,Z上)
         """
-        result = {
+        return {
             "type": "可运动障碍物" if self.is_movable else "静态障碍物",
             "category": LABEL_DESCRIPTIONS.get(self.label, "未知"),
             "movable": self.is_movable,
@@ -160,24 +155,6 @@ class ObstacleBox3D:
             "_label_id": int(self.label.value),
         }

-        # 添加跟踪和时序信息
-        if self.track_id is not None:
-            result["track_id"] = self.track_id
-        if self.frame_id is not None:
-            result["frame_id"] = self.frame_id
-        if self.timestamp is not None:
-            result["timestamp"] = self.timestamp
-
-        # 添加运动信息
-        if self.velocity is not None:
-            result["velocity_cv"] = list(self.velocity)
-            result["velocity_robot"] = list(self._cv_to_robot(self.velocity))
-        if self.motion_vector is not None:
-            result["motion_vector_cv"] = list(self.motion_vector)
-            result["motion_vector_robot"] = list(self._cv_to_robot(self.motion_vector))
-
-        return result
-
     def get_box_vertices(self) -> NDArray[np.float64]:
         """获取边界框的8个顶点，用于可视化。"""
         min_c = self.min_corner
@@ -442,29 +419,18 @@ class ObstacleMarker:

         return colors

-    def export_to_json(
-        self,
-        obstacles: list[ObstacleBox3D],
-        output_path: Path,
-        frame_id: Optional[int] = None,
-        timestamp: Optional[float] = None,
-    ) -> None:
+    def export_to_json(self, obstacles: list[ObstacleBox3D], output_path: Path) -> None:
         """
         导出障碍物列表为 JSON 格式（用于导航系统）。

         Args:
             obstacles: 障碍物列表
             output_path: 输出文件路径
-            frame_id: 帧 ID（可选，用于时序关联）
-            timestamp: 时间戳（可选，秒）
         """
         # 统计可运动和静态障碍物数量
         movable_count = sum(1 for obs in obstacles if obs.is_movable)
         static_count = len(obstacles) - movable_count

-        # 统计有跟踪 ID 的障碍物
-        tracked_count = sum(1 for obs in obstacles if obs.track_id is not None)
-
         data = {
             "coordinate_systems": {
                 "cv": {
@@ -480,16 +446,9 @@ class ObstacleMarker:
             "total_count": len(obstacles),
             "movable_count": movable_count,
             "static_count": static_count,
-            "tracked_count": tracked_count,
             "obstacles": [obs.to_dict() for obs in obstacles],
         }

-        # 添加时序元数据
-        if frame_id is not None:
-            data["frame_id"] = frame_id
-        if timestamp is not None:
-            data["timestamp"] = timestamp
-
         with open(output_path, "w", encoding="utf-8") as f:
             json.dump(data, f, indent=2, ensure_ascii=False)

diff --git a/src/aylm/tools/pipeline_processor.py b/src/aylm/tools/pipeline_processor.py
index 5131e89..4fa765e 100644
--- a/src/aylm/tools/pipeline_processor.py
+++ b/src/aylm/tools/pipeline_processor.py
@@ -5,7 +5,6 @@

 from __future__ import annotations

-import contextlib
 import gc
 import logging
 import threading
@@ -14,16 +13,10 @@ from concurrent.futures import Future, ThreadPoolExecutor
 from dataclasses import dataclass
 from enum import Enum
 from pathlib import Path
-from typing import TYPE_CHECKING, Callable, ClassVar
-
-if TYPE_CHECKING:
-    from torch.nn import Module
-
-    from aylm.tools.object_detector import ObjectDetector
-    from aylm.tools.pointcloud_voxelizer import PointCloudVoxelizer
+from typing import Callable

 import torch
-from torch.nn import functional as functional_nn
+import torch.nn.functional as F

 # 模块级常量
 DEFAULT_VOXEL_SIZE = 0.05  # 5cm 体素
@@ -81,14 +74,11 @@ class PipelineConfig:
     async_mode: bool = False
     # 切片配置
     enable_slice: bool = True  # 是否启用切片
-    slice_radius: float = 10.0  # 切片半径（米）
+    slice_radius: float = 20.0  # 切片半径（米）
     # 语义检测配置
     enable_semantic: bool = True  # 是否启用语义检测（默认开启）
-    # 输入分辨率配置
-    # 注意：SHARP 模型要求固定 1536，因为内部金字塔结构依赖 1536→768→384
-    internal_resolution: int = 1536  # 内部处理分辨率（固定值，不可更改）
     semantic_model: str = "yolo11n-seg.pt"  # YOLO 模型
-    semantic_confidence: float = 0.25  # 检测置信度
+    semantic_confidence: float = 0.5  # 检测置信度
     colorize_semantic: bool = True  # 语义着色
     # 导航输出配置
     output_navigation_ply: bool = True  # 是否输出导航用点云（机器人坐标系）
@@ -132,7 +122,7 @@ class PipelineStats:
 class PipelineLogger:
     """流水线日志记录器。"""

-    LEVEL_PREFIX: ClassVar[dict[str, str]] = {
+    LEVEL_PREFIX = {
         "INFO": "   ",
         "STAGE": ">>>",
         "OK": " ✓ ",
@@ -141,7 +131,7 @@ class PipelineLogger:
         "PROGRESS": " → ",
     }

-    STATUS_DISPLAY: ClassVar[dict[TaskStatus, str]] = {
+    STATUS_DISPLAY = {
         TaskStatus.PENDING: "⏳ 等待中",
         TaskStatus.PREDICTING: "🔄 推理中",
         TaskStatus.PREDICTED: "📦 待体素化",
@@ -234,11 +224,11 @@ class PipelineProcessor:
         self.log = PipelineLogger(self.config.verbose)
         self.stats = PipelineStats()

-        self._predictor: Module | None = None
+        self._predictor = None
         self._device: torch.device | None = None
         self._model_loaded = False
-        self._voxelizer: PointCloudVoxelizer | None = None
-        self._detector: ObjectDetector | None = None  # YOLO 语义检测器
+        self._voxelizer = None
+        self._detector = None  # YOLO 语义检测器
         self._tasks: list[ImageTask] = []
         self._stop_event = threading.Event()
         self._predict_lock = threading.Lock()
@@ -248,7 +238,7 @@ class PipelineProcessor:
     def __enter__(self):
         return self

-    def __exit__(self, _exc_type, _exc_val, _exc_tb):
+    def __exit__(self, exc_type, exc_val, exc_tb):
         self.cleanup()
         return False

@@ -270,8 +260,10 @@ class PipelineProcessor:
         try:
             if self._predictor is not None:
                 if self._device and self._device.type != "cpu":
-                    with contextlib.suppress(Exception):
+                    try:
                         self._predictor.cpu()
+                    except Exception:
+                        pass
                 del self._predictor
                 self._predictor = None

@@ -284,8 +276,10 @@ class PipelineProcessor:
                 torch.cuda.synchronize()

             if hasattr(torch, "mps") and hasattr(torch.mps, "empty_cache"):
-                with contextlib.suppress(Exception):
+                try:
                     torch.mps.empty_cache()
+                except Exception:
+                    pass

             self.log.ok("模型已卸载，内存已释放")
         except Exception as e:
@@ -329,7 +323,7 @@ class PipelineProcessor:
             return torch.device(self.config.device)
         if torch.cuda.is_available():
             return torch.device("cuda")
-        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
+        if hasattr(torch, "mps") and torch.mps.is_available():
             return torch.device("mps")
         return torch.device("cpu")

@@ -359,10 +353,9 @@ class PipelineProcessor:
                 )

             self._predictor = create_predictor(PredictorParams())
-            predictor = self._predictor  # 局部变量，mypy 可以推断非 None
-            predictor.load_state_dict(state_dict)
-            predictor.eval()
-            predictor.to(self._device)
+            self._predictor.load_state_dict(state_dict)
+            self._predictor.eval()
+            self._predictor.to(self._device)

             self._model_loaded = True
             self.log.ok("模型加载完成")
@@ -406,17 +399,15 @@ class PipelineProcessor:

             self.log.info(f"    图像尺寸: {width}x{height}, 焦距: {f_px:.1f}px")

-            # 预处理（使用配置的内部分辨率）
-            res = self.config.internal_resolution
-            internal_shape = (res, res)
-            self.log.info(f"    内部处理分辨率: {res}x{res}")
+            # 预处理
+            internal_shape = (1536, 1536)
             image_pt = (
                 torch.from_numpy(image.copy()).float().to(self._device).permute(2, 0, 1)
                 / 255.0
             )
             disparity_factor = torch.tensor([f_px / width]).float().to(self._device)

-            image_resized_pt = functional_nn.interpolate(
+            image_resized_pt = F.interpolate(
                 image_pt[None],
                 size=(internal_shape[1], internal_shape[0]),
                 mode="bilinear",
@@ -425,7 +416,6 @@ class PipelineProcessor:

             # 推理（需要锁保护，因为模型不是线程安全的）
             with self._predict_lock:
-                assert self._predictor is not None, "模型未加载"
                 gaussians_ndc = self._predictor(image_resized_pt, disparity_factor)

             # 后处理
@@ -485,31 +475,26 @@ class PipelineProcessor:

     def _voxelize_single(
         self, task: ImageTask, output_dir: Path, navigation_dir: Path | None = None
-    ) -> Path | None:
-        """对单个PLY文件进行切片、体素化。
+    ) -> bool:
+        """对单个PLY文件进行切片、体素化，可选语义融合。

-        处理流程：SHARP输出 → 切片 → 体素化
-        语义融合在外部异步执行。
+        处理流程：SHARP输出 → 切片 → 体素化 → 语义融合

         Args:
             task: 图像任务
             output_dir: 体素化输出目录
-            navigation_dir: 导航用点云输出目录（未使用，保留兼容性）
-
-        Returns:
-            体素化后的 PLY 文件路径，失败返回 None
+            navigation_dir: 导航用点云输出目录（机器人坐标系）
         """
         if task.status != TaskStatus.PREDICTED or task.ply_output_path is None:
-            return None
+            return False

         task.status = TaskStatus.VOXELIZING
         task.voxel_start_time = time.time()

-        self.log.progress(f"[{task.index+1}] 开始体素化: {task.ply_output_path.name}")
+        self.log.progress(f"[{task.index+1}] 开始处理: {task.ply_output_path.name}")

         try:
             # 确定输入文件（可能经过切片）
-            assert task.ply_output_path is not None, "PLY 输出路径未设置"
             input_ply_path = task.ply_output_path

             # 步骤1：切片（如果启用）
@@ -519,7 +504,6 @@ class PipelineProcessor:
                     input_ply_path = sliced_path

             # 步骤2：体素化
-            assert self._voxelizer is not None, "体素化器未初始化"
             output_path = output_dir / f"vox_{task.ply_output_path.name}"
             self._voxelizer.process(
                 input_ply_path,
@@ -530,48 +514,35 @@ class PipelineProcessor:

             # 清理临时切片文件
             if self.config.enable_slice and input_ply_path != task.ply_output_path:
-                with contextlib.suppress(Exception):
+                try:
                     input_ply_path.unlink()
+                except Exception:
+                    pass
+
+            # 步骤3：语义融合（如果启用）
+            if self.config.enable_semantic and self._detector is not None:
+                self._apply_semantic_fusion(task, output_path, navigation_dir)

             task.voxel_output_path = output_path
-            voxel_time = time.time() - task.voxel_start_time
+            task.status = TaskStatus.COMPLETED
+            task.voxel_end_time = time.time()
+
+            voxel_time = task.voxel_end_time - task.voxel_start_time
             self.log.ok(
-                f"[{task.index+1}] 体素化完成: {output_path.name} ({voxel_time:.2f}s)"
+                f"[{task.index+1}] 处理完成: {task.ply_output_path.name} ({voxel_time:.2f}s)"
             )
+            self.log.info(f"    输出: {output_path.name}")

-            return output_path
+            return True

         except Exception as e:
             task.status = TaskStatus.FAILED
             task.error_message = str(e)
             task.voxel_end_time = time.time()
-            self.log.error(f"[{task.index+1}] 体素化失败: {task.ply_output_path.name}")
-            self.log.error(f"    错误: {e}")
-            logger.exception(f"体素化异常 - {task.ply_output_path.name}")
-            return None
-
-    def _semantic_and_navigation(
-        self, task: ImageTask, voxel_path: Path, navigation_dir: Path | None = None
-    ) -> bool:
-        """执行语义检测和导航输出（可异步调用）。
-
-        Args:
-            task: 图像任务
-            voxel_path: 体素化后的 PLY 文件路径
-            navigation_dir: 导航输出目录
-
-        Returns:
-            是否成功
-        """
-        try:
-            self._apply_semantic_fusion(task, voxel_path, navigation_dir)
-            task.status = TaskStatus.COMPLETED
-            task.voxel_end_time = time.time()
-            return True
-        except Exception as e:
-            self.log.error(f"[{task.index+1}] 语义处理失败: {e}")
-            task.status = TaskStatus.COMPLETED  # 体素化成功，语义失败不算整体 失败
-            task.voxel_end_time = time.time()
+            self.log.error(f"[{task.index+1}] 处理失败: {task.ply_output_path.name}")
+            self.log.error(f"    错误类型: {type(e).__name__}")
+            self.log.error(f"    错误信息: {e}")
+            logger.exception(f"处理异常详情 - {task.ply_output_path.name}")
             return False

     def _apply_slice(self, task: ImageTask) -> Path | None:
@@ -585,10 +556,6 @@ class PipelineProcessor:
         """
         import numpy as np

-        if task.ply_output_path is None:
-            self.log.warn("    PLY 输出路径未设置，跳过切片")
-            return None
-
         try:
             from plyfile import PlyData, PlyElement
         except ImportError:
@@ -679,7 +646,6 @@ class PipelineProcessor:
             height, width = image.shape[:2]

             # 执行目标检测
-            assert self._detector is not None, "检测器未初始化"
             detections = self._detector.detect(image, return_masks=True)
             self.log.info(f"    检测到 {len(detections)} 个目标")

@@ -836,7 +802,7 @@ class PipelineProcessor:
         if input_path.is_file():
             return [input_path] if input_path.suffix.lower() in extensions else []

-        images: list[Path] = []
+        images = []
         for ext in extensions:
             images.extend(input_path.glob(f"*{ext}"))
             images.extend(input_path.glob(f"*{ext.upper()}"))
@@ -1053,10 +1019,10 @@ class PipelineProcessor:
     ):
         """执行流水线处理逻辑。

-        三级并行流水线:
-        - 推理(N): 主线程执行 SHARP 推理
-        - 体素化(N-1): 线程1 执行体素化
-        - 语义检测(N-2): 线程2 执行语义检测和导航输出
+        流水线策略:
+        1. 第一张图片: 只做推理（无并行）
+        2. 第2到N张图片: 推理第N张 || 体素化第N-1张（并行）
+        3. 最后: 体素化最后一张图片（无并行）

         时间线示意:
             图片1: [====推理====]
@@ -1064,11 +1030,8 @@ class PipelineProcessor:
             图片1:              [====体素化====]
             图片3:                            [====推理====]
             图片2:                            [====体素化====]
-            图片1:                            [====语义====]
             ...

-        每帧完成语义检测后立即输出导航点云，实现快速响应。
-
         Args:
             output_dir: PLY 输出目录
             voxel_output_dir: 体素化输出目录
@@ -1080,117 +1043,57 @@ class PipelineProcessor:
             self.log.warn("没有任务需要处理")
             return

-        # 使用两个线程池：体素化和语义检测
-        with (
-            ThreadPoolExecutor(
-                max_workers=1, thread_name_prefix="voxel"
-            ) as voxel_executor,
-            ThreadPoolExecutor(
-                max_workers=1, thread_name_prefix="semantic"
-            ) as semantic_executor,
-        ):
-
+        # 使用线程池执行体素化（推理在主线程，因为GPU操作需要同步）
+        with ThreadPoolExecutor(max_workers=1) as voxel_executor:
             voxel_future: Future | None = None
-            semantic_future: Future | None = None
-            current_voxel_task: ImageTask | None = None  # 当前正在体素化的任务
-
-            # 待处理队列
-            pending_voxel_task: ImageTask | None = None  # 等待体素化的任务
-            pending_semantic: tuple[ImageTask, Path] | None = None  # 等待语义 检测
+            prev_task_for_voxel: ImageTask | None = None

             for i, task in enumerate(self._tasks):
                 self.log.info(f"\n{'─' * 40}")
                 self.log.info(f"处理进度: {i+1}/{total}")

-                # 显示当前并行状态
-                parallel_info = []
-                if pending_voxel_task is not None:
-                    parallel_info.append(f"体素化第{pending_voxel_task.index+1}张")
-                if pending_semantic is not None and self.config.enable_semantic:
-                    parallel_info.append(f"语义第{pending_semantic[0].index+1} 张")
-                if parallel_info:
-                    self.log.info(
-                        f"  并行: 推理第{i+1}张 || {' || '.join(parallel_info)}"
-                    )
-                else:
-                    self.log.info(f"  阶段: 推理第{i+1}张")
-
-                # 启动语义检测（如果有待处理的）
-                if pending_semantic is not None and self.config.enable_semantic:
-                    sem_task, sem_voxel_path = pending_semantic
-                    self.log.progress(
-                        f"  启动语义检测: [{sem_task.index+1}] {sem_task.image_path.name}"
-                    )
-                    semantic_future = semantic_executor.submit(
-                        self._semantic_and_navigation,
-                        sem_task,
-                        sem_voxel_path,
-                        navigation_dir,
-                    )
-                    pending_semantic = None
+                # 显示当前阶段的并行状态
+                if i == 0:
+                    self.log.info("  阶段: 推理第1张（无并行）")
+                elif i < total:
+                    self.log.info(f"  阶段: 推理第{i+1}张 || 体素化第{i}张（并 行）")

-                # 启动体素化（如果有待处理的）
-                if pending_voxel_task is not None:
-                    current_voxel_task = pending_voxel_task
+                # 如果有上一张图片需要体素化，启动异步体素化
+                if prev_task_for_voxel is not None:
                     self.log.progress(
-                        f"  启动体素化: [{current_voxel_task.index+1}] {current_voxel_task.image_path.name}"
+                        f"  启动并行体素化: [{prev_task_for_voxel.index+1}] {prev_task_for_voxel.image_path.name}"
                     )
                     voxel_future = voxel_executor.submit(
                         self._voxelize_single,
-                        current_voxel_task,
+                        prev_task_for_voxel,
                         voxel_output_dir,
                         navigation_dir,
                     )
-                    pending_voxel_task = None

                 # 执行当前图片的推理（主线程）
                 predict_success = self._predict_single(task, output_dir)

-                # 等待体素化完成，获取结果用于语义检测
+                # 等待并行的体素化完成（如果有）
                 if voxel_future is not None:
                     try:
-                        voxel_path = voxel_future.result()
-                        if voxel_path is not None and current_voxel_task is not None:
-                            # 将体素化结果加入语义检测队列
-                            pending_semantic = (current_voxel_task, voxel_path)
+                        voxel_future.result()
                     except Exception as e:
                         self.log.error(f"体素化任务异常: {e}")
                     voxel_future = None
-                    current_voxel_task = None

                 # 记录当前任务用于下一轮的体素化
                 if predict_success:
-                    pending_voxel_task = task
+                    prev_task_for_voxel = task
+                else:
+                    prev_task_for_voxel = None

-            # 处理剩余的体素化任务
-            if pending_voxel_task is not None:
+            # 处理最后一张图片的体素化（同步执行，无并行）
+            if prev_task_for_voxel is not None:
                 self.log.info(f"\n{'─' * 40}")
-                self.log.info("收尾阶段: 处理剩余任务")
-
-                self.log.progress(
-                    f"  体素化: [{pending_voxel_task.index+1}] {pending_voxel_task.image_path.name}"
-                )
-                voxel_path = self._voxelize_single(
-                    pending_voxel_task, voxel_output_dir, navigation_dir
-                )
-
-                if voxel_path is not None:
-                    pending_semantic = (pending_voxel_task, voxel_path)
-
-            # 等待最后的语义检测完成
-            if semantic_future is not None:
-                try:
-                    semantic_future.result()
-                except Exception as e:
-                    self.log.error(f"语义检测任务异常: {e}")
-
-            # 处理最后一个语义检测
-            if pending_semantic is not None and self.config.enable_semantic:
-                sem_task, sem_voxel_path = pending_semantic
-                self.log.progress(
-                    f"  语义检测: [{sem_task.index+1}] {sem_task.image_path.name}"
+                self.log.info("最终阶段: 体素化最后一张图片")
+                self._voxelize_single(
+                    prev_task_for_voxel, voxel_output_dir, navigation_dir
                 )
-                self._semantic_and_navigation(sem_task, sem_voxel_path, navigation_dir)


 def run_pipeline(
diff --git a/src/aylm/tools/pointcloud_voxelizer.py b/src/aylm/tools/pointcloud_voxelizer.py
index 7c47fac..aa94351 100644
--- a/src/aylm/tools/pointcloud_voxelizer.py
+++ b/src/aylm/tools/pointcloud_voxelizer.py
@@ -79,62 +79,6 @@ class PointCloud:
         colors = self.colors[mask] if self.colors is not None else None
         return PointCloud(points=self.points[mask], colors=colors)

-    @classmethod
-    def from_ply(cls, path: "Path") -> "PointCloud":
-        """从 PLY 文件加载点云。"""
-        ply = PlyData.read(str(path))
-        vertex = ply["vertex"]
-
-        points = np.column_stack([vertex["x"], vertex["y"], vertex["z"]])
-
-        colors = None
-        if "red" in vertex.data.dtype.names:
-            colors = np.column_stack(
-                [vertex["red"], vertex["green"], vertex["blue"]]
-            ).astype(np.float64)
-            if colors.max() > 1.0:
-                colors = colors / 255.0
-
-        return cls(points=points.astype(np.float64), colors=colors)
-
-    def to_ply(self, path: "Path") -> None:
-        """保存点云到 PLY 文件。"""
-        path = Path(path)
-        path.parent.mkdir(parents=True, exist_ok=True)
-
-        n = len(self.points)
-        if self.colors is not None:
-            colors = self.colors
-            if colors.max() <= 1.0:
-                colors = (colors * 255).astype(np.uint8)
-            else:
-                colors = colors.astype(np.uint8)
-
-            dtype = [
-                ("x", "f4"),
-                ("y", "f4"),
-                ("z", "f4"),
-                ("red", "u1"),
-                ("green", "u1"),
-                ("blue", "u1"),
-            ]
-            data = np.zeros(n, dtype=dtype)
-            data["x"] = self.points[:, 0]
-            data["y"] = self.points[:, 1]
-            data["z"] = self.points[:, 2]
-            data["red"] = colors[:, 0]
-            data["green"] = colors[:, 1]
-            data["blue"] = colors[:, 2]
-        else:
-            dtype = [("x", "f4"), ("y", "f4"), ("z", "f4")]
-            data = np.zeros(n, dtype=dtype)
-            data["x"] = self.points[:, 0]
-            data["y"] = self.points[:, 1]
-            data["z"] = self.points[:, 2]
-
-        vertex = PlyElement.describe(data, "vertex")
-        PlyData([vertex], text=False).write(str(path))
-

 class PointCloudVoxelizer:
     """点云体素化处理器。"""
@@ -156,12 +100,9 @@ class PointCloudVoxelizer:
             return GPU_DEVICE
         elif device_config == "cuda" and torch.cuda.is_available():
             return "cuda"
-        elif (
-            device_config == "mps"
-            and hasattr(torch.backends, "mps")
-            and torch.backends.mps.is_available()
-        ):
-            return "mps"
+        elif device_config == "mps" and hasattr(torch.backends, "mps"):
+            if torch.backends.mps.is_available():
+                return "mps"
         return "cpu"

     def _should_use_gpu(self) -> bool:
@@ -228,7 +169,7 @@ class PointCloudVoxelizer:
         """使用Open3D去除离群点。"""
         pcd = self._to_o3d(pc)

-        _pcd_clean, indices = pcd.remove_statistical_outlier(
+        pcd_clean, indices = pcd.remove_statistical_outlier(
             nb_neighbors=cfg.statistical_nb_neighbors,
             std_ratio=cfg.statistical_std_ratio,
         )
@@ -573,7 +514,7 @@ class PointCloudVoxelizer:
             # 找最佳平面
             best_idx = inliers_count.argmax()
             if inliers_count[best_idx] > best_inlier_count:
-                best_inlier_count = int(inliers_count[best_idx].item())
+                best_inlier_count = inliers_count[best_idx].item()
                 best_inliers_mask = (
                     distances[:, best_idx] < cfg.ransac_distance_threshold
                 )
@@ -621,12 +562,12 @@ class PointCloudVoxelizer:
         for i, key in enumerate(map(tuple, voxel_indices)):
             voxel_dict.setdefault(key, []).append(i)

-        new_points: list[np.ndarray] = []
-        new_colors: list[np.ndarray] | None = [] if pc.colors is not None else None
+        new_points = []
+        new_colors = [] if pc.colors is not None else None

         for indices in voxel_dict.values():
             new_points.append(pc.points[indices].mean(axis=0))
-            if pc.colors is not None and new_colors is not None:
+            if pc.colors is not None:
                 new_colors.append(pc.colors[indices].mean(axis=0))

         points = np.array(new_points)
diff --git a/src/aylm/tools/semantic_fusion.py b/src/aylm/tools/semantic_fusion.py
index adf74ce..f3af4fc 100644
--- a/src/aylm/tools/semantic_fusion.py
+++ b/src/aylm/tools/semantic_fusion.py
@@ -334,7 +334,7 @@ class SemanticFusion:
         colors = semantic_pc.colorize_by_semantic()

         # 体素化：基于密度阈值生成实体方块信息
-        voxel_centers, voxel_colors, _voxel_labels, _, _ = self._create_solid_voxels(
+        voxel_centers, voxel_colors, voxel_labels, _, _ = self._create_solid_voxels(
             robot_points,
             colors,
             semantic_pc.labels,
@@ -499,8 +499,7 @@ class SemanticFusion:
         # 使用 defaultdict 聚合每个体素的点索引
         voxel_dict: dict[tuple[int, int, int], list[int]] = defaultdict(list)
         for i, idx in enumerate(voxel_indices):
-            key: tuple[int, int, int] = (int(idx[0]), int(idx[1]), int(idx[2]))
-            voxel_dict[key].append(i)
+            voxel_dict[tuple(int(x) for x in idx)].append(i)

         # 过滤：只保留点数达到阈值的体素
         solid_voxels = {k: v for k, v in voxel_dict.items() if len(v) >= min_points}
diff --git a/src/aylm/tools/semantic_types.py b/src/aylm/tools/semantic_types.py
index 8b6c669..f706963 100644
--- a/src/aylm/tools/semantic_types.py
+++ b/src/aylm/tools/semantic_types.py
@@ -61,8 +61,6 @@ class Detection2D:
     class_id: int  # COCO 类别 ID
     confidence: float  # 置信度
     semantic_label: SemanticLabel  # 语义标签
-    track_id: Optional[int] = None  # 跟踪 ID（跨帧关联）
-    frame_id: Optional[int] = None  # 帧 ID（时序关联）

     @property
     def area(self) -> float:
@@ -118,11 +116,11 @@ class CameraIntrinsics:
     cy: float  # 主点 y 坐标（像素）

     @classmethod
-    def from_matrix(cls, k: NDArray[np.float64]) -> "CameraIntrinsics":
+    def from_matrix(cls, K: NDArray[np.float64]) -> "CameraIntrinsics":
         """从 3x3 内参矩阵创建。

         Args:
-            k: 3x3 相机内参矩阵
+            K: 3x3 相机内参矩阵
                [[fx,  0, cx],
                 [ 0, fy, cy],
                 [ 0,  0,  1]]
@@ -130,7 +128,7 @@ class CameraIntrinsics:
         Returns:
             CameraIntrinsics 实例
         """
-        return cls(fx=k[0, 0], fy=k[1, 1], cx=k[0, 2], cy=k[1, 2])
+        return cls(fx=K[0, 0], fy=K[1, 1], cx=K[0, 2], cy=K[1, 2])

     @classmethod
     def from_focal_length(
diff --git a/src/aylm/tools/video_config.py b/src/aylm/tools/video_config.py
index af89399..f08218d 100644
--- a/src/aylm/tools/video_config.py
+++ b/src/aylm/tools/video_config.py
@@ -5,7 +5,6 @@

 import logging
 from dataclasses import asdict
-from enum import Enum
 from pathlib import Path
 from typing import Any

@@ -51,7 +50,7 @@ def _validate_range(value: Any, min_val: float, max_val: float, name: str) -> No
         )


-def _validate_enum(value: str, enum_class: type[Enum], name: str) -> None:
+def _validate_enum(value: str, enum_class: type, name: str) -> None:
     """验证枚举值。"""
     valid_values = [e.value for e in enum_class]
     if value not in valid_values:
diff --git a/src/aylm/tools/video_extractor.py b/src/aylm/tools/video_extractor.py
index 9b1e8da..30c2114 100644
--- a/src/aylm/tools/video_extractor.py
+++ b/src/aylm/tools/video_extractor.py
@@ -9,9 +9,10 @@ import logging
 import time
 from concurrent.futures import ThreadPoolExecutor, as_completed
 from pathlib import Path
-from typing import Any, Callable
+from typing import TYPE_CHECKING, Callable

 import cv2
+import numpy as np

 from .video_types import (
     FrameExtractionMethod,
@@ -21,6 +22,9 @@ from .video_types import (
     VideoMetadata,
 )

+if TYPE_CHECKING:
+    from numpy.typing import NDArray
+
 logger = logging.getLogger(__name__)

 ProgressCallback = Callable[[int, int, float], None]
@@ -69,36 +73,31 @@ class VideoExtractor:
         fps, total_frames = metadata.fps, metadata.total_frames

         # 计算帧间隔
-        interval_float: float
         if method == FrameExtractionMethod.INTERVAL:
-            interval_float = float(max(1, int(self.config.frame_interval * fps)))
+            interval = max(1, int(self.config.frame_interval * fps))
         elif (
             method == FrameExtractionMethod.UNIFORM
             and self.config.target_fps
             and self.config.target_fps < fps
         ):
-            interval_float = fps / self.config.target_fps
+            interval = fps / self.config.target_fps
         else:
-            interval_float = float(
-                max(1, int(fps))
-            )  # KEYFRAME 和 SCENE_CHANGE 默认每秒一帧
+            interval = max(1, int(fps))  # KEYFRAME 和 SCENE_CHANGE 默认每秒一 帧

         # 生成帧索引
         if method == FrameExtractionMethod.UNIFORM and (
             not self.config.target_fps or self.config.target_fps >= fps
         ):
             indices = [(i, i / fps) for i in range(total_frames)]
-        elif interval_float != int(interval_float):
-            # 非整数间隔，使用浮点累加
+        elif isinstance(interval, float):
             indices = []
             i = 0.0
             while int(i) < total_frames:
                 frame_idx = int(i)
                 indices.append((frame_idx, frame_idx / fps))
-                i += interval_float
+                i += interval
         else:
-            interval_int = int(interval_float)
-            indices = [(i, i / fps) for i in range(0, total_frames, interval_int)]
+            indices = [(i, i / fps) for i in range(0, total_frames, interval)]

         # 限制最大帧数
         if self.config.max_frames:
@@ -106,7 +105,7 @@ class VideoExtractor:

         return indices

-    def compress_frame(self, frame: Any) -> Any:
+    def compress_frame(self, frame: NDArray[np.uint8]) -> NDArray[np.uint8]:
         """压缩帧图像（调整大小）。"""
         if self.config.resize_width is None and self.config.resize_height is None:
             return frame
@@ -154,13 +153,13 @@ class VideoExtractor:
         try:
             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
             ret, frame = cap.read()
-            if not ret or frame is None:
+            if not ret:
                 logger.warning(f"Cannot read frame {frame_index} from {video_path}")
                 return None

-            compressed = self.compress_frame(frame)
+            frame = self.compress_frame(frame)
             output_path.parent.mkdir(parents=True, exist_ok=True)
-            cv2.imwrite(str(output_path), compressed, self._get_image_write_params())
+            cv2.imwrite(str(output_path), frame, self._get_image_write_params())

             fps = cap.get(cv2.CAP_PROP_FPS)
             return FrameInfo(
@@ -191,13 +190,13 @@ class VideoExtractor:
             for frame_index, timestamp in frame_indices:
                 cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                 ret, frame = cap.read()
-                if not ret or frame is None:
+                if not ret:
                     logger.warning(f"Cannot read frame {frame_index} from {video_path}")
                     continue

-                compressed = self.compress_frame(frame)
+                frame = self.compress_frame(frame)
                 output_path = output_dir / f"frame_{frame_index:06d}.{output_format}"
-                cv2.imwrite(str(output_path), compressed, params)
+                cv2.imwrite(str(output_path), frame, params)

                 frames.append(
                     FrameInfo(
@@ -234,7 +233,7 @@ class VideoExtractor:
         start_time = time.time()
         metadata, output_dir, error = self._prepare_extraction(video_path, output_dir)

-        if error or metadata is None:
+        if error:
             return FrameExtractionResult(
                 video_path=video_path, output_dir=output_dir, error_message=error
             )
@@ -271,7 +270,7 @@ class VideoExtractor:
         start_time = time.time()
         metadata, output_dir, error = self._prepare_extraction(video_path, output_dir)

-        if error or metadata is None:
+        if error:
             return FrameExtractionResult(
                 video_path=video_path, output_dir=output_dir, error_message=error
             )
diff --git a/src/aylm/tools/video_pipeline.py b/src/aylm/tools/video_pipeline.py
index c85f4f9..8caa0c6 100644
--- a/src/aylm/tools/video_pipeline.py
+++ b/src/aylm/tools/video_pipeline.py
@@ -1,11 +1,9 @@
 """视频处理流水线模块。

-实现帧提取与处理的并行流水线：帧提取、SHARP推理、体素化、语义融合、目标跟踪。
+实现帧提取与处理的并行流水线：帧提取、SHARP推理、体素化、语义融合。
 """

-import contextlib
 import gc
-import json
 import logging
 import queue
 import threading
@@ -13,13 +11,9 @@ import time
 from collections.abc import Callable
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Optional

-import numpy as np
 import torch

-from .motion_estimator import MotionEstimator
-from .object_tracker import MultiObjectTracker, TrackedObject, TrackerConfig
 from .pointcloud_voxelizer import PointCloudVoxelizer, VoxelizerConfig
 from .video_config import load_or_create_config
 from .video_extractor import VideoExtractor
@@ -54,24 +48,10 @@ class VideoPipelineConfig:
     # 语义检测配置
     enable_semantic: bool = True  # 是否启用语义检测
     semantic_model: str = "yolo11n-seg.pt"  # YOLO 模型
-    semantic_confidence: float = 0.25  # 检测置信度阈值
+    semantic_confidence: float = 0.5  # 检测置信度阈值
     colorize_semantic: bool = True  # 是否根据语义标签着色
-    # 点云切片配置
-    enable_slice: bool = True  # 是否启用点云切片
-    slice_radius: float = 10.0  # 切片半径（米）
     # 导航输出配置
     output_navigation_ply: bool = True  # 是否输出导航用点云（机器人坐标系）
-    # 输入分辨率配置
-    # 注意：SHARP 模型要求固定 1536，因为内部金字塔结构依赖 1536→768→384
-    internal_resolution: int = 1536  # 内部处理分辨率（固定值，不可更改）
-    # 目标跟踪配置
-    enable_tracking: bool = True  # 是否启用目标跟踪
-    tracker_max_age: int = 30  # 轨迹最大存活帧数
-    tracker_min_hits: int = 3  # 轨迹确认所需最小匹配次数
-    tracker_iou_threshold: float = 0.3  # IoU 匹配阈值
-    # 运动估计配置
-    motion_fps: float = 30.0  # 帧率（用于运动估计）
-    motion_stationary_threshold: float = 0.1  # 静止判定阈值（m/s）


 class VideoPipelineProcessor:
@@ -93,8 +73,6 @@ class VideoPipelineProcessor:
         self._model_loaded = False
         self._voxelizer: PointCloudVoxelizer | None = None
         self._detector = None  # YOLO 语义检测器
-        self._tracker: Optional[MultiObjectTracker] = None  # 目标跟踪器
-        self._motion_estimator: Optional[MotionEstimator] = None  # 运动估计器
         self._frame_queue: queue.Queue[FrameInfo] = queue.Queue(
             maxsize=self.config.frame_queue_size
         )
@@ -107,7 +85,7 @@ class VideoPipelineProcessor:
             return torch.device(self.config.device)
         if torch.cuda.is_available():
             return torch.device("cuda")
-        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
+        if hasattr(torch, "mps") and torch.mps.is_available():
             return torch.device("mps")
         return torch.device("cpu")

@@ -138,11 +116,10 @@ class VideoPipelineProcessor:
                     model_url, progress=True, map_location=self._device
                 )

-            predictor = create_predictor(PredictorParams())
-            predictor.load_state_dict(state_dict)
-            predictor.eval()
-            predictor.to(self._device)
-            self._predictor = predictor
+            self._predictor = create_predictor(PredictorParams())
+            self._predictor.load_state_dict(state_dict)
+            self._predictor.eval()
+            self._predictor.to(self._device)

             self._model_loaded = True
             logger.info("Model loaded successfully")
@@ -159,8 +136,10 @@ class VideoPipelineProcessor:
         logger.info("Unloading model...")
         if self._predictor is not None:
             if self._device and self._device.type != "cpu":
-                with contextlib.suppress(Exception):
+                try:
                     self._predictor.cpu()
+                except Exception:
+                    pass
             del self._predictor
             self._predictor = None
         self._device = None
@@ -201,96 +180,6 @@ class VideoPipelineProcessor:
             del self._detector
             self._detector = None

-    def _load_tracker(self):
-        """加载目标跟踪器和运动估计器。"""
-        logger.info("Initializing object tracker and motion estimator...")
-        tracker_config = TrackerConfig(
-            max_age=self.config.tracker_max_age,
-            min_hits=self.config.tracker_min_hits,
-            iou_threshold=self.config.tracker_iou_threshold,
-        )
-        self._tracker = MultiObjectTracker(config=tracker_config)
-        self._motion_estimator = MotionEstimator(
-            fps=self.config.motion_fps,
-            stationary_threshold=self.config.motion_stationary_threshold,
-        )
-        logger.info(
-            f"Tracker initialized (max_age={self.config.tracker_max_age}, "
-            f"min_hits={self.config.tracker_min_hits})"
-        )
-
-    def _cleanup_tracker(self):
-        """清理跟踪器和运动估计器。"""
-        if self._tracker is not None:
-            logger.info("Cleaning up tracker...")
-            self._tracker.reset()
-            del self._tracker
-            self._tracker = None
-        if self._motion_estimator is not None:
-            self._motion_estimator.clear()
-            del self._motion_estimator
-            self._motion_estimator = None
-
-    def _apply_slice(self, ply_path: Path, frame_stem: str) -> Path | None:
-        """对点云执行半径切片，只保留摄像机附近的点。
-
-        Args:
-            ply_path: 原始 PLY 文件路径
-            frame_stem: 帧文件名（不含扩展名）
-
-        Returns:
-            切片后的临时 PLY 文件路径，失败返回 None
-        """
-        import numpy as np
-
-        try:
-            from plyfile import PlyData, PlyElement
-        except ImportError:
-            logger.warning("plyfile not installed, skipping slice")
-            return None
-
-        logger.info(f"Applying slice (radius: {self.config.slice_radius}m)")
-
-        try:
-            # 读取点云
-            ply_data = PlyData.read(str(ply_path))
-            vertex = ply_data["vertex"]
-
-            # 获取坐标
-            x = np.array(vertex["x"], dtype=np.float64)
-            z = np.array(vertex["z"], dtype=np.float64)
-
-            # 计算水平距离（X-Z平面，以原点为圆心）
-            horizontal_dist = np.sqrt(x**2 + z**2)
-
-            # 筛选在半径内的点
-            mask = horizontal_dist <= self.config.slice_radius
-            kept_count = mask.sum()
-            total_count = len(mask)
-
-            if kept_count == 0:
-                logger.warning("No points within slice radius, skipping slice")
-                return None
-
-            logger.info(
-                f"  Slice: {kept_count}/{total_count} points "
-                f"({100*kept_count/total_count:.1f}%)"
-            )
-
-            # 创建新的顶点数据
-            new_vertex_data = vertex.data[mask]
-            new_vertex = PlyElement.describe(new_vertex_data, "vertex")
-
-            # 保存到临时文件
-            sliced_path = ply_path.parent / f"sliced_{frame_stem}.ply"
-            PlyData([new_vertex], text=False).write(str(sliced_path))
-
-            return sliced_path
-
-        except Exception as e:
-            logger.warning(f"Failed to apply slice: {e}")
-            return None
-
     def _apply_semantic_fusion(
         self,
         frame_path: Path,
@@ -298,10 +187,8 @@ class VideoPipelineProcessor:
         detections_dir: Path,
         navigation_dir: Path | None = None,
         focal_length: float | None = None,
-        frame_id: int = 0,
-        timestamp: float = 0.0,
     ) -> None:
-        """对体素化后的点云应用语义融合和目标跟踪。
+        """对体素化后的点云应用语义融合。

         Args:
             frame_path: 原始帧图像路径
@@ -309,10 +196,9 @@ class VideoPipelineProcessor:
             detections_dir: 检测结果图片保存目录
             navigation_dir: 导航用点云输出目录（机器人坐标系）
             focal_length: SHARP 推理时使用的焦距（像素），如果为 None 则估算
-            frame_id: 帧 ID（用于跟踪）
-            timestamp: 时间戳（秒，用于运动估计）
         """
         import cv2
+        import numpy as np

         from aylm.tools.semantic_fusion import FusionConfig, SemanticFusion
         from aylm.tools.semantic_types import CameraIntrinsics
@@ -331,14 +217,8 @@ class VideoPipelineProcessor:
             height, width = image.shape[:2]

             # 执行目标检测
-            assert self._detector is not None, "Detector not initialized"
             detections = self._detector.detect(image, return_masks=True)
-            logger.info(f"Detected {len(detections)} objects in {frame_path.name}")
-            for det in detections:
-                logger.debug(
-                    f"  - {det.semantic_label.name} (class_id={det.class_id}, "
-                    f"conf={det.confidence:.2f})"
-                )
+            logger.debug(f"Detected {len(detections)} objects")

             # 保存检测结果可视化图片
             if detections:
@@ -348,15 +228,6 @@ class VideoPipelineProcessor:
                 )
                 logger.debug(f"Detection image saved: {detection_image_path.name}")

-            # 目标跟踪（如果启用）
-            tracked_objects: list[TrackedObject] = []
-            if self.config.enable_tracking and self._tracker is not None:
-                tracked_objects = self._tracker.update(detections, frame_id)
-                logger.info(
-                    f"Tracking: {len(tracked_objects)} confirmed tracks "
-                    f"(total: {self._tracker.track_count})"
-                )
-
             if not detections:
                 logger.debug("No detections, skipping semantic fusion")
                 return
@@ -426,22 +297,11 @@ class VideoPipelineProcessor:
                 intrinsics=intrinsics,
             )

-            # 计算运动信息并导出带跟踪信息的 JSON
             if obstacles:
                 json_path = (
                     voxel_ply_path.parent / f"{voxel_ply_path.stem}_obstacles.json"
                 )
-                self._export_obstacles_with_tracking(
-                    obstacles=obstacles,
-                    tracked_objects=tracked_objects,
-                    detections=detections,
-                    points=points,
-                    intrinsics=intrinsics,
-                    image_shape=(height, width),
-                    frame_id=frame_id,
-                    timestamp=timestamp,
-                    output_path=json_path,
-                )
+                marker.export_to_json(obstacles, json_path)
                 logger.debug(f"Obstacles exported: {json_path.name}")

             # 输出导航用点云（机器人坐标系）
@@ -454,152 +314,6 @@ class VideoPipelineProcessor:
             logger.warning(f"Semantic fusion failed: {e}")
             logger.exception(f"Semantic fusion error - {frame_path.name}")

-    def _export_obstacles_with_tracking(
-        self,
-        obstacles: list,
-        tracked_objects: list[TrackedObject],
-        detections: list,
-        points: np.ndarray,
-        intrinsics,
-        image_shape: tuple[int, int],
-        frame_id: int,
-        timestamp: float,
-        output_path: Path,
-    ) -> None:
-        """导出带跟踪和运动信息的障碍物 JSON。
-
-        Args:
-            obstacles: 障碍物列表
-            tracked_objects: 跟踪对象列表
-            detections: 检测结果列表
-            points: 3D 点云
-            intrinsics: 相机内参
-            image_shape: 图像尺寸 (height, width)
-            frame_id: 帧 ID
-            timestamp: 时间戳
-            output_path: 输出路径
-        """
-        from aylm.tools.coordinate_utils import opencv_to_robot
-        from aylm.tools.obstacle_marker import MOVABLE_LABELS
-
-        # 建立检测到跟踪对象的映射（基于 IoU）
-        det_to_track: dict[int, TrackedObject] = {}
-        if tracked_objects:
-            for det_idx, det in enumerate(detections):
-                best_iou = 0.0
-                best_track = None
-                for track in tracked_objects:
-                    iou = self._compute_iou(det.bbox, track.bbox)
-                    if iou > best_iou and iou > 0.3:
-                        best_iou = iou
-                        best_track = track
-                if best_track is not None:
-                    det_to_track[det_idx] = best_track
-
-        # 构建带运动信息的障碍物数据
-        obstacles_data = []
-        for obs_idx, obs in enumerate(obstacles):
-            obs_dict = obs.to_dict()
-
-            # 查找对应的跟踪对象
-            track = det_to_track.get(obs_idx)
-            if track is not None:
-                obs_dict["track_id"] = track.track_id
-                obs_dict["track_age"] = track.age
-                obs_dict["track_hits"] = track.hits
-
-                # 计算 3D 中心点并更新运动估计
-                center_cv = np.array(obs.center, dtype=np.float32)
-                if self._motion_estimator is not None:
-                    motion = self._motion_estimator.update(
-                        track_id=track.track_id,
-                        position_cv=center_cv,
-                        frame_id=frame_id,
-                        timestamp=timestamp,
-                    )
-                    if motion is not None:
-                        obs_dict["motion"] = {
-                            "velocity_cv": motion.velocity_cv.tolist(),
-                            "velocity_robot": motion.velocity_robot.tolist(),
-                            "speed": motion.speed,
-                            "heading": motion.heading,
-                            "is_stationary": motion.is_stationary,
-                        }
-                        logger.debug(
-                            f"Track {track.track_id}: speed={motion.speed:.2f}m/s, "
-                            f"stationary={motion.is_stationary}"
-                        )
-
-                        # 预测未来位置（1秒后）
-                        predicted_pos = self._motion_estimator.predict(
-                            track.track_id, dt=1.0
-                        )
-                        if predicted_pos is not None:
-                            predicted_robot = opencv_to_robot(predicted_pos)
-                            obs_dict["predicted_position_1s"] = {
-                                "cv": predicted_pos.tolist(),
-                                "robot": predicted_robot.tolist(),
-                            }
-            else:
-                obs_dict["track_id"] = None
-
-            obstacles_data.append(obs_dict)
-
-        # 统计
-        movable_count = sum(1 for obs in obstacles if obs.label in MOVABLE_LABELS)
-        static_count = len(obstacles) - movable_count
-        tracked_count = sum(1 for d in obstacles_data if d.get("track_id") is not None)
-
-        data = {
-            "frame_id": frame_id,
-            "timestamp": timestamp,
-            "coordinate_systems": {
-                "cv": {
-                    "description": "OpenCV/相机坐标系",
-                    "axes": "X右, Y下, Z前",
-                },
-                "robot": {
-                    "description": "机器人/ROS坐标系",
-                    "axes": "X前, Y左, Z上",
-                },
-                "transform": "X_robot=Z_cv, Y_robot=-X_cv, Z_robot=-Y_cv",
-            },
-            "total_count": len(obstacles),
-            "movable_count": movable_count,
-            "static_count": static_count,
-            "tracked_count": tracked_count,
-            "obstacles": obstacles_data,
-        }
-
-        with open(output_path, "w", encoding="utf-8") as f:
-            json.dump(data, f, indent=2, ensure_ascii=False)
-
-        logger.info(
-            f"Exported {len(obstacles)} obstacles ({tracked_count} tracked) "
-            f"to {output_path.name}"
-        )
-
-    @staticmethod
-    def _compute_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
-        """计算两个边界框的 IoU。"""
-        x1 = max(bbox1[0], bbox2[0])
-        y1 = max(bbox1[1], bbox2[1])
-        x2 = min(bbox1[2], bbox2[2])
-        y2 = min(bbox1[3], bbox2[3])
-
-        if x2 <= x1 or y2 <= y1:
-            return 0.0
-
-        intersection = (x2 - x1) * (y2 - y1)
-        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
-        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
-        union = area1 + area2 - intersection
-
-        if union <= 0:
-            return 0.0
-
-        return float(intersection / union)
-
     def _save_navigation_ply(
         self,
         semantic_pc,
@@ -651,7 +365,7 @@ class VideoPipelineProcessor:
             )

             # 逐帧提取并放入队列
-            for frame_index, _timestamp in frame_indices:
+            for frame_index, timestamp in frame_indices:
                 if self._stop_event.is_set():
                     break
                 output_format = video_config.output_format.lower()
@@ -675,9 +389,8 @@ class VideoPipelineProcessor:
         voxel_output_dir: Path,
         detections_dir: Path | None = None,
         navigation_dir: Path | None = None,
-        frame_id: int = 0,
     ) -> bool:
-        """处理单帧：推理 + 体素化 + 语义融合 + 跟踪（可选）。
+        """处理单帧：推理 + 体素化 + 语义融合（可选）。

         Args:
             frame_info: 帧信息
@@ -685,14 +398,13 @@ class VideoPipelineProcessor:
             voxel_output_dir: 体素化输出目录
             detections_dir: 检测结果图片目录
             navigation_dir: 导航用点云输出目录（机器人坐标系）
-            frame_id: 帧 ID（用于跟踪）

         Returns:
             是否处理成功
         """
+        import torch.nn.functional as F
         from sharp.utils import io
         from sharp.utils.gaussians import save_ply, unproject_gaussians
-        from torch.nn import functional as functional_nn

         frame_path = frame_info.output_path
         if frame_path is None or not frame_path.exists():
@@ -703,16 +415,15 @@ class VideoPipelineProcessor:
             image, _, f_px = io.load_rgb(frame_path)
             height, width = image.shape[:2]

-            # 预处理（使用配置的内部分辨率）
-            res = self.config.internal_resolution
-            internal_shape = (res, res)
+            # 预处理
+            internal_shape = (1536, 1536)
             image_pt = (
                 torch.from_numpy(image.copy()).float().to(self._device).permute(2, 0, 1)
                 / 255.0
             )
             disparity_factor = torch.tensor([f_px / width]).float().to(self._device)

-            image_resized_pt = functional_nn.interpolate(
+            image_resized_pt = F.interpolate(
                 image_pt[None],
                 size=(internal_shape[1], internal_shape[0]),
                 mode="bilinear",
@@ -720,7 +431,6 @@ class VideoPipelineProcessor:
             )

             # 推理
-            assert self._predictor is not None, "Model not loaded"
             gaussians_ndc = self._predictor(image_resized_pt, disparity_factor)

             # 后处理
@@ -752,44 +462,23 @@ class VideoPipelineProcessor:
             ply_path = ply_output_dir / f"{frame_path.stem}.ply"
             save_ply(gaussians, f_px, (height, width), ply_path)

-            # 切片（如果启用）
-            input_ply_path = ply_path
-            if self.config.enable_slice:
-                sliced_path = self._apply_slice(ply_path, frame_path.stem)
-                if sliced_path is not None:
-                    input_ply_path = sliced_path
-
             # 体素化
             voxel_path = voxel_output_dir / f"vox_{frame_path.stem}.ply"
-            assert self._voxelizer is not None, "Voxelizer not initialized"
             self._voxelizer.process(
-                input_ply_path,
+                ply_path,
                 voxel_path,
                 remove_ground=self.config.remove_ground,
                 transform_coords=self.config.transform_coords,
             )

-            # 清理临时切片文件
-            if self.config.enable_slice and input_ply_path != ply_path:
-                with contextlib.suppress(Exception):
-                    input_ply_path.unlink()
-
             # 语义融合（如果启用）
             if self.config.enable_semantic and self._detector is not None:
-                # 计算时间戳
-                timestamp = (
-                    frame_info.timestamp
-                    if frame_info.timestamp
-                    else (frame_id / self.config.motion_fps)
-                )
                 self._apply_semantic_fusion(
                     frame_path,
                     voxel_path,
                     detections_dir or voxel_output_dir,
                     navigation_dir=navigation_dir,
                     focal_length=f_px,  # 传递 SHARP 的精确焦距
-                    frame_id=frame_id,
-                    timestamp=timestamp,
                 )

             return True
@@ -866,12 +555,6 @@ class VideoPipelineProcessor:
         if self.config.enable_semantic:
             self._load_detector()

-        # 加载跟踪器（如果启用）
-        if self.config.enable_tracking:
-            self._load_tracker()
-            if self.config.verbose:
-                logger.info("Object tracking: ENABLED")
-
         self._stop_event.clear()
         self._extraction_done.clear()
         self._frame_queue = queue.Queue(maxsize=self.config.frame_queue_size)
@@ -899,7 +582,6 @@ class VideoPipelineProcessor:
                     voxel_dir,
                     detections_dir,
                     navigation_dir,
-                    frame_id=processed_count,
                 )
                 process_time = time.time() - process_start

@@ -909,14 +591,9 @@ class VideoPipelineProcessor:
                     self.stats.total_processing_time += process_time

                     if self.config.verbose:
-                        output_name = (
-                            frame_info.output_path.name
-                            if frame_info.output_path
-                            else f"frame_{frame_info.index}"
-                        )
                         logger.info(
                             f"Processed frame {processed_count}: "
-                            f"{output_name} ({process_time:.2f}s)"
+                            f"{frame_info.output_path.name} ({process_time:.2f}s)"
                         )

                     if progress_callback:
@@ -936,7 +613,6 @@ class VideoPipelineProcessor:
         if self.config.auto_unload:
             self._unload_model()
             self._cleanup_detector()
-            self._cleanup_tracker()

         if self.config.verbose:
             logger.info("=" * 50)
@@ -949,8 +625,6 @@ class VideoPipelineProcessor:
             logger.info(f"  FPS: {self.stats.frames_per_second:.2f}")
             if self.config.enable_semantic:
                 logger.info("  Semantic detection: ENABLED")
-            if self.config.enable_tracking:
-                logger.info("  Object tracking: ENABLED")
             if navigation_dir:
                 logger.info(f"  Navigation PLY: {navigation_dir}")
             logger.info("=" * 50)
@@ -962,7 +636,6 @@ class VideoPipelineProcessor:
         self._stop_event.set()
         self._unload_model()
         self._cleanup_detector()
-        self._cleanup_tracker()
         self._voxelizer = None


@@ -978,7 +651,6 @@ def process_video(
     semantic_model: str = "yolo11n-seg.pt",
     semantic_confidence: float = 0.5,
     output_navigation_ply: bool = True,
-    enable_tracking: bool = True,
 ) -> VideoProcessingStats:
     """便捷函数：处理视频。

@@ -994,7 +666,6 @@ def process_video(
         semantic_model: YOLO 模型名称
         semantic_confidence: 检测置信度阈值
         output_navigation_ply: 是否输出导航用点云（机器人坐标系）
-        enable_tracking: 是否启用目标跟踪

     Returns:
         VideoProcessingStats: 处理统计
@@ -1005,7 +676,6 @@ def process_video(
         ...     video_path="video.mp4",
         ...     output_dir="output/",
         ...     enable_semantic=True,
-        ...     enable_tracking=True,
         ...     output_navigation_ply=True,
         ... )
         >>> print(f"Processed {stats.total_frames_processed} frames")
@@ -1019,7 +689,6 @@ def process_video(
         semantic_model=semantic_model,
         semantic_confidence=semantic_confidence,
         output_navigation_ply=output_navigation_ply,
-        enable_tracking=enable_tracking,
     )

     processor = VideoPipelineProcessor(config)
diff --git a/src/aylm/tools/voxel_player.py b/src/aylm/tools/voxel_player.py
index a32a3a4..aa1357f 100644
--- a/src/aylm/tools/voxel_player.py
+++ b/src/aylm/tools/voxel_player.py
@@ -15,10 +15,13 @@ import time
 from dataclasses import dataclass
 from enum import Enum
 from pathlib import Path
-from typing import Any, Callable
+from typing import TYPE_CHECKING, Any, Callable

 import numpy as np

+if TYPE_CHECKING:
+    import open3d as o3d
+
 logger = logging.getLogger(__name__)

 # 尝试导入Open3D
@@ -34,7 +37,7 @@ except ImportError:
 # 尝试导入matplotlib作为fallback
 try:
     import matplotlib.pyplot as plt
-    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - required for 3d projection
+    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

     HAS_MATPLOTLIB = True
 except ImportError:
@@ -301,7 +304,7 @@ class VoxelPlayer:
                 time.sleep(sleep_time)

     # 键盘回调
-    def _on_space(self, _vis):
+    def _on_space(self, vis):
         """空格键：播放/暂停。"""
         if self._state == PlaybackState.PLAYING:
             self._state = PlaybackState.PAUSED
@@ -313,50 +316,50 @@ class VoxelPlayer:
             logger.info("Playing")
         return False

-    def _on_right(self, _vis):
+    def _on_right(self, vis):
         """右箭头：下一帧。"""
         if self._current_index < len(self._ply_files) - 1:
             self._update_frame(self._current_index + 1)
         return False

-    def _on_left(self, _vis):
+    def _on_left(self, vis):
         """左箭头：上一帧。"""
         if self._current_index > 0:
             self._update_frame(self._current_index - 1)
         return False

-    def _on_up(self, _vis):
+    def _on_up(self, vis):
         """上箭头：加速。"""
         self._fps = min(60.0, self._fps * 1.25)
         logger.info(f"FPS: {self._fps:.1f}")
         return False

-    def _on_down(self, _vis):
+    def _on_down(self, vis):
         """下箭头：减速。"""
         self._fps = max(1.0, self._fps / 1.25)
         logger.info(f"FPS: {self._fps:.1f}")
         return False

-    def _on_reset(self, _vis):
+    def _on_reset(self, vis):
         """R键：重置到开头。"""
         self._update_frame(0)
         self._start_time = None
         logger.info("Reset to beginning")
         return False

-    def _on_loop_toggle(self, _vis):
+    def _on_loop_toggle(self, vis):
         """L键：切换循环模式。"""
         self._loop = not self._loop
         logger.info(f"Loop: {'ON' if self._loop else 'OFF'}")
         return False

-    def _on_center(self, _vis):
+    def _on_center(self, vis):
         """C键：重置视角。"""
         if self._vis is not None:
             self._vis.reset_view_point(True)
         return False

-    def _on_quit(self, _vis):
+    def _on_quit(self, vis):
         """Q键/Esc：退出。"""
         self._stop_event.set()
         self._state = PlaybackState.STOPPED
@@ -494,11 +497,11 @@ class VoxelPlayer:
         logger.info(f"Exporting video to {output_path}")
         logger.info(f"  Resolution: {width}x{height}, FPS: {fps}")

-        fourcc = cv2.VideoWriter_fourcc(*"mp4v")  # type: ignore[attr-defined]
+        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
         writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

         try:
-            for i, _ply_path in enumerate(self._ply_files):
+            for i, ply_path in enumerate(self._ply_files):
                 # 使用matplotlib渲染帧
                 pcd = self._load_frame(i)
                 if pcd is None:
@@ -534,10 +537,8 @@ class VoxelPlayer:
                 ax.set_title(f"Frame {i + 1}/{len(self._ply_files)}")

                 fig.canvas.draw()
-                img = np.frombuffer(
-                    fig.canvas.tostring_rgb(), dtype=np.uint8  # type: ignore[attr-defined]
-                )
-                img = img.reshape((*fig.canvas.get_width_height()[::-1], 3))
+                img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
+                img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))
                 img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

                 writer.write(img_bgr)
@@ -709,7 +710,6 @@ def play_sequence(
         >>> play_sequence("output/voxelized", fps=15, loop=True)
     """
     config = PlayerConfig(fps=fps, loop=loop)
-    player: VoxelPlayer | MatplotlibVoxelPlayer

     if HAS_OPEN3D:
         player = VoxelPlayer(config)
diff --git a/tests/unit/test_motion_estimator.py b/tests/unit/test_motion_estimator.py
deleted file mode 100644
index 8fb1810..0000000
--- a/tests/unit/test_motion_estimator.py
+++ /dev/null
@@ -1,378 +0,0 @@
-"""运动矢量估计器模块测试。"""
-
-import numpy as np
-import pytest
-
-from aylm.tools.motion_estimator import (
-    KalmanConfig,
-    MotionEstimator,
-    MotionVector,
-    TrackedObject3D,
-    create_tracked_object,
-)
-from aylm.tools.semantic_types import SemanticLabel
-
-
-class TestMotionVector:
-    """MotionVector 数据类测试。"""
-
-    def test_creation(self):
-        """测试创建 MotionVector。"""
-        velocity_cv = np.array([1.0, 0.0, 2.0], dtype=np.float32)
-        velocity_robot = np.array([2.0, -1.0, 0.0], dtype=np.float32)
-
-        mv = MotionVector(
-            velocity_cv=velocity_cv,
-            velocity_robot=velocity_robot,
-            speed=2.236,
-            heading=0.5,
-            is_stationary=False,
-        )
-
-        np.testing.assert_array_equal(mv.velocity_cv, velocity_cv)
-        np.testing.assert_array_equal(mv.velocity_robot, velocity_robot)
-        assert mv.speed == pytest.approx(2.236)
-        assert mv.heading == pytest.approx(0.5)
-        assert mv.is_stationary is False
-
-    def test_stationary_flag(self):
-        """测试静止标志。"""
-        mv = MotionVector(
-            velocity_cv=np.array([0.01, 0.01, 0.01], dtype=np.float32),
-            velocity_robot=np.array([0.01, -0.01, -0.01], dtype=np.float32),
-            speed=0.017,
-            heading=0.0,
-            is_stationary=True,
-        )
-        assert mv.is_stationary is True
-
-
-class TestKalmanConfig:
-    """KalmanConfig 数据类测试。"""
-
-    def test_default_values(self):
-        """测试默认配置值。"""
-        config = KalmanConfig()
-        assert config.process_noise == 0.1
-        assert config.measurement_noise == 0.5
-        assert config.initial_covariance == 1.0
-
-    def test_custom_values(self):
-        """测试自定义配置值。"""
-        config = KalmanConfig(
-            process_noise=0.2,
-            measurement_noise=0.3,
-            initial_covariance=2.0,
-        )
-        assert config.process_noise == 0.2
-        assert config.measurement_noise == 0.3
-        assert config.initial_covariance == 2.0
-
-
-class TestMotionEstimator:
-    """MotionEstimator 类测试。"""
-
-    def test_init_default(self):
-        """测试默认初始化。"""
-        estimator = MotionEstimator()
-        assert estimator.fps == 30.0
-        assert estimator.stationary_threshold == 0.1
-        assert len(estimator.active_tracks) == 0
-
-    def test_init_custom(self):
-        """测试自定义初始化。"""
-        config = KalmanConfig(process_noise=0.2)
-        estimator = MotionEstimator(
-            fps=60.0,
-            stationary_threshold=0.2,
-            kalman_config=config,
-        )
-        assert estimator.fps == 60.0
-        assert estimator.stationary_threshold == 0.2
-        assert estimator.config.process_noise == 0.2
-
-    def test_first_update_returns_none(self):
-        """测试首次更新返回 None（无速度信息）。"""
-        estimator = MotionEstimator()
-        position = np.array([1.0, 2.0, 3.0])
-
-        result = estimator.update(track_id=1, position_cv=position, frame_id=0)
-
-        assert result is None
-        assert 1 in estimator.active_tracks
-
-    def test_second_update_returns_motion(self):
-        """测试第二次更新返回运动矢量。"""
-        estimator = MotionEstimator(fps=30.0)
-
-        # 第一帧
-        pos1 = np.array([0.0, 0.0, 0.0])
-        estimator.update(track_id=1, position_cv=pos1, frame_id=0)
-
-        # 第二帧，移动了 1 米（在 Z 方向）
-        pos2 = np.array([0.0, 0.0, 1.0])
-        result = estimator.update(track_id=1, position_cv=pos2, frame_id=1)
-
-        assert result is not None
-        assert isinstance(result, MotionVector)
-        # 速度应该大于 0
-        assert result.speed > 0
-
-    def test_velocity_calculation(self):
-        """测试速度计算。"""
-        estimator = MotionEstimator(fps=1.0)  # 1 FPS 便于计算
-
-        # 第一帧
-        pos1 = np.array([0.0, 0.0, 0.0])
-        estimator.update(track_id=1, position_cv=pos1, frame_id=0, timestamp=0.0)
-
-        # 第二帧，1 秒后移动 1 米
-        pos2 = np.array([1.0, 0.0, 0.0])
-        result = estimator.update(
-            track_id=1, position_cv=pos2, frame_id=1, timestamp=1.0
-        )
-
-        assert result is not None
-        # Kalman 滤波会平滑速度，但应该接近 1 m/s
-        assert result.speed > 0.5
-
-    def test_stationary_detection(self):
-        """测试静止检测。"""
-        estimator = MotionEstimator(fps=30.0, stationary_threshold=0.1)
-
-        # 第一帧
-        pos1 = np.array([0.0, 0.0, 5.0])
-        estimator.update(track_id=1, position_cv=pos1, frame_id=0)
-
-        # 第二帧，几乎不动
-        pos2 = np.array([0.001, 0.001, 5.001])
-        result = estimator.update(track_id=1, position_cv=pos2, frame_id=1)
-
-        assert result is not None
-        # 速度很小，应该被判定为静止
-        assert result.is_stationary is True
-
-    def test_moving_detection(self):
-        """测试运动检测。"""
-        estimator = MotionEstimator(fps=1.0, stationary_threshold=0.1)
-
-        # 第一帧
-        pos1 = np.array([0.0, 0.0, 0.0])
-        estimator.update(track_id=1, position_cv=pos1, frame_id=0, timestamp=0.0)
-
-        # 第二帧，明显移动
-        pos2 = np.array([2.0, 0.0, 0.0])
-        result = estimator.update(
-            track_id=1, position_cv=pos2, frame_id=1, timestamp=1.0
-        )
-
-        assert result is not None
-        assert result.is_stationary is False
-
-    def test_predict_future_position(self):
-        """测试位置预测。"""
-        estimator = MotionEstimator(fps=1.0)
-
-        # 建立轨迹
-        estimator.update(
-            track_id=1, position_cv=np.array([0.0, 0.0, 0.0]), frame_id=0, timestamp=0.0
-        )
-        estimator.update(
-            track_id=1, position_cv=np.array([1.0, 0.0, 0.0]), frame_id=1, timestamp=1.0
-        )
-
-        # 预测 1 秒后的位置
-        predicted = estimator.predict(track_id=1, dt=1.0)
-
-        assert predicted is not None
-        # 应该在 X 方向继续移动
-        assert predicted[0] > 1.0
-
-    def test_predict_nonexistent_track(self):
-        """测试预测不存在的轨迹。"""
-        estimator = MotionEstimator()
-        result = estimator.predict(track_id=999, dt=1.0)
-        assert result is None
-
-    def test_get_velocity(self):
-        """测试获取速度。"""
-        estimator = MotionEstimator(fps=1.0)
-
-        # 建立轨迹
-        estimator.update(
-            track_id=1, position_cv=np.array([0.0, 0.0, 0.0]), frame_id=0, timestamp=0.0
-        )
-        estimator.update(
-            track_id=1, position_cv=np.array([1.0, 0.0, 0.0]), frame_id=1, timestamp=1.0
-        )
-
-        velocity = estimator.get_velocity(track_id=1)
-
-        assert velocity is not None
-        assert len(velocity) == 3
-
-    def test_get_velocity_nonexistent_track(self):
-        """测试获取不存在轨迹的速度。"""
-        estimator = MotionEstimator()
-        result = estimator.get_velocity(track_id=999)
-        assert result is None
-
-    def test_multiple_tracks(self):
-        """测试多目标同时跟踪。"""
-        estimator = MotionEstimator()
-
-        # 跟踪两个目标
-        estimator.update(track_id=1, position_cv=np.array([0.0, 0.0, 0.0]), frame_id=0)
-        estimator.update(
-            track_id=2, position_cv=np.array([10.0, 10.0, 10.0]), frame_id=0
-        )
-
-        assert len(estimator.active_tracks) == 2
-        assert 1 in estimator.active_tracks
-        assert 2 in estimator.active_tracks
-
-    def test_remove_track(self):
-        """测试移除轨迹。"""
-        estimator = MotionEstimator()
-
-        estimator.update(track_id=1, position_cv=np.array([0.0, 0.0, 0.0]), frame_id=0)
-        assert 1 in estimator.active_tracks
-
-        result = estimator.remove_track(track_id=1)
-        assert result is True
-        assert 1 not in estimator.active_tracks
-
-    def test_remove_nonexistent_track(self):
-        """测试移除不存在的轨迹。"""
-        estimator = MotionEstimator()
-        result = estimator.remove_track(track_id=999)
-        assert result is False
-
-    def test_clear(self):
-        """测试清除所有轨迹。"""
-        estimator = MotionEstimator()
-
-        # 添加多个轨迹
-        for i in range(5):
-            estimator.update(
-                track_id=i, position_cv=np.array([float(i), 0.0, 0.0]), frame_id=0
-            )
-
-        assert len(estimator.active_tracks) == 5
-
-        estimator.clear()
-        assert len(estimator.active_tracks) == 0
-
-    def test_heading_calculation(self):
-        """测试航向角计算。"""
-        estimator = MotionEstimator(fps=1.0)
-
-        # 第一帧
-        estimator.update(
-            track_id=1, position_cv=np.array([0.0, 0.0, 0.0]), frame_id=0, timestamp=0.0
-        )
-
-        # 第二帧，沿 Z 轴正方向移动（OpenCV 坐标系）
-        # 在机器人坐标系中，这对应 X 轴正方向（前进）
-        result = estimator.update(
-            track_id=1, position_cv=np.array([0.0, 0.0, 5.0]), frame_id=1, timestamp=1.0
-        )
-
-        assert result is not None
-        # 航向角应该接近 0（正前方）
-        assert abs(result.heading) < 1.0  # 允许一定误差
-
-    def test_kalman_smoothing(self):
-        """测试 Kalman 滤波平滑效果。"""
-        estimator = MotionEstimator(fps=10.0)
-
-        # 模拟带噪声的轨迹
-        np.random.seed(42)
-        base_positions = [np.array([float(i), 0.0, 0.0]) for i in range(10)]
-        noisy_positions = [p + np.random.normal(0, 0.1, 3) for p in base_positions]
-
-        results = []
-        for i, pos in enumerate(noisy_positions):
-            result = estimator.update(track_id=1, position_cv=pos, frame_id=i)
-            if result is not None:
-                results.append(result)
-
-        # 速度应该相对稳定（Kalman 滤波平滑了噪声）
-        speeds = [r.speed for r in results]
-        speed_std = np.std(speeds)
-
-        # 标准差应该较小（平滑效果）
-        assert speed_std < 5.0  # 合理的阈值
-
-
-class TestTrackedObject3D:
-    """TrackedObject3D 数据类测试。"""
-
-    def test_creation(self):
-        """测试创建 TrackedObject3D。"""
-        obj = TrackedObject3D(
-            track_id=1,
-            center_cv=np.array([1.0, 2.0, 3.0], dtype=np.float32),
-            center_robot=np.array([3.0, -1.0, -2.0], dtype=np.float32),
-            dimensions=np.array([0.5, 1.7, 0.4], dtype=np.float32),
-            motion=None,
-            semantic_label=SemanticLabel.PERSON,
-            confidence=0.95,
-            frame_id=10,
-            timestamp=0.333,
-        )
-
-        assert obj.track_id == 1
-        assert obj.semantic_label == SemanticLabel.PERSON
-        assert obj.confidence == 0.95
-        assert obj.frame_id == 10
-        assert obj.timestamp == pytest.approx(0.333)
-        assert obj.motion is None
-
-
-class TestCreateTrackedObject:
-    """create_tracked_object 辅助函数测试。"""
-
-    def test_basic_creation(self):
-        """测试基本创建。"""
-        obj = create_tracked_object(
-            track_id=1,
-            center_cv=np.array([0.0, 0.0, 5.0]),
-            dimensions=np.array([0.6, 1.7, 0.4]),
-            semantic_label=SemanticLabel.PERSON,
-            confidence=0.9,
-            frame_id=0,
-            timestamp=0.0,
-        )
-
-        assert obj.track_id == 1
-        assert obj.semantic_label == SemanticLabel.PERSON
-        assert obj.confidence == 0.9
-        # 验证坐标系转换
-        assert obj.center_cv is not None
-        assert obj.center_robot is not None
-
-    def test_with_motion(self):
-        """测试带运动信息的创建。"""
-        motion = MotionVector(
-            velocity_cv=np.array([0.0, 0.0, 1.0], dtype=np.float32),
-            velocity_robot=np.array([1.0, 0.0, 0.0], dtype=np.float32),
-            speed=1.0,
-            heading=0.0,
-            is_stationary=False,
-        )
-
-        obj = create_tracked_object(
-            track_id=1,
-            center_cv=np.array([0.0, 0.0, 5.0]),
-            dimensions=np.array([0.6, 1.7, 0.4]),
-            semantic_label=SemanticLabel.VEHICLE,
-            confidence=0.85,
-            frame_id=10,
-            timestamp=0.333,
-            motion=motion,
-        )
-
-        assert obj.motion is not None
-        assert obj.motion.speed == 1.0
-        assert obj.motion.is_stationary is False
diff --git a/tests/unit/test_object_tracker.py b/tests/unit/test_object_tracker.py
deleted file mode 100644
index 5aca646..0000000
--- a/tests/unit/test_object_tracker.py
+++ /dev/null
@@ -1,311 +0,0 @@
-"""多目标跟踪器模块测试。"""
-
-import numpy as np
-import pytest
-
-from aylm.tools.object_tracker import (
-    MultiObjectTracker,
-    TrackedObject,
-    TrackerConfig,
-    _compute_iou,
-)
-from aylm.tools.semantic_types import Detection2D, SemanticLabel
-
-
-class TestComputeIoU:
-    """IoU 计算测试。"""
-
-    def test_identical_boxes(self):
-        """测试完全重叠的边界框。"""
-        bbox = np.array([0, 0, 100, 100], dtype=np.float32)
-        iou = _compute_iou(bbox, bbox)
-        assert iou == pytest.approx(1.0)
-
-    def test_no_overlap(self):
-        """测试无重叠的边界框。"""
-        bbox1 = np.array([0, 0, 50, 50], dtype=np.float32)
-        bbox2 = np.array([100, 100, 150, 150], dtype=np.float32)
-        iou = _compute_iou(bbox1, bbox2)
-        assert iou == 0.0
-
-    def test_partial_overlap(self):
-        """测试部分重叠的边界框。"""
-        bbox1 = np.array([0, 0, 100, 100], dtype=np.float32)
-        bbox2 = np.array([50, 50, 150, 150], dtype=np.float32)
-        # 交集: 50x50 = 2500
-        # 并集: 10000 + 10000 - 2500 = 17500
-        # IoU = 2500 / 17500 ≈ 0.143
-        iou = _compute_iou(bbox1, bbox2)
-        assert iou == pytest.approx(2500 / 17500, rel=0.01)
-
-    def test_one_inside_other(self):
-        """测试一个边界框完全在另一个内部。"""
-        bbox1 = np.array([0, 0, 100, 100], dtype=np.float32)
-        bbox2 = np.array([25, 25, 75, 75], dtype=np.float32)
-        # 交集: 50x50 = 2500
-        # 并集: 10000 (大框面积)
-        # IoU = 2500 / 10000 = 0.25
-        iou = _compute_iou(bbox1, bbox2)
-        assert iou == pytest.approx(0.25, rel=0.01)
-
-
-class TestTrackedObject:
-    """TrackedObject 数据类测试。"""
-
-    def test_creation(self):
-        """测试创建 TrackedObject。"""
-        bbox = np.array([10, 20, 110, 120], dtype=np.float32)
-        obj = TrackedObject(
-            track_id=1,
-            bbox=bbox,
-            class_id=0,
-            confidence=0.9,
-            age=5,
-            hits=3,
-            time_since_update=0,
-        )
-        assert obj.track_id == 1
-        assert obj.class_id == 0
-        assert obj.confidence == 0.9
-        assert obj.age == 5
-        assert obj.hits == 3
-        np.testing.assert_array_equal(obj.bbox, bbox)
-
-    def test_bbox_type_conversion(self):
-        """测试 bbox 类型自动转换。"""
-        # 使用列表创建
-        obj = TrackedObject(
-            track_id=1,
-            bbox=[10, 20, 110, 120],  # type: ignore
-            class_id=0,
-            confidence=0.9,
-        )
-        assert obj.bbox.dtype == np.float32
-        assert isinstance(obj.bbox, np.ndarray)
-
-
-class TestMultiObjectTracker:
-    """MultiObjectTracker 类测试。"""
-
-    def _create_detection(
-        self,
-        bbox: list[float],
-        class_id: int = 0,
-        confidence: float = 0.9,
-    ) -> Detection2D:
-        """创建测试用检测结果。"""
-        return Detection2D(
-            bbox=np.array(bbox, dtype=np.float32),
-            mask=None,
-            class_id=class_id,
-            confidence=confidence,
-            semantic_label=SemanticLabel.PERSON,
-        )
-
-    def test_init_default_config(self):
-        """测试默认配置初始化。"""
-        tracker = MultiObjectTracker()
-        assert tracker.config.max_age == 30
-        assert tracker.config.min_hits == 3
-        assert tracker.config.iou_threshold == 0.3
-
-    def test_init_custom_config(self):
-        """测试自定义配置初始化。"""
-        config = TrackerConfig(max_age=10, min_hits=2, iou_threshold=0.5)
-        tracker = MultiObjectTracker(config=config)
-        assert tracker.config.max_age == 10
-        assert tracker.config.min_hits == 2
-        assert tracker.config.iou_threshold == 0.5
-
-    def test_init_with_params(self):
-        """测试使用参数初始化。"""
-        tracker = MultiObjectTracker(max_age=20, min_hits=5, iou_threshold=0.4)
-        assert tracker.config.max_age == 20
-        assert tracker.config.min_hits == 5
-        assert tracker.config.iou_threshold == 0.4
-
-    def test_first_frame_creates_tracks(self):
-        """测试第一帧创建轨迹。"""
-        tracker = MultiObjectTracker(min_hits=1)
-        detections = [
-            self._create_detection([0, 0, 50, 50]),
-            self._create_detection([100, 100, 150, 150]),
-        ]
-
-        tracked = tracker.update(detections, frame_id=0)
-
-        # min_hits=1，第一帧就应该返回确认轨迹
-        assert tracker.track_count == 2
-        assert len(tracked) == 2
-
-    def test_track_id_assignment(self):
-        """测试 track_id 分配。"""
-        tracker = MultiObjectTracker(min_hits=1)
-        detections = [
-            self._create_detection([0, 0, 50, 50]),
-            self._create_detection([100, 100, 150, 150]),
-        ]
-
-        tracked = tracker.update(detections, frame_id=0)
-
-        # 验证 track_id 是唯一的
-        track_ids = [t.track_id for t in tracked]
-        assert len(track_ids) == len(set(track_ids))
-        assert 1 in track_ids
-        assert 2 in track_ids
-
-    def test_track_association(self):
-        """测试跨帧轨迹关联。"""
-        tracker = MultiObjectTracker(min_hits=1, iou_threshold=0.3)
-
-        # 第一帧
-        det1 = [self._create_detection([0, 0, 50, 50])]
-        tracked1 = tracker.update(det1, frame_id=0)
-        track_id = tracked1[0].track_id
-
-        # 第二帧，目标稍微移动
-        det2 = [self._create_detection([5, 5, 55, 55])]
-        tracked2 = tracker.update(det2, frame_id=1)
-
-        # 应该保持相同的 track_id
-        assert len(tracked2) == 1
-        assert tracked2[0].track_id == track_id
-
-    def test_new_track_creation(self):
-        """测试新目标出现时创建新轨迹。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        # 第一帧：一个目标
-        det1 = [self._create_detection([0, 0, 50, 50])]
-        tracker.update(det1, frame_id=0)
-
-        # 第二帧：两个目标（原有 + 新增）
-        det2 = [
-            self._create_detection([0, 0, 50, 50]),
-            self._create_detection([200, 200, 250, 250]),
-        ]
-        tracked = tracker.update(det2, frame_id=1)
-
-        assert tracker.track_count == 2
-        assert len(tracked) == 2
-
-    def test_track_deletion_after_max_age(self):
-        """测试超过 max_age 后删除轨迹。"""
-        tracker = MultiObjectTracker(max_age=3, min_hits=1)
-
-        # 第一帧：创建轨迹
-        det1 = [self._create_detection([0, 0, 50, 50])]
-        tracker.update(det1, frame_id=0)
-        assert tracker.track_count == 1
-
-        # 后续帧：无检测
-        for i in range(1, 5):
-            tracker.update([], frame_id=i)
-
-        # 轨迹应该被删除
-        assert tracker.track_count == 0
-
-    def test_min_hits_confirmation(self):
-        """测试 min_hits 确认机制。"""
-        tracker = MultiObjectTracker(min_hits=3)
-
-        det = [self._create_detection([0, 0, 50, 50])]
-
-        # 第一帧：未确认
-        tracked1 = tracker.update(det, frame_id=0)
-        assert len(tracked1) == 0
-
-        # 第二帧：未确认
-        tracked2 = tracker.update(det, frame_id=1)
-        assert len(tracked2) == 0
-
-        # 第三帧：确认
-        tracked3 = tracker.update(det, frame_id=2)
-        assert len(tracked3) == 1
-
-    def test_reset(self):
-        """测试重置跟踪器。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        det = [self._create_detection([0, 0, 50, 50])]
-        tracker.update(det, frame_id=0)
-        assert tracker.track_count == 1
-
-        tracker.reset()
-        assert tracker.track_count == 0
-        assert tracker.confirmed_track_count == 0
-
-    def test_get_all_tracks(self):
-        """测试获取所有轨迹（包括未确认的）。"""
-        tracker = MultiObjectTracker(min_hits=3)
-
-        det = [self._create_detection([0, 0, 50, 50])]
-        tracker.update(det, frame_id=0)
-
-        # 未确认，但应该在 all_tracks 中
-        all_tracks = tracker.get_all_tracks()
-        assert len(all_tracks) == 1
-
-    def test_empty_detections(self):
-        """测试空检测列表。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        # 空检测不应该崩溃
-        tracked = tracker.update([], frame_id=0)
-        assert len(tracked) == 0
-        assert tracker.track_count == 0
-
-    def test_track_hits_increment(self):
-        """测试 hits 计数递增。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        det = [self._create_detection([0, 0, 50, 50])]
-
-        # 连续更新
-        for i in range(5):
-            tracked = tracker.update(det, frame_id=i)
-
-        # hits 应该是 5
-        assert len(tracked) == 1
-        assert tracked[0].hits == 5
-
-    def test_time_since_update(self):
-        """测试 time_since_update 字段。"""
-        tracker = MultiObjectTracker(min_hits=1, max_age=10)
-
-        det = [self._create_detection([0, 0, 50, 50])]
-
-        # 第一帧
-        tracker.update(det, frame_id=0)
-
-        # 后续帧无检测
-        tracker.update([], frame_id=1)
-        tracker.update([], frame_id=2)
-
-        # 获取所有轨迹（包括未匹配的）
-        all_tracks = tracker.get_all_tracks()
-        assert len(all_tracks) == 1
-        assert all_tracks[0].time_since_update == 2
-
-    def test_class_id_preserved(self):
-        """测试 class_id 保持不变。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        det = [self._create_detection([0, 0, 50, 50], class_id=2)]
-        tracked = tracker.update(det, frame_id=0)
-
-        assert tracked[0].class_id == 2
-
-    def test_confidence_updated(self):
-        """测试置信度更新。"""
-        tracker = MultiObjectTracker(min_hits=1)
-
-        # 第一帧
-        det1 = [self._create_detection([0, 0, 50, 50], confidence=0.8)]
-        tracker.update(det1, frame_id=0)
-
-        # 第二帧，置信度变化
-        det2 = [self._create_detection([0, 0, 50, 50], confidence=0.95)]
-        tracked = tracker.update(det2, frame_id=1)
-
-        assert tracked[0].confidence == 0.95
