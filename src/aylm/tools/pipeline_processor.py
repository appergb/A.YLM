"""æµæ°´çº¿å¤„ç†å™¨æ¨¡å—ã€‚

å®ç°å¤šå›¾åƒæµæ°´çº¿å¤„ç†ï¼Œæ”¯æŒæ¨ç†ä¸ä½“ç´ åŒ–å¹¶è¡Œã€è¯­ä¹‰æ£€æµ‹èåˆã€‚
"""

from __future__ import annotations

import contextlib
import gc
import logging
import threading
import time
from concurrent.futures import Future, ThreadPoolExecutor
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Callable

import torch
from torch.nn import functional as functional_nn

# æ¨¡å—çº§å¸¸é‡
DEFAULT_VOXEL_SIZE = 0.05  # 5cm ä½“ç´ 
DEFAULT_SLICE_RADIUS = 20.0  # 20m åˆ‡ç‰‡åŠå¾„
DEFAULT_FOV_DEGREES = 60.0  # ç›¸æœºè§†åœºè§’

logger = logging.getLogger(__name__)


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾ã€‚"""

    PENDING = "pending"  # ç­‰å¾…ä¸­
    PREDICTING = "predicting"  # æ¨¡å‹æ¨ç†ä¸­
    PREDICTED = "predicted"  # æ¨ç†å®Œæˆ
    VOXELIZING = "voxelizing"  # ä½“ç´ åŒ–ä¸­
    COMPLETED = "completed"  # å…¨éƒ¨å®Œæˆ
    FAILED = "failed"  # å¤±è´¥


@dataclass
class ImageTask:
    """å•å¼ å›¾åƒçš„å¤„ç†ä»»åŠ¡ã€‚"""

    image_path: Path
    index: int
    status: TaskStatus = TaskStatus.PENDING
    ply_output_path: Path | None = None
    voxel_output_path: Path | None = None
    predict_start_time: float | None = None
    predict_end_time: float | None = None
    voxel_start_time: float | None = None
    voxel_end_time: float | None = None
    error_message: str | None = None
    # è¯­ä¹‰æ£€æµ‹ç›¸å…³å­—æ®µ
    detections: list | None = None  # æ£€æµ‹ç»“æœ
    semantic_ply_path: Path | None = None  # è¯­ä¹‰ PLY è·¯å¾„
    # ç›¸æœºå‚æ•°ï¼ˆç”¨äºè¯­ä¹‰èåˆï¼‰
    focal_length: float | None = None  # ç„¦è·ï¼ˆåƒç´ ï¼‰
    image_width: int | None = None  # å›¾åƒå®½åº¦
    image_height: int | None = None  # å›¾åƒé«˜åº¦


@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®ã€‚"""

    voxel_size: float = 0.005
    remove_ground: bool = True
    transform_coords: bool = False
    device: str = "auto"
    verbose: bool = True
    checkpoint_path: Path | None = None
    auto_unload: bool = True
    async_mode: bool = False
    # åˆ‡ç‰‡é…ç½®
    enable_slice: bool = True  # æ˜¯å¦å¯ç”¨åˆ‡ç‰‡
    slice_radius: float = 20.0  # åˆ‡ç‰‡åŠå¾„ï¼ˆç±³ï¼‰
    # è¯­ä¹‰æ£€æµ‹é…ç½®
    enable_semantic: bool = True  # æ˜¯å¦å¯ç”¨è¯­ä¹‰æ£€æµ‹ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    semantic_model: str = "yolo11n-seg.pt"  # YOLO æ¨¡å‹
    semantic_confidence: float = 0.5  # æ£€æµ‹ç½®ä¿¡åº¦
    colorize_semantic: bool = True  # è¯­ä¹‰ç€è‰²
    # å¯¼èˆªè¾“å‡ºé…ç½®
    output_navigation_ply: bool = True  # æ˜¯å¦è¾“å‡ºå¯¼èˆªç”¨ç‚¹äº‘ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰


@dataclass
class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡ä¿¡æ¯ã€‚"""

    total_images: int = 0
    completed_images: int = 0
    failed_images: int = 0
    total_predict_time: float = 0.0
    total_voxel_time: float = 0.0
    pipeline_start_time: float | None = None
    pipeline_end_time: float | None = None

    @property
    def total_time(self) -> float:
        if self.pipeline_start_time and self.pipeline_end_time:
            return self.pipeline_end_time - self.pipeline_start_time
        return 0.0

    @property
    def avg_predict_time(self) -> float:
        return (
            self.total_predict_time / self.completed_images
            if self.completed_images
            else 0.0
        )

    @property
    def avg_voxel_time(self) -> float:
        return (
            self.total_voxel_time / self.completed_images
            if self.completed_images
            else 0.0
        )


class PipelineLogger:
    """æµæ°´çº¿æ—¥å¿—è®°å½•å™¨ã€‚"""

    LEVEL_PREFIX = {
        "INFO": "   ",
        "STAGE": ">>>",
        "OK": " âœ“ ",
        "WARN": " ! ",
        "ERROR": " âœ— ",
        "PROGRESS": " â†’ ",
    }

    STATUS_DISPLAY = {
        TaskStatus.PENDING: "â³ ç­‰å¾…ä¸­",
        TaskStatus.PREDICTING: "ğŸ”„ æ¨ç†ä¸­",
        TaskStatus.PREDICTED: "ğŸ“¦ å¾…ä½“ç´ åŒ–",
        TaskStatus.VOXELIZING: "ğŸ”„ ä½“ç´ åŒ–ä¸­",
        TaskStatus.COMPLETED: "âœ… å®Œæˆ",
        TaskStatus.FAILED: "âŒ å¤±è´¥",
    }

    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self._lock = threading.Lock()
        self._start_time = time.time()

    def _timestamp(self) -> str:
        return f"[{time.time() - self._start_time:8.2f}s]"

    def _print(self, msg: str, level: str = "INFO"):
        with self._lock:
            prefix = self.LEVEL_PREFIX.get(level, "   ")
            print(f"{self._timestamp()} {prefix} {msg}")

    def header(self, title: str):
        with self._lock:
            print("\n" + "=" * 60)
            print(f"  {title}")
            print("=" * 60)

    def section(self, title: str):
        with self._lock:
            print(f"\n{'â”€' * 40}\n  {title}\n{'â”€' * 40}")

    def stage(self, msg: str):
        self._print(msg, "STAGE")

    def info(self, msg: str):
        if self.verbose:
            self._print(msg, "INFO")

    def ok(self, msg: str):
        self._print(msg, "OK")

    def warn(self, msg: str):
        self._print(msg, "WARN")

    def error(self, msg: str):
        self._print(msg, "ERROR")

    def progress(self, msg: str):
        self._print(msg, "PROGRESS")

    def task_status(self, tasks: list[ImageTask]):
        with self._lock:
            print("\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print("â”‚ No. â”‚ æ–‡ä»¶å                     â”‚ çŠ¶æ€        â”‚")
            print("â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
            for task in tasks:
                name = task.image_path.name[:24]
                status = self.STATUS_DISPLAY.get(task.status, "æœªçŸ¥")
                print(f"â”‚ {task.index + 1:3d} â”‚ {name:<26} â”‚ {status:<11} â”‚")
            print("â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½ï¿½â”˜")

    def stats(self, stats: PipelineStats):
        with self._lock:
            print("\n" + "=" * 60)
            print("  æµæ°´çº¿æ‰§è¡Œç»Ÿè®¡")
            print("=" * 60)
            print(f"  æ€»å›¾åƒæ•°:       {stats.total_images}")
            print(f"  æˆåŠŸå®Œæˆ:       {stats.completed_images}")
            print(f"  å¤±è´¥æ•°é‡:       {stats.failed_images}")
            print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            print(f"  æ€»è€—æ—¶:         {stats.total_time:.2f} ç§’")
            print(f"  æ¨ç†æ€»è€—æ—¶:     {stats.total_predict_time:.2f} ç§’")
            print(f"  ä½“ç´ åŒ–æ€»è€—æ—¶:   {stats.total_voxel_time:.2f} ç§’")
            print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            print(f"  å¹³å‡æ¨ç†æ—¶é—´:   {stats.avg_predict_time:.2f} ç§’/å¼ ")
            print(f"  å¹³å‡ä½“ç´ åŒ–æ—¶é—´: {stats.avg_voxel_time:.2f} ç§’/å¼ ")
            if stats.total_images > 1 and stats.total_time > 0:
                sequential = (
                    stats.avg_predict_time + stats.avg_voxel_time
                ) * stats.completed_images
                print(f"  æµæ°´çº¿æ•ˆç‡:     {sequential / stats.total_time:.1%}")
            print("=" * 60 + "\n")


class PipelineProcessor:
    """æµæ°´çº¿å¤„ç†å™¨ï¼Œå®ç°æ¨¡å‹æ¨ç†å’Œä½“ç´ åŒ–çš„å¹¶è¡Œå¤„ç†ã€‚"""

    def __init__(self, config: PipelineConfig | None = None):
        self.config = config or PipelineConfig()
        self.log = PipelineLogger(self.config.verbose)
        self.stats = PipelineStats()

        self._predictor = None
        self._device: torch.device | None = None
        self._model_loaded = False
        self._voxelizer = None
        self._detector = None  # YOLO è¯­ä¹‰æ£€æµ‹å™¨
        self._tasks: list[ImageTask] = []
        self._stop_event = threading.Event()
        self._predict_lock = threading.Lock()
        self._async_executor: ThreadPoolExecutor | None = None
        self._async_future: Future | None = None

    def __enter__(self):
        return self

    def __exit__(self, _exc_type, _exc_val, _exc_tb):
        self.cleanup()
        return False

    def __del__(self):
        self.cleanup()

    def cleanup(self):
        self._unload_model()
        self._cleanup_voxelizer()
        self._cleanup_detector()
        self._cleanup_async()

    def _unload_model(self):
        if not self._model_loaded:
            return

        self.log.stage("å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜...")

        try:
            if self._predictor is not None:
                if self._device and self._device.type != "cpu":
                    with contextlib.suppress(Exception):
                        self._predictor.cpu()
                del self._predictor
                self._predictor = None

            self._device = None
            self._model_loaded = False
            gc.collect()

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            if hasattr(torch, "mps") and hasattr(torch.mps, "empty_cache"):
                with contextlib.suppress(Exception):
                    torch.mps.empty_cache()

            self.log.ok("æ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
        except Exception as e:
            self.log.warn(f"æ¨¡å‹å¸è½½æ—¶å‡ºç°è­¦å‘Š: {e}")

    def _cleanup_voxelizer(self):
        if self._voxelizer is not None:
            del self._voxelizer
            self._voxelizer = None

    def _load_detector(self):
        """åŠ è½½ YOLO è¯­ä¹‰æ£€æµ‹å™¨ã€‚"""
        from aylm.tools.object_detector import DetectorConfig, ObjectDetector

        self.log.stage("åŠ è½½ YOLO è¯­ä¹‰æ£€æµ‹å™¨...")
        detector_config = DetectorConfig(
            model_name=self.config.semantic_model,
            confidence_threshold=self.config.semantic_confidence,
            device=self.config.device,
        )
        self._detector = ObjectDetector(detector_config)
        self._detector.load()
        self.log.ok(f"è¯­ä¹‰æ£€æµ‹å™¨å·²åŠ è½½ (æ¨¡å‹: {self.config.semantic_model})")

    def _cleanup_detector(self):
        """æ¸…ç†è¯­ä¹‰æ£€æµ‹å™¨ã€‚"""
        if self._detector is not None:
            self.log.info("å¸è½½è¯­ä¹‰æ£€æµ‹å™¨...")
            self._detector.unload()
            del self._detector
            self._detector = None

    def _cleanup_async(self):
        if self._async_executor is not None:
            self._async_executor.shutdown(wait=False)
            self._async_executor = None
        self._async_future = None

    def _detect_device(self) -> torch.device:
        if self.config.device != "auto":
            return torch.device(self.config.device)
        if torch.cuda.is_available():
            return torch.device("cuda")
        if hasattr(torch, "mps") and torch.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")

    def _load_model(self) -> bool:
        self.log.stage("åŠ è½½SHARPæ¨¡å‹åˆ°å†…å­˜...")

        try:
            from sharp.models import PredictorParams, create_predictor

            self._device = self._detect_device()
            self.log.info(f"ä½¿ç”¨è®¾å¤‡: {self._device}")

            if self.config.checkpoint_path and self.config.checkpoint_path.exists():
                self.log.info(f"ä»æœ¬åœ°åŠ è½½: {self.config.checkpoint_path}")
                state_dict = torch.load(
                    self.config.checkpoint_path,
                    weights_only=True,
                    map_location=self._device,
                )
            else:
                model_url = (
                    "https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt"
                )
                self.log.info("ä»ç½‘ç»œä¸‹è½½æ¨¡å‹...")
                state_dict = torch.hub.load_state_dict_from_url(
                    model_url, progress=True, map_location=self._device
                )

            self._predictor = create_predictor(PredictorParams())
            self._predictor.load_state_dict(state_dict)
            self._predictor.eval()
            self._predictor.to(self._device)

            self._model_loaded = True
            self.log.ok("æ¨¡å‹åŠ è½½å®Œæˆ")
            return True

        except ImportError as e:
            self.log.error("æ¨¡å‹åŠ è½½å¤±è´¥: ç¼ºå°‘ä¾èµ–æ¨¡å—")
            self.log.error("    è¯·ç¡®ä¿å·²å®‰è£… sharp åŒ…: pip install sharp")
            self.log.error(f"    è¯¦ç»†ä¿¡æ¯: {e}")
            logger.exception("æ¨¡å‹å¯¼å…¥å¤±è´¥")
            return False
        except Exception as e:
            self.log.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.log.error(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.exception("æ¨¡å‹åŠ è½½å¼‚å¸¸")
            return False

    def _load_voxelizer(self):
        from aylm.tools.pointcloud_voxelizer import PointCloudVoxelizer, VoxelizerConfig

        self._voxelizer = PointCloudVoxelizer(
            config=VoxelizerConfig(voxel_size=self.config.voxel_size)
        )
        self.log.info(f"ä½“ç´ åŒ–å™¨å·²åˆå§‹åŒ– (ä½“ç´ å°ºå¯¸: {self.config.voxel_size}m)")

    @torch.no_grad()
    def _predict_single(self, task: ImageTask, output_dir: Path) -> bool:
        """å¯¹å•å¼ å›¾åƒè¿›è¡Œæ¨¡å‹æ¨ç†ã€‚"""
        from sharp.utils import io
        from sharp.utils.gaussians import save_ply, unproject_gaussians

        task.status = TaskStatus.PREDICTING
        task.predict_start_time = time.time()

        self.log.progress(f"[{task.index+1}] å¼€å§‹æ¨ç†: {task.image_path.name}")

        try:
            # åŠ è½½å›¾åƒ
            image, _, f_px = io.load_rgb(task.image_path)
            height, width = image.shape[:2]

            self.log.info(f"    å›¾åƒå°ºå¯¸: {width}x{height}, ç„¦è·: {f_px:.1f}px")

            # é¢„å¤„ç†
            internal_shape = (1536, 1536)
            image_pt = (
                torch.from_numpy(image.copy()).float().to(self._device).permute(2, 0, 1)
                / 255.0
            )
            disparity_factor = torch.tensor([f_px / width]).float().to(self._device)

            image_resized_pt = functional_nn.interpolate(
                image_pt[None],
                size=(internal_shape[1], internal_shape[0]),
                mode="bilinear",
                align_corners=True,
            )

            # æ¨ç†ï¼ˆéœ€è¦é”ä¿æŠ¤ï¼Œå› ä¸ºæ¨¡å‹ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼‰
            with self._predict_lock:
                gaussians_ndc = self._predictor(image_resized_pt, disparity_factor)

            # åå¤„ç†
            intrinsics = (
                torch.tensor(
                    [
                        [f_px, 0, width / 2, 0],
                        [0, f_px, height / 2, 0],
                        [0, 0, 1, 0],
                        [0, 0, 0, 1],
                    ]
                )
                .float()
                .to(self._device)
            )

            intrinsics_resized = intrinsics.clone()
            intrinsics_resized[0] *= internal_shape[0] / width
            intrinsics_resized[1] *= internal_shape[1] / height

            gaussians = unproject_gaussians(
                gaussians_ndc,
                torch.eye(4).to(self._device),
                intrinsics_resized,
                internal_shape,
            )

            # ä¿å­˜PLY
            output_path = output_dir / f"{task.image_path.stem}.ply"
            save_ply(gaussians, f_px, (height, width), output_path)

            task.ply_output_path = output_path
            task.status = TaskStatus.PREDICTED
            task.predict_end_time = time.time()
            # ä¿å­˜ç›¸æœºå‚æ•°ä¾›è¯­ä¹‰èåˆä½¿ç”¨
            task.focal_length = f_px
            task.image_width = width
            task.image_height = height

            predict_time = task.predict_end_time - task.predict_start_time
            self.log.ok(
                f"[{task.index+1}] æ¨ç†å®Œæˆ: {task.image_path.name} ({predict_time:.2f}s)"
            )
            self.log.info(f"    è¾“å‡º: {output_path.name}")

            return True

        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.predict_end_time = time.time()
            self.log.error(f"[{task.index+1}] æ¨ç†å¤±è´¥: {task.image_path.name}")
            self.log.error(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
            self.log.error(f"    é”™è¯¯ä¿¡æ¯: {e}")
            logger.exception(f"æ¨ç†å¼‚å¸¸è¯¦æƒ… - {task.image_path.name}")
            return False

    def _voxelize_single(
        self, task: ImageTask, output_dir: Path, navigation_dir: Path | None = None
    ) -> bool:
        """å¯¹å•ä¸ªPLYæ–‡ä»¶è¿›è¡Œåˆ‡ç‰‡ã€ä½“ç´ åŒ–ï¼Œå¯é€‰è¯­ä¹‰èåˆã€‚

        å¤„ç†æµç¨‹ï¼šSHARPè¾“å‡º â†’ åˆ‡ç‰‡ â†’ ä½“ç´ åŒ– â†’ è¯­ä¹‰èåˆ

        Args:
            task: å›¾åƒä»»åŠ¡
            output_dir: ä½“ç´ åŒ–è¾“å‡ºç›®å½•
            navigation_dir: å¯¼èˆªç”¨ç‚¹äº‘è¾“å‡ºç›®å½•ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
        """
        if task.status != TaskStatus.PREDICTED or task.ply_output_path is None:
            return False

        task.status = TaskStatus.VOXELIZING
        task.voxel_start_time = time.time()

        self.log.progress(f"[{task.index+1}] å¼€å§‹å¤„ç†: {task.ply_output_path.name}")

        try:
            # ç¡®å®šè¾“å…¥æ–‡ä»¶ï¼ˆå¯èƒ½ç»è¿‡åˆ‡ç‰‡ï¼‰
            input_ply_path = task.ply_output_path

            # æ­¥éª¤1ï¼šåˆ‡ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.enable_slice:
                sliced_path = self._apply_slice(task)
                if sliced_path is not None:
                    input_ply_path = sliced_path

            # æ­¥éª¤2ï¼šä½“ç´ åŒ–
            output_path = output_dir / f"vox_{task.ply_output_path.name}"
            self._voxelizer.process(
                input_ply_path,
                output_path,
                remove_ground=self.config.remove_ground,
                transform_coords=self.config.transform_coords,
            )

            # æ¸…ç†ä¸´æ—¶åˆ‡ç‰‡æ–‡ä»¶
            if self.config.enable_slice and input_ply_path != task.ply_output_path:
                with contextlib.suppress(Exception):
                    input_ply_path.unlink()

            # æ­¥éª¤3ï¼šè¯­ä¹‰èåˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.enable_semantic and self._detector is not None:
                self._apply_semantic_fusion(task, output_path, navigation_dir)

            task.voxel_output_path = output_path
            task.status = TaskStatus.COMPLETED
            task.voxel_end_time = time.time()

            voxel_time = task.voxel_end_time - task.voxel_start_time
            self.log.ok(
                f"[{task.index+1}] å¤„ç†å®Œæˆ: {task.ply_output_path.name} ({voxel_time:.2f}s)"
            )
            self.log.info(f"    è¾“å‡º: {output_path.name}")

            return True

        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.voxel_end_time = time.time()
            self.log.error(f"[{task.index+1}] å¤„ç†å¤±è´¥: {task.ply_output_path.name}")
            self.log.error(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
            self.log.error(f"    é”™è¯¯ä¿¡æ¯: {e}")
            logger.exception(f"å¤„ç†å¼‚å¸¸è¯¦æƒ… - {task.ply_output_path.name}")
            return False

    def _apply_slice(self, task: ImageTask) -> Path | None:
        """å¯¹ç‚¹äº‘æ‰§è¡ŒåŠå¾„åˆ‡ç‰‡ï¼Œåªä¿ç•™æ‘„åƒæœºé™„è¿‘çš„ç‚¹ã€‚

        Args:
            task: å›¾åƒä»»åŠ¡

        Returns:
            åˆ‡ç‰‡åçš„ä¸´æ—¶ PLY æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        import numpy as np

        try:
            from plyfile import PlyData, PlyElement
        except ImportError:
            self.log.warn("    plyfile æœªå®‰è£…ï¼Œè·³è¿‡åˆ‡ç‰‡")
            return None

        self.log.info(f"    æ‰§è¡Œåˆ‡ç‰‡ (åŠå¾„: {self.config.slice_radius}m)")

        try:
            # è¯»å–ç‚¹äº‘
            ply_data = PlyData.read(str(task.ply_output_path))
            vertex = ply_data["vertex"]

            # è·å–åæ ‡
            x = np.array(vertex["x"], dtype=np.float64)
            z = np.array(vertex["z"], dtype=np.float64)

            # è®¡ç®—æ°´å¹³è·ç¦»ï¼ˆX-Zå¹³é¢ï¼Œä»¥åŸç‚¹ä¸ºåœ†å¿ƒï¼‰
            # OpenCV åæ ‡ç³»: Xå³, Yä¸‹, Zå‰
            # æ°´å¹³é¢æ˜¯ X-Z å¹³é¢
            distance_xz = np.sqrt(x**2 + z**2)

            # ç­›é€‰åœ¨åŠå¾„å†…çš„ç‚¹
            mask = distance_xz <= self.config.slice_radius
            n_original = len(x)
            n_kept = mask.sum()

            self.log.info(
                f"    åˆ‡ç‰‡ç»“æœ: {n_kept}/{n_original} ç‚¹ "
                f"({n_kept / n_original * 100:.1f}%)"
            )

            if n_kept == 0:
                self.log.warn("    åˆ‡ç‰‡åæ— ç‚¹ï¼Œè·³è¿‡")
                return None

            if n_kept == n_original:
                self.log.info("    æ‰€æœ‰ç‚¹éƒ½åœ¨åŠå¾„å†…ï¼Œè·³è¿‡åˆ‡ç‰‡")
                return None

            # æ„å»ºæ–°çš„é¡¶ç‚¹æ•°æ®
            new_vertex_data = vertex.data[mask]

            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            sliced_path = (
                task.ply_output_path.parent / f"sliced_{task.ply_output_path.name}"
            )
            new_vertex = PlyElement.describe(new_vertex_data, "vertex")
            PlyData([new_vertex], text=False).write(str(sliced_path))

            self.log.ok(f"    åˆ‡ç‰‡å®Œæˆ: {sliced_path.name}")
            return sliced_path

        except Exception as e:
            self.log.warn(f"    åˆ‡ç‰‡å¤±è´¥: {e}")
            logger.exception(f"åˆ‡ç‰‡å¼‚å¸¸ - {task.ply_output_path.name}")
            return None

    def _apply_semantic_fusion(
        self, task: ImageTask, voxel_ply_path: Path, navigation_dir: Path | None = None
    ) -> None:
        """å¯¹ä½“ç´ åŒ–åçš„ç‚¹äº‘åº”ç”¨è¯­ä¹‰èåˆã€‚

        å¤„ç†æµç¨‹ï¼š
        1. è¯­ä¹‰èåˆ -> ä¿å­˜ vox_xxx.ply (OpenCV åæ ‡ç³»ï¼Œç”¨äºå¯è§†åŒ–)
        2. åæ ‡è½¬æ¢ -> ä¿å­˜ nav_xxx.ply (æœºå™¨äººåæ ‡ç³»ï¼Œç”¨äºå¯¼èˆª)
        3. éšœç¢ç‰©æå– -> ä¿å­˜ xxx_obstacles.json (æœºå™¨äººåæ ‡ç³»)

        Args:
            task: å›¾åƒä»»åŠ¡
            voxel_ply_path: ä½“ç´ åŒ–åçš„ PLY æ–‡ä»¶è·¯å¾„
            navigation_dir: å¯¼èˆªç”¨ç‚¹äº‘è¾“å‡ºç›®å½•ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
        """
        import cv2

        from aylm.tools.semantic_fusion import FusionConfig, SemanticFusion
        from aylm.tools.semantic_types import CameraIntrinsics

        self.log.info(f"    æ‰§è¡Œè¯­ä¹‰èåˆ: {task.image_path.name}")

        try:
            # è¯»å–åŸå§‹å›¾åƒ
            image = cv2.imread(str(task.image_path))
            if image is None:
                self.log.warn(f"    æ— æ³•è¯»å–å›¾åƒï¼Œè·³è¿‡è¯­ä¹‰èåˆ: {task.image_path}")
                return

            height, width = image.shape[:2]

            # æ‰§è¡Œç›®æ ‡æ£€æµ‹
            detections = self._detector.detect(image, return_masks=True)
            self.log.info(f"    æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡")

            # ä¿å­˜æ£€æµ‹ç»“æœå¯è§†åŒ–å›¾ç‰‡ï¼ˆæ”¾åœ¨ PLY è¾“å‡ºç›®å½•ï¼‰
            if detections:
                detection_image_path = (
                    voxel_ply_path.parent / f"det_{task.image_path.name}"
                )
                self._detector.save_detection_image(
                    image, detections, detection_image_path, draw_masks=True
                )
                self.log.ok(f"    æ£€æµ‹ç»“æœå›¾ç‰‡å·²ä¿å­˜: {detection_image_path.name}")

            if not detections:
                self.log.info("    æ— æ£€æµ‹ç»“æœï¼Œè·³è¿‡è¯­ä¹‰èåˆ")
                return

            # è¯»å–ä½“ç´ åŒ–åçš„ç‚¹äº‘
            try:
                from plyfile import PlyData
            except ImportError:
                self.log.warn("    plyfile æœªå®‰è£…ï¼Œè·³è¿‡è¯­ä¹‰èåˆ")
                return

            ply_data = PlyData.read(str(voxel_ply_path))
            vertex = ply_data["vertex"]

            import numpy as np

            points = np.column_stack([vertex["x"], vertex["y"], vertex["z"]]).astype(
                np.float64
            )
            colors = None
            if "red" in vertex.data.dtype.names:
                colors = (
                    np.column_stack(
                        [vertex["red"], vertex["green"], vertex["blue"]]
                    ).astype(np.float64)
                    / 255.0
                )

            # ä½¿ç”¨ SHARP æ¨ç†æ—¶ä¿å­˜çš„ç›¸æœºå†…å‚ï¼ˆç²¾ç¡®ç„¦è·ï¼‰
            if task.focal_length is not None:
                f_px = task.focal_length
                self.log.info(f"    ä½¿ç”¨ SHARP ç„¦è·: {f_px:.1f}px")
            else:
                # å›é€€ï¼šä»å›¾åƒé‡æ–°è¯»å–ç„¦è·
                from sharp.utils import io

                _, _, f_px = io.load_rgb(task.image_path)
                self.log.info(f"    ä» EXIF è¯»å–ç„¦è·: {f_px:.1f}px")

            intrinsics = CameraIntrinsics(fx=f_px, fy=f_px, cx=width / 2, cy=height / 2)

            # æ‰§è¡Œè¯­ä¹‰èåˆ
            fusion_config = FusionConfig(
                min_confidence=self.config.semantic_confidence,
                colorize_semantic=self.config.colorize_semantic,
            )
            fusion = SemanticFusion(fusion_config)
            semantic_pc = fusion.fuse(
                points=points,
                colors=colors,
                detections=detections,
                image_shape=(height, width),
                intrinsics=intrinsics,
            )

            # æ­¥éª¤1: ä¿å­˜å¸¦è¯­ä¹‰æ ‡ç­¾çš„ PLYï¼ˆOpenCV åæ ‡ç³»ï¼Œç”¨äºå¯è§†åŒ–ï¼‰
            fusion.save_semantic_ply(
                semantic_pc,
                voxel_ply_path,
                include_semantic_colors=self.config.colorize_semantic,
            )
            self.log.ok(f"    è¯­ä¹‰èåˆå®Œæˆ (OpenCVåæ ‡ç³»): {voxel_ply_path.name}")

            # æ­¥éª¤2: ä¿å­˜å¯¼èˆªç”¨ç‚¹äº‘ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼Œä½“ç´ åŒ–ï¼‰
            if navigation_dir is not None and self.config.output_navigation_ply:
                nav_ply_path = navigation_dir / f"nav_{voxel_ply_path.stem[4:]}.ply"
                fusion.save_navigation_ply(
                    semantic_pc, nav_ply_path, voxel_size=DEFAULT_VOXEL_SIZE
                )
                self.log.ok(
                    f"    å¯¼èˆªç‚¹äº‘å·²ä¿å­˜ (æœºå™¨äººåæ ‡ç³», {DEFAULT_VOXEL_SIZE*100:.0f}cmä½“ç´ ): {nav_ply_path.name}"
                )

            # æ­¥éª¤3: æå–éšœç¢ç‰©å¹¶å¯¼å‡º JSONï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
            from aylm.tools.obstacle_marker import ObstacleMarker, ObstacleMarkerConfig

            marker_intrinsics = CameraIntrinsics(
                fx=f_px, fy=f_px, cx=width / 2, cy=height / 2
            )
            marker = ObstacleMarker(ObstacleMarkerConfig())
            obstacles = marker.extract_obstacles_from_detections(
                points=points,
                detections=detections,
                image_shape=(height, width),
                intrinsics=marker_intrinsics,
            )

            if obstacles:
                # å°†éšœç¢ç‰©åæ ‡è½¬æ¢ä¸ºæœºå™¨äººåæ ‡ç³»
                obstacles_robot = self._transform_obstacles_to_robot(obstacles)
                json_path = (
                    voxel_ply_path.parent / f"{voxel_ply_path.stem}_obstacles.json"
                )
                marker.export_to_json(obstacles_robot, json_path)
                self.log.ok(f"    éšœç¢ç‰©ä¿¡æ¯å·²å¯¼å‡º (æœºå™¨äººåæ ‡ç³»): {json_path.name}")

        except Exception as e:
            self.log.warn(f"    è¯­ä¹‰èåˆå¤±è´¥: {e}")
            logger.exception(f"è¯­ä¹‰èåˆå¼‚å¸¸ - {task.image_path.name}")

    def _transform_obstacles_to_robot(self, obstacles: list) -> list:
        """å°†éšœç¢ç‰©è¾¹ç•Œæ¡†åæ ‡ä» OpenCV åæ ‡ç³»è½¬æ¢åˆ°æœºå™¨äººåæ ‡ç³»ã€‚

        Args:
            obstacles: OpenCV åæ ‡ç³»ä¸‹çš„éšœç¢ç‰©åˆ—è¡¨

        Returns:
            æœºå™¨äººåæ ‡ç³»ä¸‹çš„éšœç¢ç‰©åˆ—è¡¨
        """
        import numpy as np

        from aylm.tools.coordinate_utils import transform_opencv_to_robot
        from aylm.tools.obstacle_marker import ObstacleBox3D

        transformed = []
        for obs in obstacles:
            # è½¬æ¢ä¸­å¿ƒç‚¹
            center_opencv = np.array(obs.center)
            center_robot = transform_opencv_to_robot(center_opencv)

            # è½¬æ¢å°ºå¯¸ï¼ˆéœ€è¦é‡æ–°æ˜ å°„ç»´åº¦ï¼‰
            # OpenCV: (width_x, height_y, depth_z) -> Robot: (depth_z, width_x, height_y)
            # å› ä¸º Robot X = OpenCV Z, Robot Y = -OpenCV X, Robot Z = -OpenCV Y
            dims_opencv = obs.dimensions
            dims_robot = (dims_opencv[2], dims_opencv[0], dims_opencv[1])

            transformed_obs = ObstacleBox3D(
                center=tuple(center_robot),
                dimensions=dims_robot,
                label=obs.label,
                confidence=obs.confidence,
                point_indices=obs.point_indices,
            )
            transformed.append(transformed_obs)

        return transformed

    def _collect_images(self, input_path: Path) -> list[Path]:
        extensions = {".jpg", ".jpeg", ".png", ".heic", ".webp", ".tiff", ".bmp"}

        if input_path.is_file():
            return [input_path] if input_path.suffix.lower() in extensions else []

        images = []
        for ext in extensions:
            images.extend(input_path.glob(f"*{ext}"))
            images.extend(input_path.glob(f"*{ext.upper()}"))
        return sorted(images)

    def process(
        self,
        input_path: Path,
        output_dir: Path,
        voxel_output_dir: Path | None = None,
    ) -> PipelineStats:
        """æ‰§è¡Œæµæ°´çº¿å¤„ç†ã€‚

        Args:
            input_path: è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•
            output_dir: PLYè¾“å‡ºç›®å½•
            voxel_output_dir: ä½“ç´ åŒ–è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºoutput_dir/voxelizedï¼‰

        Returns:
            PipelineStats: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        self.log.header("A.YLM æµæ°´çº¿å¤„ç†å™¨ v2.0")

        # åˆå§‹åŒ–
        self.stats = PipelineStats()
        self.stats.pipeline_start_time = time.time()

        # éªŒè¯è¾“å…¥è·¯å¾„
        if not input_path.exists():
            self.log.error(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
            self.stats.pipeline_end_time = time.time()
            return self.stats

        if voxel_output_dir is None:
            voxel_output_dir = output_dir / "voxelized"

        # åˆ›å»ºå¯¼èˆªè¾“å‡ºç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        navigation_dir = (
            output_dir / "navigation"
            if self.config.output_navigation_ply and self.config.enable_semantic
            else None
        )

        # åˆ›å»ºè¾“å‡ºç›®å½•
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            voxel_output_dir.mkdir(parents=True, exist_ok=True)
            if navigation_dir is not None:
                navigation_dir.mkdir(parents=True, exist_ok=True)
            self.log.info(f"PLYè¾“å‡ºç›®å½•: {output_dir}")
            self.log.info(f"ä½“ç´ åŒ–è¾“å‡ºç›®å½•: {voxel_output_dir}")
            if navigation_dir is not None:
                self.log.info(f"å¯¼èˆªè¾“å‡ºç›®å½•: {navigation_dir}")
        except PermissionError as e:
            self.log.error(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
            self.stats.pipeline_end_time = time.time()
            return self.stats

        # æ”¶é›†å›¾åƒ
        self.log.section("é˜¶æ®µ 1: æ”¶é›†å›¾åƒ")
        image_paths = self._collect_images(input_path)

        if not image_paths:
            self.log.error(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {input_path}")
            return self.stats

        self.stats.total_images = len(image_paths)
        self.log.ok(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")

        for i, path in enumerate(image_paths):
            self.log.info(f"  [{i+1}] {path.name}")

        # åˆ›å»ºä»»åŠ¡
        self._tasks = [
            ImageTask(image_path=path, index=i) for i, path in enumerate(image_paths)
        ]

        # åŠ è½½æ¨¡å‹
        self.log.section("é˜¶æ®µ 2: åŠ è½½æ¨¡å‹")
        if not self._load_model():
            self.log.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return self.stats

        # åŠ è½½ä½“ç´ åŒ–å™¨
        self._load_voxelizer()

        # åŠ è½½è¯­ä¹‰æ£€æµ‹å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_semantic:
            self._load_detector()

        # æ‰§è¡Œæµæ°´çº¿
        self.log.section("é˜¶æ®µ 3: æµæ°´çº¿å¤„ç†")
        self.log.info("æµæ°´çº¿æ¨¡å¼: æ¨ç†(N) || ä½“ç´ åŒ–(N-1)")
        if self.config.enable_semantic:
            self.log.info("è¯­ä¹‰æ£€æµ‹: å·²å¯ç”¨")
        if navigation_dir is not None:
            self.log.info("å¯¼èˆªè¾“å‡º: å·²å¯ç”¨ (æœºå™¨äººåæ ‡ç³»)")
        self.log.info("")

        self._execute_pipeline(output_dir, voxel_output_dir, navigation_dir)

        # ç»Ÿè®¡ç»“æœ
        self.stats.pipeline_end_time = time.time()

        for task in self._tasks:
            if task.status == TaskStatus.COMPLETED:
                self.stats.completed_images += 1
                if task.predict_start_time and task.predict_end_time:
                    self.stats.total_predict_time += (
                        task.predict_end_time - task.predict_start_time
                    )
                if task.voxel_start_time and task.voxel_end_time:
                    self.stats.total_voxel_time += (
                        task.voxel_end_time - task.voxel_start_time
                    )
            elif task.status == TaskStatus.FAILED:
                self.stats.failed_images += 1

        # æ‰“å°æœ€ç»ˆçŠ¶æ€
        self.log.section("å¤„ç†ç»“æœ")
        self.log.task_status(self._tasks)
        self.log.stats(self.stats)

        # è‡ªåŠ¨å¸è½½æ¨¡å‹
        if self.config.auto_unload:
            self.log.section("é˜¶æ®µ 4: æ¸…ç†èµ„æº")
            self._unload_model()
            self._cleanup_voxelizer()
            self._cleanup_detector()

        return self.stats

    def process_async(
        self,
        input_path: Path,
        output_dir: Path,
        voxel_output_dir: Path | None = None,
        callback: Callable[[PipelineStats], None] | None = None,
    ) -> Future:
        """å¼‚æ­¥æ‰§è¡Œæµæ°´çº¿å¤„ç†ã€‚

        åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¤„ç†ï¼Œç«‹å³è¿”å› Future å¯¹è±¡ã€‚

        Args:
            input_path: è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•
            output_dir: PLYè¾“å‡ºç›®å½•
            voxel_output_dir: ä½“ç´ åŒ–è¾“å‡ºç›®å½•
            callback: å¤„ç†å®Œæˆåçš„å›è°ƒå‡½æ•°

        Returns:
            Future: å¯ç”¨äºè·å–ç»“æœæˆ–æ£€æŸ¥çŠ¶æ€

        Example:
            >>> processor = PipelineProcessor(config)
            >>> future = processor.process_async(input_path, output_dir)
            >>> # åšå…¶ä»–äº‹æƒ…...
            >>> if future.done():
            ...     stats = future.result()
            >>> # æˆ–è€…ç­‰å¾…å®Œæˆ
            >>> stats = future.result(timeout=300)
        """
        if self._async_executor is None:
            self._async_executor = ThreadPoolExecutor(
                max_workers=1, thread_name_prefix="pipeline"
            )

        def _run():
            try:
                stats = self.process(input_path, output_dir, voxel_output_dir)
                if callback:
                    callback(stats)
                return stats
            except Exception:
                logger.exception("å¼‚æ­¥å¤„ç†å¤±è´¥")
                raise

        self._async_future = self._async_executor.submit(_run)
        return self._async_future

    def is_processing(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ä¸­ã€‚"""
        if self._async_future is None:
            return False
        return not self._async_future.done()

    def wait_for_completion(self, timeout: float | None = None) -> PipelineStats | None:
        """ç­‰å¾…å¼‚æ­¥å¤„ç†å®Œæˆã€‚

        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNone è¡¨ç¤ºæ— é™ç­‰å¾…

        Returns:
            PipelineStats æˆ– Noneï¼ˆå¦‚æœè¶…æ—¶ï¼‰
        """
        if self._async_future is None:
            return None
        try:
            return self._async_future.result(timeout=timeout)
        except TimeoutError:
            return None

    def cancel(self) -> bool:
        """å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„å¤„ç†ã€‚"""
        self._stop_event.set()
        if self._async_future is not None:
            return self._async_future.cancel()
        return False

    def _execute_pipeline(
        self,
        output_dir: Path,
        voxel_output_dir: Path,
        navigation_dir: Path | None = None,
    ):
        """æ‰§è¡Œæµæ°´çº¿å¤„ç†é€»è¾‘ã€‚

        æµæ°´çº¿ç­–ç•¥:
        1. ç¬¬ä¸€å¼ å›¾ç‰‡: åªåšæ¨ç†ï¼ˆæ— å¹¶è¡Œï¼‰
        2. ç¬¬2åˆ°Nå¼ å›¾ç‰‡: æ¨ç†ç¬¬Nå¼  || ä½“ç´ åŒ–ç¬¬N-1å¼ ï¼ˆå¹¶è¡Œï¼‰
        3. æœ€å: ä½“ç´ åŒ–æœ€åä¸€å¼ å›¾ç‰‡ï¼ˆæ— å¹¶è¡Œï¼‰

        æ—¶é—´çº¿ç¤ºæ„:
            å›¾ç‰‡1: [====æ¨ç†====]
            å›¾ç‰‡2:              [====æ¨ç†====]
            å›¾ç‰‡1:              [====ä½“ç´ åŒ–====]
            å›¾ç‰‡3:                            [====æ¨ç†====]
            å›¾ç‰‡2:                            [====ä½“ç´ åŒ–====]
            ...

        Args:
            output_dir: PLY è¾“å‡ºç›®å½•
            voxel_output_dir: ä½“ç´ åŒ–è¾“å‡ºç›®å½•
            navigation_dir: å¯¼èˆªç”¨ç‚¹äº‘è¾“å‡ºç›®å½•ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
        """
        total = len(self._tasks)

        if total == 0:
            self.log.warn("æ²¡æœ‰ä»»åŠ¡éœ€è¦å¤„ç†")
            return

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä½“ç´ åŒ–ï¼ˆæ¨ç†åœ¨ä¸»çº¿ç¨‹ï¼Œå› ä¸ºGPUæ“ä½œéœ€è¦åŒæ­¥ï¼‰
        with ThreadPoolExecutor(max_workers=1) as voxel_executor:
            voxel_future: Future | None = None
            prev_task_for_voxel: ImageTask | None = None

            for i, task in enumerate(self._tasks):
                self.log.info(f"\n{'â”€' * 40}")
                self.log.info(f"å¤„ç†è¿›åº¦: {i+1}/{total}")

                # æ˜¾ç¤ºå½“å‰é˜¶æ®µçš„å¹¶è¡ŒçŠ¶æ€
                if i == 0:
                    self.log.info("  é˜¶æ®µ: æ¨ç†ç¬¬1å¼ ï¼ˆæ— å¹¶è¡Œï¼‰")
                elif i < total:
                    self.log.info(f"  é˜¶æ®µ: æ¨ç†ç¬¬{i+1}å¼  || ä½“ç´ åŒ–ç¬¬{i}å¼ ï¼ˆå¹¶è¡Œï¼‰")

                # å¦‚æœæœ‰ä¸Šä¸€å¼ å›¾ç‰‡éœ€è¦ä½“ç´ åŒ–ï¼Œå¯åŠ¨å¼‚æ­¥ä½“ç´ åŒ–
                if prev_task_for_voxel is not None:
                    self.log.progress(
                        f"  å¯åŠ¨å¹¶è¡Œä½“ç´ åŒ–: [{prev_task_for_voxel.index+1}] {prev_task_for_voxel.image_path.name}"
                    )
                    voxel_future = voxel_executor.submit(
                        self._voxelize_single,
                        prev_task_for_voxel,
                        voxel_output_dir,
                        navigation_dir,
                    )

                # æ‰§è¡Œå½“å‰å›¾ç‰‡çš„æ¨ç†ï¼ˆä¸»çº¿ç¨‹ï¼‰
                predict_success = self._predict_single(task, output_dir)

                # ç­‰å¾…å¹¶è¡Œçš„ä½“ç´ åŒ–å®Œæˆï¼ˆå¦‚æœæœ‰ï¼‰
                if voxel_future is not None:
                    try:
                        voxel_future.result()
                    except Exception as e:
                        self.log.error(f"ä½“ç´ åŒ–ä»»åŠ¡å¼‚å¸¸: {e}")
                    voxel_future = None

                # è®°å½•å½“å‰ä»»åŠ¡ç”¨äºä¸‹ä¸€è½®çš„ä½“ç´ åŒ–
                prev_task_for_voxel = task if predict_success else None

            # å¤„ç†æœ€åä¸€å¼ å›¾ç‰‡çš„ä½“ç´ åŒ–ï¼ˆåŒæ­¥æ‰§è¡Œï¼Œæ— å¹¶è¡Œï¼‰
            if prev_task_for_voxel is not None:
                self.log.info(f"\n{'â”€' * 40}")
                self.log.info("æœ€ç»ˆé˜¶æ®µ: ä½“ç´ åŒ–æœ€åä¸€å¼ å›¾ç‰‡")
                self._voxelize_single(
                    prev_task_for_voxel, voxel_output_dir, navigation_dir
                )


def run_pipeline(
    input_path: str,
    output_dir: str,
    voxel_size: float = 0.005,
    checkpoint_path: str | None = None,
    verbose: bool = True,
    auto_unload: bool = True,
    enable_slice: bool = True,
    slice_radius: float = 10.0,
    enable_semantic: bool = False,
    semantic_model: str = "yolo11n-seg.pt",
    semantic_confidence: float = 0.5,
    colorize_semantic: bool = True,
) -> PipelineStats:
    """ä¾¿æ·å‡½æ•°ï¼šè¿è¡Œæµæ°´çº¿å¤„ç†ã€‚

    Args:
        input_path: è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        voxel_size: ä½“ç´ å°ºå¯¸ï¼ˆç±³ï¼‰
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        auto_unload: å¤„ç†å®Œæˆåè‡ªåŠ¨å¸è½½æ¨¡å‹ï¼ˆé»˜è®¤Trueï¼‰
        enable_slice: æ˜¯å¦å¯ç”¨åˆ‡ç‰‡ï¼ˆé»˜è®¤Trueï¼‰
        slice_radius: åˆ‡ç‰‡åŠå¾„ï¼ˆç±³ï¼Œé»˜è®¤10.0ï¼‰
        enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰æ£€æµ‹
        semantic_model: YOLO æ¨¡å‹åç§°
        semantic_confidence: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        colorize_semantic: æ˜¯å¦æ ¹æ®è¯­ä¹‰æ ‡ç­¾ç€è‰²

    Returns:
        PipelineStats: å¤„ç†ç»Ÿè®¡ä¿¡æ¯

    Example:
        >>> from aylm.tools.pipeline_processor import run_pipeline
        >>> stats = run_pipeline(
        ...     input_path="inputs/input_images",
        ...     output_dir="outputs/output_gaussians",
        ...     voxel_size=0.005,
        ...     slice_radius=10.0,
        ...     enable_semantic=True
        ... )
        >>> print(f"å¤„ç†å®Œæˆ: {stats.completed_images}/{stats.total_images}")
    """
    config = PipelineConfig(
        voxel_size=voxel_size,
        checkpoint_path=Path(checkpoint_path) if checkpoint_path else None,
        verbose=verbose,
        auto_unload=auto_unload,
        enable_slice=enable_slice,
        slice_radius=slice_radius,
        enable_semantic=enable_semantic,
        semantic_model=semantic_model,
        semantic_confidence=semantic_confidence,
        colorize_semantic=colorize_semantic,
    )

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºé‡Šæ”¾
    with PipelineProcessor(config) as processor:
        return processor.process(Path(input_path), Path(output_dir))


def run_pipeline_async(
    input_path: str,
    output_dir: str,
    voxel_size: float = 0.005,
    checkpoint_path: str | None = None,
    verbose: bool = True,
    callback: Callable[[PipelineStats], None] | None = None,
    enable_slice: bool = True,
    slice_radius: float = 10.0,
    enable_semantic: bool = False,
    semantic_model: str = "yolo11n-seg.pt",
    semantic_confidence: float = 0.5,
    colorize_semantic: bool = True,
) -> tuple[PipelineProcessor, Future]:
    """ä¾¿æ·å‡½æ•°ï¼šå¼‚æ­¥è¿è¡Œæµæ°´çº¿å¤„ç†ã€‚

    Args:
        input_path: è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        voxel_size: ä½“ç´ å°ºå¯¸ï¼ˆç±³ï¼‰
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        callback: å¤„ç†å®Œæˆåçš„å›è°ƒå‡½æ•°
        enable_slice: æ˜¯å¦å¯ç”¨åˆ‡ç‰‡ï¼ˆé»˜è®¤Trueï¼‰
        slice_radius: åˆ‡ç‰‡åŠå¾„ï¼ˆç±³ï¼Œé»˜è®¤10.0ï¼‰
        enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰æ£€æµ‹
        semantic_model: YOLO æ¨¡å‹åç§°
        semantic_confidence: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        colorize_semantic: æ˜¯å¦æ ¹æ®è¯­ä¹‰æ ‡ç­¾ç€è‰²

    Returns:
        Tuple[PipelineProcessor, Future]: å¤„ç†å™¨å®ä¾‹å’ŒFutureå¯¹è±¡

    Example:
        >>> from aylm.tools.pipeline_processor import run_pipeline_async
        >>> processor, future = run_pipeline_async(
        ...     input_path="inputs/input_images",
        ...     output_dir="outputs/output_gaussians",
        ...     slice_radius=10.0,
        ...     enable_semantic=True
        ... )
        >>> # åšå…¶ä»–äº‹æƒ…...
        >>> stats = future.result()  # ç­‰å¾…å®Œæˆ
        >>> processor.cleanup()  # æ‰‹åŠ¨æ¸…ç†ï¼ˆæˆ–è®©processorè¢«åƒåœ¾å›æ”¶ï¼‰
    """
    config = PipelineConfig(
        voxel_size=voxel_size,
        checkpoint_path=Path(checkpoint_path) if checkpoint_path else None,
        verbose=verbose,
        auto_unload=True,  # å¼‚æ­¥æ¨¡å¼ä¸‹ä¹Ÿè‡ªåŠ¨å¸è½½
        enable_slice=enable_slice,
        slice_radius=slice_radius,
        enable_semantic=enable_semantic,
        semantic_model=semantic_model,
        semantic_confidence=semantic_confidence,
        colorize_semantic=colorize_semantic,
    )

    processor = PipelineProcessor(config)
    future = processor.process_async(
        Path(input_path), Path(output_dir), callback=callback
    )
    return processor, future


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    import sys

    if len(sys.argv) < 3:
        print("ç”¨æ³•: python pipeline_processor.py <è¾“å…¥ç›®å½•> <è¾“å‡ºç›®å½•>")
        sys.exit(1)

    stats = run_pipeline(sys.argv[1], sys.argv[2])
    sys.exit(0 if stats.failed_images == 0 else 1)
